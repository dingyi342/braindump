@ARTICLE{10.3389/fnbot.2019.00018,
  AUTHOR          = {Bing, Zhenshan and Baumann, Ivan and Jiang, Zhuangyi and
                  Huang, Kai and Cai, Caixia and Knoll, Alois},
  TITLE           = {Supervised Learning in SNN via Reward-Modulated
                  Spike-Timing-Dependent Plasticity for a Target Reaching
                  Vehicle},
  JOURNAL         = {Frontiers in Neurorobotics},
  VOLUME          = 13,
  PAGES           = 18,
  YEAR            = 2019,
  URL             =
                  {https://www.frontiersin.org/article/10.3389/fnbot.2019.00018},
  DOI             = {10.3389/fnbot.2019.00018},
  ISSN            = {1662-5218},
  ABSTRACT        = {Spiking neural networks (SNNs) offer many advantages over
                  traditional artificial neural networks (ANNs) such as
                  biological plausibility, fast information processing, and
                  energy efficiency. Although SNNs have been used to solve a
                  variety of control tasks using the Spike-Timing-Dependent
                  Plasticity (STDP) learning rule, existing solutions usually
                  involve hard-coded network architectures solving specific
                  tasks rather than solving different kinds of tasks generally.
                  This results in neglecting one of the biggest advantages of
                  ANNs, i.e., being general-purpose and easy-to-use due to their
                  simple network architecture, which usually consists of an
                  input layer, one or multiple hidden layers and an output
                  layer. This paper addresses the problem by introducing an
                  end-to-end learning approach of spiking neural networks
                  constructed with one hidden layer and reward-modulated
                  Spike-Timing-Dependent Plasticity (R-STDP) synapses in an
                  all-to-all fashion. We use the supervised reward-modulated
                  Spike-Timing-Dependent-Plasticity learning rule to train two
                  different SNN-based sub-controllers to replicate a desired
                  obstacle avoiding and goal approaching behavior, provided by
                  pre-generated datasets. Together they make up a
                  target-reaching controller, which is used to control a
                  simulated mobile robot to reach a target area while avoiding
                  obstacles in its path. We demonstrate the performance and
                  effectiveness of our trained SNNs to achieve target reaching
                  tasks in different unknown scenarios.}
}

@ARTICLE{10.3389/fncom.2017.00024,
  author          = {Scellier, Benjamin and Bengio, Yoshua},
  title           = {Equilibrium Propagation: Bridging the Gap between
                  Energy-Based Models and Backpropagation},
  journal         = {Frontiers in Computational Neuroscience},
  volume          = 11,
  pages           = 24,
  year            = 2017,
  url             =
                  {https://www.frontiersin.org/article/10.3389/fncom.2017.00024},
  doi             = {10.3389/fncom.2017.00024},
  issn            = {1662-5188},
  abstract        = {We introduce Equilibrium Propagation, a learning framework
                  for energy-based models. It involves only one kind of neural
                  computation, performed in both the first phase (when the
                  prediction is made) and the second phase of training (after
                  the target or prediction error is revealed). Although this
                  algorithm computes the gradient of an objective function just
                  like Backpropagation, it does not need a special computation
                  or circuit for the second phase, where errors are implicitly
                  propagated. Equilibrium Propagation shares similarities with
                  Contrastive Hebbian Learning and Contrastive Divergence while
                  solving the theoretical issues of both algorithms: our
                  algorithm computes the gradient of a well-defined objective
                  function. Because the objective function is defined in terms
                  of local perturbations, the second phase of Equilibrium
                  Propagation corresponds to only nudging the prediction (fixed
                  point or stationary distribution) toward a configuration that
                  reduces prediction error. In the case of a recurrent
                  multi-layer supervised network, the output units are slightly
                  nudged toward their target in the second phase, and the
                  perturbation introduced at the output layer propagates
                  backward in the hidden layers. We show that the signal
                  “back-propagated” during this second phase corresponds to the
                  propagation of error derivatives and encodes the gradient of
                  the objective function, when the synaptic update corresponds
                  to a standard form of spike-timing dependent plasticity. This
                  work makes it more plausible that a mechanism similar to
                  Backpropagation could be implemented by brains, since leaky
                  integrator neural computation performs both inference and
                  error back-propagation in our model. The only local difference
                  between the two phases is whether synaptic changes are allowed
                  or not. We also show experimentally that multi-layer
                  recurrently connected networks with 1, 2, and 3 hidden layers
                  can be trained by Equilibrium Propagation on the
                  permutation-invariant MNIST task.}
}

@ARTICLE{10.3389/fninf.2018.00089,
  AUTHOR          = {Hazan, Hananel and Saunders, Daniel J. and Khan, Hassaan
                  and Patel, Devdhar and Sanghavi, Darpan T. and Siegelmann,
                  Hava T. and Kozma, Robert},
  TITLE           = {BindsNET: A Machine Learning-Oriented Spiking Neural
                  Networks Library in Python},
  JOURNAL         = {Frontiers in Neuroinformatics},
  VOLUME          = 12,
  PAGES           = 89,
  YEAR            = 2018,
  URL             =
                  {https://www.frontiersin.org/article/10.3389/fninf.2018.00089},
  DOI             = {10.3389/fninf.2018.00089},
  ISSN            = {1662-5196},
}

@ARTICLE{10.3389/fnins.2015.00481,
  AUTHOR          = {Serrano-Gotarredona, Teresa and Linares-Barranco, Bernabé},
  TITLE           = {Poker-DVS and MNIST-DVS. Their History, How They Were Made,
                  and Other Details},
  JOURNAL         = {Frontiers in Neuroscience},
  VOLUME          = 9,
  PAGES           = 481,
  YEAR            = 2015,
  URL             =
                  {https://www.frontiersin.org/article/10.3389/fnins.2015.00481},
  DOI             = {10.3389/fnins.2015.00481},
  ISSN            = {1662-453X},
  ABSTRACT        = {This article reports on two databases for event-driven
                  object recognition using a Dynamic Vision Sensor (DVS). The
                  first, which we call Poker-DVS and is being released together
                  with this article, was obtained by browsing specially made
                  poker card decks in front of a DVS camera for 2–4 s. Each card
                  appeared on the screen for about 20–30 ms. The poker pips were
                  tracked and isolated off-line to constitute the 131-recording
                  Poker-DVS database. The second database, which we call
                  MNIST-DVS and which was released in December 2013, consists of
                  a set of 30,000 DVS camera recordings obtained by displaying
                  10,000 moving symbols from the standard MNIST 70,000-picture
                  database on an LCD monitor for about 2–3 s each. Each of the
                  10,000 symbols was displayed at three different scales, so
                  that event-driven object recognition algorithms could easily
                  be tested for different object sizes. This article tells the
                  story behind both databases, covering, among other aspects,
                  details of how they work and the reasons for their creation.
                  We provide not only the databases with corresponding scripts,
                  but also the scripts and data used to generate the figures
                  shown in this article (as Supplementary Material).}
}

@ARTICLE{10.3389/fnins.2016.00508,
  AUTHOR          = {Lee, Jun Haeng and Delbruck, Tobi and Pfeiffer, Michael},
  TITLE           = {Training Deep Spiking Neural Networks Using
                  Backpropagation},
  JOURNAL         = {Frontiers in Neuroscience},
  VOLUME          = 10,
  PAGES           = 508,
  YEAR            = 2016,
  URL             =
                  {https://www.frontiersin.org/article/10.3389/fnins.2016.00508},
  DOI             = {10.3389/fnins.2016.00508},
  ISSN            = {1662-453X},
  ABSTRACT        = {Deep spiking neural networks (SNNs) hold the potential for
                  improving the latency and energy efficiency of deep neural
                  networks through data-driven event-based computation. However,
                  training such networks is difficult due to the
                  non-differentiable nature of spike events. In this paper, we
                  introduce a novel technique, which treats the membrane
                  potentials of spiking neurons as differentiable signals, where
                  discontinuities at spike times are considered as noise. This
                  enables an error backpropagation mechanism for deep SNNs that
                  follows the same principles as in conventional deep networks,
                  but works directly on spike signals and membrane potentials.
                  Compared with previous methods relying on indirect training
                  and conversion, our technique has the potential to capture the
                  statistics of spikes more precisely. We evaluate the proposed
                  framework on artificially generated events from the original
                  MNIST handwritten digit benchmark, and also on the N-MNIST
                  benchmark recorded with an event-based dynamic vision sensor,
                  in which the proposed method reduces the error rate by a
                  factor of more than three compared to the best previous SNN,
                  and also achieves a higher accuracy than a conventional
                  convolutional neural network (CNN) trained and tested on the
                  same data. We demonstrate in the context of the MNIST task
                  that thanks to their event-driven operation, deep SNNs (both
                  fully connected and convolutional) trained with our method
                  achieve accuracy equivalent with conventional neural networks.
                  In the N-MNIST example, equivalent accuracy is achieved with
                  about five times fewer computational operations.}
}

@ARTICLE{10.3389/fnins.2018.00435,
  AUTHOR          = {Lee, Chankyu and Panda, Priyadarshini and Srinivasan,
                  Gopalakrishnan and Roy, Kaushik},
  TITLE           = {Training Deep Spiking Convolutional Neural Networks With
                  STDP-Based Unsupervised Pre-training Followed by Supervised
                  Fine-Tuning},
  JOURNAL         = {Frontiers in Neuroscience},
  VOLUME          = 12,
  PAGES           = 435,
  YEAR            = 2018,
  URL             =
                  {https://www.frontiersin.org/article/10.3389/fnins.2018.00435},
  DOI             = {10.3389/fnins.2018.00435},
  ISSN            = {1662-453X},
  ABSTRACT        = {Spiking Neural Networks (SNNs) are fast becoming a
                  promising candidate for brain-inspired neuromorphic computing
                  because of their inherent power efficiency and impressive
                  inference accuracy across several cognitive tasks such as
                  image classification and speech recognition. The recent
                  efforts in SNNs have been focused on implementing deeper
                  networks with multiple hidden layers to incorporate
                  exponentially more difficult functional representations. In
                  this paper, we propose a pre-training scheme using
                  biologically plausible unsupervised learning, namely
                  Spike-Timing-Dependent-Plasticity (STDP), in order to better
                  initialize the parameters in multi-layer systems prior to
                  supervised optimization. The multi-layer SNN is comprised of
                  alternating convolutional and pooling layers followed by
                  fully-connected layers, which are populated with leaky
                  integrate-and-fire spiking neurons. We train the deep SNNs in
                  two phases wherein, first, convolutional kernels are
                  pre-trained in a layer-wise manner with unsupervised learning
                  followed by fine-tuning the synaptic weights with spike-based
                  supervised gradient descent backpropagation. Our experiments
                  on digit recognition demonstrate that the STDP-based
                  pre-training with gradient-based optimization provides
                  improved robustness, faster (~2.5 ×) training time and better
                  generalization compared with purely gradient-based training
                  without pre-training.}
}

@misc{17_bartek,
  author          = {Bartek},
  howpublished    = {https://www.bfilipek.com/2017/10/notnull.html},
  note            = {Online; accessed 02 April 2019},
  title           = {Bartek's coding blog: How not_null can improve your code?},
  year            = 2017,
}

@Article{Cybenko1989,
  author          = "Cybenko, G.",
  title           = "Approximation by superpositions of a sigmoidal function",
  journal         = "Mathematics of Control, Signals and Systems",
  year            = 1989,
  month           = "Dec",
  day             = 01,
  volume          = 2,
  number          = 4,
  pages           = "303--314",
  abstract        = "In this paper we demonstrate that finite linear
                  combinations of compositions of a fixed, univariate function
                  and a set of affine functionals can uniformly approximate any
                  continuous function ofn real variables with support in the
                  unit hypercube; only mild conditions are imposed on the
                  univariate function. Our results settle an open question about
                  representability in the class of single hidden layer neural
                  networks. In particular, we show that arbitrary decision
                  regions can be arbitrarily well approximated by continuous
                  feedforward neural networks with only a single internal,
                  hidden layer and any continuous sigmoidal nonlinearity. The
                  paper discusses approximation properties of other possible
                  types of nonlinearities that might be implemented by
                  artificial neural networks.",
  issn            = "1435-568X",
  doi             = "10.1007/BF02551274",
  url             = "https://doi.org/10.1007/BF02551274"
}

@book{DBLP:books/oreilly/Kleppmann2014,
  author          = {Martin Kleppmann},
  title           = {Designing Data-Intensive Applications: The Big Ideas Behind
                  Reliable, Scalable, and Maintainable Systems},
  publisher       = {O'Reilly},
  year            = 2016,
  url             = {http://shop.oreilly.com/product/0636920032175.do},
  isbn            = {978-1-4493-7332-0},
  timestamp       = {Mon, 27 May 2019 17:50:46 +0200},
  biburl          = {https://dblp.org/rec/bib/books/oreilly/Kleppmann2014},
  bibsource       = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Du2010APA,
  title           = {A POMDP Approach to Robot Motion Planning under
                  Uncertainty},
  author          = {Yanzhu Du and David Hsu and Hanna Kurniawati and Wee Sun
                  Lee and Sylvie C. W. Ong and Shao Wei Png},
  year            = 2010
}

@article{MAASS19971659,
  title           = "Networks of spiking neurons: The third generation of neural
                  network models",
  journal         = "Neural Networks",
  volume          = 10,
  number          = 9,
  pages           = "1659 - 1671",
  year            = 1997,
  issn            = "0893-6080",
  doi             = "https://doi.org/10.1016/S0893-6080(97)00011-7",
  url             =
                  "http://www.sciencedirect.com/science/article/pii/S0893608097000117",
  author          = "Wolfgang Maass",
  keywords        = "Spiking neuron, Integrate-and-fire neutron, Computational
                  complexity, Sigmoidal neural nets, Lower bounds",
  abstract        = "The computational power of formal models for networks of
                  spiking neurons is compared with that of other neural network
                  models based on McCulloch Pitts neurons (i.e., threshold
                  gates), respectively, sigmoidal gates. In particular it is
                  shown that networks of spiking neurons are, with regard to the
                  number of neurons that are needed, computationally more
                  powerful than these other neural network models. A concrete
                  biologically relevant function is exhibited which can be
                  computed by a single spiking neuron (for biologically
                  reasonable values of its parameters), but which requires
                  hundreds of hidden units on a sigmoidal neural net. On the
                  other hand, it is known that any function that can be computed
                  by a small sigmoidal neural net can also be computed by a
                  small network of spiking neurons. This article does not assume
                  prior knowledge about spiking neurons, and it contains an
                  extensive list of references to the currently available
                  literature on computations in networks of spiking neurons and
                  relevant results from neurobiology."
}

@article {Merolla668,
  author          = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza,
                  Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan,
                  Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and
                  Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser,
                  Steven K. and Appuswamy, Rathinakumar and Taba, Brian and
                  Amir, Arnon and Flickner, Myron D. and Risk, William P. and
                  Manohar, Rajit and Modha, Dharmendra S.},
  title           = {A million spiking-neuron integrated circuit with a scalable
                  communication network and interface},
  volume          = 345,
  number          = 6197,
  pages           = {668--673},
  year            = 2014,
  doi             = {10.1126/science.1254642},
  publisher       = {American Association for the Advancement of Science},
  abstract        = {Computers are nowhere near as versatile as our own brains.
                  Merolla et al. applied our present knowledge of the structure
                  and function of the brain to design a new computer chip that
                  uses the same wiring rules and architecture. The flexible,
                  scalable chip operated efficiently in real time, while using
                  very little power.Science, this issue p. 668 Inspired by the
                  brain{\textquoteright}s structure, we have developed an
                  efficient, scalable, and flexible non{\textendash}von Neumann
                  architecture that leverages contemporary silicon technology.
                  To demonstrate, we built a 5.4-billion-transistor chip with
                  4096 neurosynaptic cores interconnected via an intrachip
                  network that integrates 1 million programmable spiking neurons
                  and 256 million configurable synapses. Chips can be tiled in
                  two dimensions via an interchip communication interface,
                  seamlessly scaling the architecture to a cortexlike sheet of
                  arbitrary size. The architecture is well suited to many
                  applications that use complex neural networks in real time,
                  for example, multiobject detection and classification. With
                  400-pixel-by-240-pixel video input at 30 frames per second,
                  the chip consumes 63 milliwatts.},
  issn            = {0036-8075},
  URL             = {https://science.sciencemag.org/content/345/6197/668},
  eprint          =
                  {https://science.sciencemag.org/content/345/6197/668.full.pdf},
  journal         = {Science}
}

@Article{Mnih_2015,
  author          = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David
                  and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G.
                  and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas
                  K. and Ostrovski, Georg and et al.},
  title           = {Human-level control through deep reinforcement learning},
  year            = 2015,
  volume          = 518,
  number          = 7540,
  month           = {Feb},
  pages           = {529–533},
  issn            = {1476-4687},
  doi             = {10.1038/nature14236},
  url             = {http://dx.doi.org/10.1038/nature14236},
  journal         = {Nature},
  publisher       = {Springer Nature}
}

@incollection{NIPS2016_6383,
  title           = {Unifying Count-Based Exploration and Intrinsic Motivation},
  author          = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg
                  and Schaul, Tom and Saxton, David and Munos, Remi},
  booktitle       = {Advances in Neural Information Processing Systems 29},
  editor          = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon
                  and R. Garnett},
  pages           = {1471--1479},
  year            = 2016,
  publisher       = {Curran Associates, Inc.},
  url             =
                  {http://papers.nips.cc/paper/6383-unifying-count-based-exploration-and-intrinsic-motivation.pdf}
}

@incollection{NIPS2018_7415,
  title           = {SLAYER: Spike Layer Error Reassignment in Time},
  author          = {Shrestha, Sumit Bam and Orchard, Garrick},
  booktitle       = {Advances in Neural Information Processing Systems 31},
  editor          = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman
                  and N. Cesa-Bianchi and R. Garnett},
  pages           = {1412--1421},
  year            = 2018,
  publisher       = {Curran Associates, Inc.},
  url             =
                  {http://papers.nips.cc/paper/7415-slayer-spike-layer-error-reassignment-in-time.pdf}
}

@incollection{NIPS2018_7417,
  title           = {Gradient Descent for Spiking Neural Networks},
  author          = {Huh, Dongsung and Sejnowski, Terrence J},
  booktitle       = {Advances in Neural Information Processing Systems 31},
  editor          = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman
                  and N. Cesa-Bianchi and R. Garnett},
  pages           = {1433--1443},
  year            = 2018,
  publisher       = {Curran Associates, Inc.},
  url             =
                  {http://papers.nips.cc/paper/7417-gradient-descent-for-spiking-neural-networks.pdf}
}

@Article{Rafferty_2015,
  author          = {Rafferty, Anna N. and Brunskill, Emma and Griffiths, Thomas
                  L. and Shafto, Patrick},
  title           = {Faster Teaching via POMDP Planning},
  year            = 2015,
  volume          = 40,
  number          = 6,
  month           = {Sep},
  pages           = {1290–1332},
  issn            = {0364-0213},
  doi             = {10.1111/cogs.12290},
  url             = {http://dx.doi.org/10.1111/cogs.12290},
  journal         = {Cognitive Science},
  publisher       = {Wiley}
}

@article{SHRESTHA201733,
  title           = "Robust spike-train learning in spike-event based weight
                  update",
  journal         = "Neural Networks",
  volume          = 96,
  pages           = "33 - 46",
  year            = 2017,
  issn            = "0893-6080",
  doi             = "https://doi.org/10.1016/j.neunet.2017.08.010",
  url             =
                  "http://www.sciencedirect.com/science/article/pii/S0893608017302009",
  author          = "Sumit Bam Shrestha and Qing Song",
  keywords        = "Spiking neural network, Multilayer spike-train learning,
                  Supervised learning, Weight convergence, Robust stability,
                  Adaptive learning rate",
  abstract        = "Supervised learning algorithms in a spiking neural network
                  either learn a spike-train pattern for a single neuron
                  receiving input spike-train from multiple input synapses or
                  learn to output the first spike time in a feedforward network
                  setting. In this paper, we build upon spike-event based weight
                  update strategy to learn continuous spike-train in a spiking
                  neural network with a hidden layer using a dead zone on–off
                  based adaptive learning rate rule which ensures convergence of
                  the learning process in the sense of weight convergence and
                  robustness of the learning process to external disturbances.
                  Based on different benchmark problems, we compare this new
                  method with other relevant spike-train learning algorithms.
                  The results show that the speed of learning is much improved
                  and the rate of successful learning is also greatly improved."
}

@article{Severa2016SpikingNA,
  title           = "Spiking network algorithms for scientific computing",
  author          = "William Severa and Ojas Parekh and Kristofor D. Carlson and
                  Conrad D. James and James B. Aimone",
  journal         = "2016 IEEE International Conference on Rebooting Computing
                  (ICRC)",
  year            = 2016,
  pages           = "1-8"
}

@book{Shalev-Shwartz:2014:UML:2621980,
  author          = {Shalev-Shwartz, Shai and Ben-David, Shai},
  title           = {Understanding Machine Learning: From Theory to Algorithms},
  year            = 2014,
  isbn            = {1107057132, 9781107057135},
  publisher       = {Cambridge University Press},
  address         = {New York, NY, USA},
}

@article{SnnSlam,
  author          = "Tang, Guangzhi and Shah, Arpit and Michmizos, Konstantinos
                  P.",
  title           = "Spiking Neural Network on Neuromorphic Hardware for
                  Energy-Efficient Unidimensional Slam",
  journal         = "CoRR",
  year            = 2019,
  url             = "http://arxiv.org/abs/1903.02504v2",
  abstract        = "Energy-efficient simultaneous localization and mapping
                  (SLAM) is crucial for mobile robots exploring unknown
                  environments. The mammalian brain solves SLAM via a network of
                  specialized neurons, exhibiting asynchronous computations and
                  event-based communications, with very low energy consumption.
                  We propose a brain-inspired spiking neural network (SNN)
                  architecture that solves the unidimensional SLAM by
                  introducing spike-based reference frame transformation, visual
                  likelihood computation, and Bayesian inference. We integrated
                  our neuromorphic algorithm to Intel's Loihi neuromorphic
                  processor, a non-Von Neumann hardware that mimics the brain's
                  computing paradigms. We performed comparative analyses for
                  accuracy and energy-efficiency between our neuromorphic
                  approach and the GMapping algorithm, which is widely used in
                  small environments. Our Loihi-based SNN architecture consumes
                  100 times less energy than GMapping run on a CPU while having
                  comparable accuracy in head direction localization and
                  map-generation. These results pave the way for scaling our
                  approach towards active-SLAM alternative solutions for
                  Loihi-controlled autonomous robots.",
  archivePrefix   = "arXiv",
  eprint          = "1903.02504",
  primaryClass    = "cs.RO",
}

@article{TAVANAEI201947,
  title           = "Deep learning in spiking neural networks",
  journal         = "Neural Networks",
  volume          = 111,
  pages           = "47 - 63",
  year            = 2019,
  issn            = "0893-6080",
  doi             = "https://doi.org/10.1016/j.neunet.2018.12.002",
  url             =
                  "http://www.sciencedirect.com/science/article/pii/S0893608018303332",
  author          = "Amirhossein Tavanaei and Masoud Ghodrati and Saeed Reza
                  Kheradpisheh and Timothée Masquelier and Anthony Maida",
  keywords        = "Deep learning, Spiking neural network, Biological
                  plausibility, Machine learning, Power-efficient architecture",
  abstract        = "In recent years, deep learning has revolutionized the field
                  of machine learning, for computer vision in particular. In
                  this approach, a deep (multilayer) artificial neural network
                  (ANN) is trained, most often in a supervised manner using
                  backpropagation. Vast amounts of labeled training examples are
                  required, but the resulting classification accuracy is truly
                  impressive, sometimes outperforming humans. Neurons in an ANN
                  are characterized by a single, static, continuous-valued
                  activation. Yet biological neurons use discrete spikes to
                  compute and transmit information, and the spike times, in
                  addition to the spike rates, matter. Spiking neural networks
                  (SNNs) are thus more biologically realistic than ANNs, and are
                  arguably the only viable option if one wants to understand how
                  the brain computes at the neuronal description level. The
                  spikes of biological neurons are sparse in time and space, and
                  event-driven. Combined with bio-plausible local learning
                  rules, this makes it easier to build low-power, neuromorphic
                  hardware for SNNs. However, training deep SNNs remains a
                  challenge. Spiking neurons’ transfer function is usually
                  non-differentiable, which prevents using backpropagation. Here
                  we review recent supervised and unsupervised methods to train
                  deep SNNs, and compare them in terms of accuracy and
                  computational cost. The emerging picture is that SNNs still
                  lag behind ANNs in terms of accuracy, but the gap is
                  decreasing, and can even vanish on some tasks, while SNNs
                  typically require many fewer operations and are the better
                  candidates to process spatio-temporal data."
}

@article{VITANZA20153122,
  title           = "Spiking neural controllers in multi-agent competitive
                  systems for adaptive targeted motor learning",
  journal         = "Journal of the Franklin Institute",
  volume          = 352,
  number          = 8,
  pages           = "3122 - 3143",
  year            = 2015,
  note            = "Special Issue on Advances in Nonlinear Dynamics and
                  Control",
  issn            = "0016-0032",
  doi             = "https://doi.org/10.1016/j.jfranklin.2015.04.014",
  url             =
                  "http://www.sciencedirect.com/science/article/pii/S001600321500174X",
  author          = "Alessandra Vitanza and Luca Patané and Paolo Arena",
  abstract        = "The proposed work introduces a neural control strategy for
                  guiding adaptation in spiking neural structures acting as
                  nonlinear controllers in a group of bio-inspired robots which
                  compete in reaching targets in a virtual environment. The
                  neural structures embedded into each agent are inspired by a
                  specific part of the insect brain, namely Central Complex,
                  devoted to detect, learn and memorize visual features for
                  targeted motor control. A reduced-order model of a spiking
                  neuron is used as the basic building block for the neural
                  controller. The control methodology employs bio-inspired,
                  correlation based learning mechanisms like Spike timing
                  dependent plasticity with the addition of a
                  reward/punishment-based method experimentally found in
                  insects. The reference signal for the overall multi-agent
                  control system is imposed by a global reward, which guides
                  motor learning to direct each agent towards specific visual
                  targets. The neural controllers within the agents start from
                  identical conditions: the learning strategy induces each robot
                  to show anticipated targeting actions upon specific visual
                  stimuli. The whole control structure also contributes to make
                  the robots refractory or more sensitive to specific visual
                  stimuli, showing distinct preferences in future choices. This
                  leads to an environmentally induced, targeted motor control,
                  even without a direct communication among the agents, giving
                  robots, while running, the ability to perform adaptation in
                  real-time. Experiments, carried out in a dynamic simulation
                  environment, show the suitability of the proposed approach.
                  Specific performance indexes, like Shannon׳s Entropy, are
                  adopted to quantitatively analyze diversity and specialization
                  within the group."
}

@book{White:2009:HDG:1717298,
  author          = {White, Tom},
  title           = {Hadoop: The Definitive Guide},
  year            = 2009,
  isbn            = {0596521979, 9780596521974},
  edition         = {1st},
  publisher       = {O'Reilly Media, Inc.}
}

@inproceedings{Whiteside2017HowTW,
  title           = {How to Write a Technical Paper},
  author          = {James D. Whiteside},
  year            = 2017
}

@inproceedings{abbeel2004apprenticeship,
  title           = {Apprenticeship learning via inverse reinforcement learning},
  author          = {Abbeel, Pieter and Ng, Andrew Y},
  booktitle       = {Proceedings of the twenty-first international conference on
                  Machine learning},
  pages           = 1,
  year            = 2004,
  organization    = {ACM}
}

@article{aenugu19_reinf_learn_with_spikin_coagen,
  author          = {Aenugu, Sneha and Sharma, Abhishek and Yelamarthi,
                  Sasikiran and Hazan, Hananel and Thomas, Philip S. and Kozma,
                  Robert},
  title           = {Reinforcement Learning With Spiking Coagents},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1910.06489v2},
  abstract        = {Neuroscientific theory suggests that dopaminergic neurons
                  broadcast global reward prediction errors to large areas of
                  the brain influencing the synaptic plasticity of the neurons
                  in those regions. We build on this theory to propose a
                  multi-agent learning framework with spiking neurons in the
                  generalized linear model (GLM) formulation as agents, to solve
                  reinforcement learning (RL) tasks. We show that a network of
                  GLM spiking agents connected in a hierarchical fashion, where
                  each spiking agent modulates its firing policy based on local
                  information and a global prediction error, can learn complex
                  action representations to solve RL tasks. We further show how
                  leveraging principles of modularity and population coding
                  inspired from the brain can help reduce variance in the
                  learning updates making it a viable optimization technique.},
  archivePrefix   = {arXiv},
  eprint          = {1910.06489},
  primaryClass    = {cs.LG},
}

@article{aiskinLee,
  author          = {Lee, Wang Wei and Tan, Yu Jun and Yao, Haicheng and Li, Si
                  and See, Hian Hian and Hon, Matthew and Ng, Kian Ann and
                  Xiong, Betty and Ho, John S. and Tee, Benjamin C. K.},
  title           = {A neuro-inspired artificial peripheral nervous system for
                  scalable electronic skins},
  volume          = 4,
  number          = 32,
  elocation-id    = {eaax2198},
  year            = 2019,
  doi             = {10.1126/scirobotics.aax2198},
  publisher       = {Science Robotics},
  abstract        = {The human sense of touch is essential for dexterous tool
                  usage, spatial awareness, and social communication. Equipping
                  intelligent human-like androids and prosthetics with
                  electronic skins{\textemdash}a large array of sensors
                  spatially distributed and capable of rapid somatosensory
                  perception{\textemdash}will enable them to work
                  collaboratively and naturally with humans to manipulate
                  objects in unstructured living environments. Previously
                  reported tactile-sensitive electronic skins largely transmit
                  the tactile information from sensors serially, resulting in
                  readout latency bottlenecks and complex wiring as the number
                  of sensors increases. Here, we introduce the Asynchronously
                  Coded Electronic Skin (ACES){\textemdash}a neuromimetic
                  architecture that enables simultaneous transmission of
                  thermotactile information while maintaining exceptionally low
                  readout latencies, even with array sizes beyond 10,000
                  sensors. We demonstrate prototype arrays of up to 240
                  artificial mechanoreceptors that transmitted events
                  asynchronously at a constant latency of 1 ms while maintaining
                  an ultra-high temporal precision of \&lt;60 ns, thus resolving
                  fine spatiotemporal features necessary for rapid tactile
                  perception. Our platform requires only a single electrical
                  conductor for signal propagation, realizing sensor arrays that
                  are dynamically reconfigurable and robust to damage. We
                  anticipate that the ACES platform can be integrated with a
                  wide range of skin-like sensors for artificial intelligence
                  (AI){\textendash}enhanced autonomous robots, neuroprosthetics,
                  and neuromorphic computing hardware for dexterous object
                  manipulation and somatosensory perception.},
  URL             = {https://robotics.sciencemag.org/content/4/32/eaax2198},
  eprint          =
                  {https://robotics.sciencemag.org/content/4/32/eaax2198.full.pdf},
  journal         = {Science Robotics}
}

@inproceedings{andrychowicz2017hindsight,
  title           = {Hindsight experience replay},
  author          = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and
                  Schneider, Jonas and Fong, Rachel and Welinder, Peter and
                  McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and
                  Zaremba, Wojciech},
  booktitle       = {Advances in Neural Information Processing Systems},
  pages           = {5048--5058},
  year            = 2017
}

@inproceedings{anschel2017averaged,
  title           = {Averaged-dqn: Variance reduction and stabilization for deep
                  reinforcement learning},
  author          = {Anschel, Oron and Baram, Nir and Shimkin, Nahum},
  booktitle       = {Proceedings of the 34th International Conference on Machine
                  Learning-Volume 70},
  pages           = {176--185},
  year            = 2017,
  organization    = {JMLR. org}
}

@article{arjovsky19_invar_risk_minim,
  author          = {Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan
                  and Lopez-Paz, David},
  title           = {Invariant Risk Minimization},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1907.02893v1},
  abstract        = {We introduce Invariant Risk Minimization (IRM), a learning
                  paradigm to estimate invariant correlations across multiple
                  training distributions. To achieve this goal, IRM learns a
                  data representation such that the optimal classifier, on top
                  of that data representation, matches for all training
                  distributions. Through theory and experiments, we show how the
                  invariances learned by IRM relate to the causal structures
                  governing the data and enable out-of-distribution
                  generalization.},
  archivePrefix   = {arXiv},
  eprint          = {1907.02893},
  primaryClass    = {stat.ML},
}

@article{baltrusaitis17:_multim_machin_learn,
  journal         = {CoRR},
  title           = {Multimodal Machine Learning: A Survey and Taxonomy},
  author          = {Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency,
                  Louis-Philippe},
  archivePrefix   = {arXiv},
  year            = 2017,
  eprint          = {1705.09406},
  primaryClass    = {cs.LG},
  abstract        = {Our experience of the world is multimodal - we see objects,
                  hear sounds, feel texture, smell odors, and taste flavors.
                  Modality refers to the way in which something happens or is
                  experienced and a research problem is characterized as
                  multimodal when it includes multiple such modalities.
                  Baltru\vsaitis, Tadas, Ahuja, C., & Morency, L., Multimodal
                  machine learning: a survey and taxonomy, CoRR, (), (2017). In
                  order for Artificial Intelligence to make progress in
                  understanding the world around us, it needs to be able to
                  interpret such multimodal signals together. Multimodal machine
                  learning aims to build models that can process and relate
                  information from multiple modalities. It is a vibrant
                  multi-disciplinary field of increasing importance and with
                  extraordinary potential. Instead of focusing on specific
                  multimodal applications, this paper surveys the recent
                  advances in multimodal machine learning itself and presents
                  them in a common taxonomy. We go beyond the typical early and
                  late fusion categorization and identify broader challenges
                  that are faced by multimodal machine learning, namely:
                  representation, translation, alignment, fusion, and
                  co-learning. This new taxonomy will enable researchers to
                  better understand the state of the field and identify
                  directions for future research.},
  url             = {http://arxiv.org/abs/1705.09406v2},
}

@article{bellec18_long_short_term_memor_learn,
  author          = {Bellec, Guillaume and Salaj, Darjan and Subramoney, Anand
                  and Legenstein, Robert and Maass, Wolfgang},
  title           = {Long Short-Term Memory and Learning-To-Learn in Networks of
                  Spiking Neurons},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1803.09574v4},
  abstract        = {Recurrent networks of spiking neurons (RSNNs) underlie the
                  astounding computing and learning capabilities of the brain.
                  But computing and learning capabilities of RSNN models have
                  remained poor, at least in comparison with artificial neural
                  networks (ANNs). We address two possible reasons for that. One
                  is that RSNNs in the brain are not randomly connected or
                  designed according to simple rules, and they do not start
                  learning as a tabula rasa network. Rather, RSNNs in the brain
                  were optimized for their tasks through evolution, development,
                  and prior experience. Details of these optimization processes
                  are largely unknown. But their functional contribution can be
                  approximated through powerful optimization methods, such as
                  backpropagation through time (BPTT). A second major mismatch
                  between RSNNs in the brain and models is that the latter only
                  show a small fraction of the dynamics of neurons and synapses
                  in the brain. We include neurons in our RSNN model that
                  reproduce one prominent dynamical process of biological
                  neurons that takes place at the behaviourally relevant time
                  scale of seconds: neuronal adaptation. We denote these
                  networks as LSNNs because of their Long short-term memory. The
                  inclusion of adapting neurons drastically increases the
                  computing and learning capability of RSNNs if they are trained
                  and configured by deep learning (BPTT combined with a rewiring
                  algorithm that optimizes the network architecture). In fact,
                  the computational performance of these RSNNs approaches for
                  the first time that of LSTM networks. In addition RSNNs with
                  adapting neurons can acquire abstract knowledge from prior
                  learning in a Learning-to-Learn (L2L) scheme, and transfer
                  that knowledge in order to learn new but related tasks from
                  very few examples. We demonstrate this for supervised learning
                  and reinforcement learning.},
  archivePrefix   = {arXiv},
  eprint          = {1803.09574},
  primaryClass    = {cs.NE},
}

@book{bishop2006pattern,
  title           = {Pattern recognition and machine learning},
  author          = {Bishop, Christopher M},
  year            = 2006,
  publisher       = {springer}
}

@article{borman2004expectation,
  title           = {The expectation maximization algorithm-a short tutorial},
  author          = {Borman, Sean},
  journal         = {Submitted for publication},
  volume          = 41,
  year            = 2004
}

@article{brown18_machin_teach_inver_reinf_learn,
  author          = {Brown, Daniel S. and Niekum, Scott},
  title           = {Machine Teaching for Inverse Reinforcement Learning:
                  Algorithms and Applications},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1805.07687v6},
  abstract        = {Inverse reinforcement learning (IRL) infers a reward
                  function from demonstrations, allowing for policy improvement
                  and generalization. However, despite much recent interest in
                  IRL, little work has been done to understand the minimum set
                  of demonstrations needed to teach a specific sequential
                  decision-making task. We formalize the problem of finding
                  maximally informative demonstrations for IRL as a machine
                  teaching problem where the goal is to find the minimum number
                  of demonstrations needed to specify the reward equivalence
                  class of the demonstrator. We extend previous work on
                  algorithmic teaching for sequential decision-making tasks by
                  showing a reduction to the set cover problem which enables an
                  efficient approximation algorithm for determining the set of
                  maximally-informative demonstrations. We apply our proposed
                  machine teaching algorithm to two novel applications:
                  providing a lower bound on the number of queries needed to
                  learn a policy using active IRL and developing a novel IRL
                  algorithm that can learn more efficiently from informative
                  demonstrations than a standard IRL approach.},
  archivePrefix   = {arXiv},
  eprint          = {1805.07687},
  primaryClass    = {cs.LG},
}

@misc{buckar_c_packag_manag_missin,
  author          = {Buckaroo},
  howpublished    = {https://buckaroo.pm/posts/value-ptr-the-missing-smart-ptr/},
  note            = {Online; accessed 24 February 2019},
  title           = {Buckaroo - The C++ Package Manager value_ptr - The Missing
                  C++ Smart-pointer},
  year            = 2019,
}

@article{camerer2004cognitive,
  title           = {A cognitive hierarchy model of games},
  author          = {Camerer, Colin F and Ho, Teck-Hua and Chong, Juin-Kuan},
  journal         = {The Quarterly Journal of Economics},
  volume          = 119,
  number          = 3,
  pages           = {861--898},
  year            = 2004,
  publisher       = {MIT Press}
}

@book{chatterjee06_regres_analy_examp,
  author          = {Samprit Chatterjee and Ali S. Hadi},
  title           = {Regression Analysis by Example},
  year            = 2006,
  publisher       = {John Wiley \& Sons, Inc.},
  url             = {https://doi.org/10.1002/0470055464},
  DATE_ADDED      = {Tue Jan 15 13:13:16 2019},
  doi             = {10.1002/0470055464},
  pages           = {nil},
  series          = {Wiley Series in Probability and Statistics},
}

@article{chen16_xgboos,
  author          = {Chen, Tianqi and Guestrin, Carlos},
  title           = {Xgboost: a Scalable Tree Boosting System},
  journal         = {CoRR},
  year            = 2016,
  url             = {http://arxiv.org/abs/1603.02754v3},
  abstract        = {Tree boosting is a highly effective and widely used machine
                  learning method. In this paper, we describe a scalable
                  end-to-end tree boosting system called XGBoost, which is used
                  widely by data scientists to achieve state-of-the-art results
                  on many machine learning challenges. We propose a novel
                  sparsity-aware algorithm for sparse data and weighted quantile
                  sketch for approximate tree learning. More importantly, we
                  provide insights on cache access patterns, data compression
                  and sharding to build a scalable tree boosting system. By
                  combining these insights, XGBoost scales beyond billions of
                  examples using far fewer resources than existing systems.},
  archivePrefix   = {arXiv},
  eprint          = {1603.02754},
  primaryClass    = {cs.LG},
}

@article{chen18_neural_ordin_differ_equat,
  author          = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt,
                  Jesse and Duvenaud, David},
  title           = {Neural Ordinary Differential Equations},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1806.07366v4},
  abstract        = {We introduce a new family of deep neural network models.
                  Instead of specifying a discrete sequence of hidden layers, we
                  parameterize the derivative of the hidden state using a neural
                  network. The output of the network is computed using a
                  black-box differential equation solver. These continuous-depth
                  models have constant memory cost, adapt their evaluation
                  strategy to each input, and can explicitly trade numerical
                  precision for speed. We demonstrate these properties in
                  continuous-depth residual networks and continuous-time latent
                  variable models. We also construct continuous normalizing
                  flows, a generative model that can train by maximum
                  likelihood, without partitioning or ordering the data
                  dimensions. For training, we show how to scalably
                  backpropagate through any ODE solver, without access to its
                  internal operations. This allows end-to-end training of ODEs
                  within larger models.},
  archivePrefix   = {arXiv},
  eprint          = {1806.07366},
  primaryClass    = {cs.LG},
}

@article{chen18_neural_ordin_differ_equat,
  author          = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt,
                  Jesse and Duvenaud, David},
  title           = {Neural Ordinary Differential Equations},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1806.07366v4},
  abstract        = {We introduce a new family of deep neural network models.
                  Instead of specifying a discrete sequence of hidden layers, we
                  parameterize the derivative of the hidden state using a neural
                  network. The output of the network is computed using a
                  black-box differential equation solver. These continuous-depth
                  models have constant memory cost, adapt their evaluation
                  strategy to each input, and can explicitly trade numerical
                  precision for speed. We demonstrate these properties in
                  continuous-depth residual networks and continuous-time latent
                  variable models. We also construct continuous normalizing
                  flows, a generative model that can train by maximum
                  likelihood, without partitioning or ordering the data
                  dimensions. For training, we show how to scalably
                  backpropagate through any ODE solver, without access to its
                  internal operations. This allows end-to-end training of ODEs
                  within larger models.},
  archivePrefix   = {arXiv},
  eprint          = {1806.07366},
  primaryClass    = {cs.LG},
}

@article{chen20_simpl_framew_contr_learn_visual_repres,
  author          = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and
                  Hinton, Geoffrey},
  title           = {A Simple Framework for Contrastive Learning of Visual
                  Representations},
  journal         = {CoRR},
  year            = 2020,
  url             = {http://arxiv.org/abs/2002.05709v1},
  abstract        = {This paper presents SimCLR: a simple framework for
                  contrastive learning of visual representations. We simplify
                  recently proposed contrastive self-supervised learning
                  algorithms without requiring specialized architectures or a
                  memory bank. In order to understand what enables the
                  contrastive prediction tasks to learn useful representations,
                  we systematically study the major components of our framework.
                  We show that (1) composition of data augmentations plays a
                  critical role in defining effective predictive tasks, (2)
                  introducing a learnable nonlinear transformation between the
                  representation and the contrastive loss substantially improves
                  the quality of the learned representations, and (3)
                  contrastive learning benefits from larger batch sizes and more
                  training steps compared to supervised learning. By combining
                  these findings, we are able to considerably outperform
                  previous methods for self-supervised and semi-supervised
                  learning on ImageNet. A linear classifier trained on
                  self-supervised representations learned by SimCLR achieves
                  76.5 \% top-1 accuracy, which is a 7 \% relative improvement
                  over previous state-of-the-art, matching the performance of a
                  supervised ResNet-50. When fine-tuned on only 1 \% of the
                  labels, we achieve 85.8 \% top-5 accuracy, outperforming
                  AlexNet with 100X fewer labels.},
  archivePrefix   = {arXiv},
  eprint          = {2002.05709},
  primaryClass    = {cs.LG},
}

@article{chen3rabit,
  title           = {RABIT: A Reliable Allreduce and Broadcast Interface},
  author          = {Chen, Tianqi and Cano, Ignacio and Zhou, Tianyi},
  journal         = {Transfer},
  volume          = 3,
  number          = 2
}

@misc{codesbay_codes_cplus_smart,
  author          = {CodesBay},
  howpublished    = {https://github.com/CodesBay/CplusPlus_SmartPointer},
  note            = {Online; accessed 25 February 2019},
  title           = {GitHub - CodesBay/CplusPlus_SmartPointer: This repository
                  contains description of C++11 and C++14 Smart Pointers Trilogy
                  of shared_ptr, unique_ptr and weak_ptr},
  year            = {nil},
}

@misc{colyer_tmp_neural_ode,
  author          = {Adrian Colyer},
  howpublished    =
                  {https://blog.acolyer.org/2019/01/09/neural-ordinary-differential-equations/},
  note            = {Online; accessed 28 February 2019},
  title           = {Neural Ordinary Differential Equations},
  year            = 2019,
}

@article{comsa19_tempor_codin_spikin_neural_networ,
  author          = {Comsa, Iulia M. and Potempa, Krzysztof and Versari, Luca
                  and Fischbacher, Thomas and Gesmundo, Andrea and Alakuijala,
                  Jyrki},
  title           = {Temporal Coding in Spiking Neural Networks With Alpha
                  Synaptic Function},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1907.13223v1},
  abstract        = {The timing of individual neuronal spikes is essential for
                  biological brains to make fast responses to sensory stimuli.
                  However, conventional artificial neural networks lack the
                  intrinsic temporal coding ability present in biological
                  networks. We propose a spiking neural network model that
                  encodes information in the relative timing of individual
                  neuron spikes. In classification tasks, the output of the
                  network is indicated by the first neuron to spike in the
                  output layer. This temporal coding scheme allows the
                  supervised training of the network with backpropagation, using
                  locally exact derivatives of the postsynaptic spike times with
                  respect to presynaptic spike times. The network operates using
                  a biologically-plausible alpha synaptic transfer function.
                  Additionally, we use trainable synchronisation pulses that
                  provide bias, add flexibility during training and exploit the
                  decay part of the alpha function. We show that such networks
                  can be trained successfully on noisy Boolean logic tasks and
                  on the MNIST dataset encoded in time. The results show that
                  the spiking neural network outperforms comparable spiking
                  models on MNIST and achieves similar quality to fully
                  connected conventional networks with the same architecture. We
                  also find that the spiking network spontaneously discovers two
                  operating regimes, mirroring the accuracy-speed trade-off
                  observed in human decision-making: a slow regime, where a
                  decision is taken after all hidden neurons have spiked and the
                  accuracy is very high, and a fast regime, where a decision is
                  taken very fast but the accuracy is lower. These results
                  demonstrate the computational power of spiking networks with
                  biological characteristics that encode information in the
                  timing of individual neurons. By studying temporal coding in
                  spiking networks, we aim to create building blocks towards
                  energy-efficient and more complex biologically-inspired neural
                  architectures.},
  archivePrefix   = {arXiv},
  eprint          = {1907.13223},
  primaryClass    = {cs.NE},
}

@article{comsa19_tempor_codin_spikin_neural_networ,
  author          = {Comsa, Iulia M. and Potempa, Krzysztof and Versari, Luca
                  and Fischbacher, Thomas and Gesmundo, Andrea and Alakuijala,
                  Jyrki},
  title           = {Temporal Coding in Spiking Neural Networks With Alpha
                  Synaptic Function},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1907.13223v2},
  abstract        = {The timing of individual neuronal spikes is essential for
                  biological brains to make fast responses to sensory stimuli.
                  However, conventional artificial neural networks lack the
                  intrinsic temporal coding ability present in biological
                  networks. We propose a spiking neural network model that
                  encodes information in the relative timing of individual
                  neuron spikes. In classification tasks, the output of the
                  network is indicated by the first neuron to spike in the
                  output layer. This temporal coding scheme allows the
                  supervised training of the network with backpropagation, using
                  locally exact derivatives of the postsynaptic spike times with
                  respect to presynaptic spike times. The network operates using
                  a biologically-plausible alpha synaptic transfer function.
                  Additionally, we use trainable synchronisation pulses that
                  provide bias, add flexibility during training and exploit the
                  decay part of the alpha function. We show that such networks
                  can be trained successfully on noisy Boolean logic tasks and
                  on the MNIST dataset encoded in time. The results show that
                  the spiking neural network outperforms comparable spiking
                  models on MNIST and achieves similar quality to fully
                  connected conventional networks with the same architecture. We
                  also find that the spiking network spontaneously discovers two
                  operating regimes, mirroring the accuracy-speed trade-off
                  observed in human decision-making: a slow regime, where a
                  decision is taken after all hidden neurons have spiked and the
                  accuracy is very high, and a fast regime, where a decision is
                  taken very fast but the accuracy is lower. These results
                  demonstrate the computational power of spiking networks with
                  biological characteristics that encode information in the
                  timing of individual neurons. By studying temporal coding in
                  spiking networks, we aim to create building blocks towards
                  energy-efficient and more complex biologically-inspired neural
                  architectures.},
  archivePrefix   = {arXiv},
  eprint          = {1907.13223},
  primaryClass    = {cs.NE},
}

@misc{cppref_raii,
  author          = {nil},
  howpublished    = {https://en.cppreference.com/w/cpp/language/raii},
  note            = {Online; accessed 25 January 2019},
  title           = {RAII - cppreference.com},
  year            = {nil},
}

@inproceedings{dabney2018distributional,
  title           = {Distributional reinforcement learning with quantile
                  regression},
  author          = {Dabney, Will and Rowland, Mark and Bellemare, Marc G and
                  Munos, R{\'e}mi},
  booktitle       = {Thirty-Second AAAI Conference on Artificial Intelligence},
  year            = 2018
}

@article{dacrema19_are_we_reall_makin_much_progr,
  author          = {Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach,
                  Dietmar},
  title           = {Are We Really Making Much Progress? a Worrying Analysis of
                  Recent Neural Recommendation Approaches},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1907.06902v1},
  abstract        = {Deep learning techniques have become the method of choice
                  for researchers working on algorithmic aspects of recommender
                  systems. With the strongly increased interest in machine
                  learning in general, it has, as a result, become difficult to
                  keep track of what represents the state-of-the-art at the
                  moment, e.g., for top-n recommendation tasks. At the same
                  time, several recent publications point out problems in
                  today's research practice in applied machine learning, e.g.,
                  in terms of the reproducibility of the results or the choice
                  of the baselines when proposing new models. In this work, we
                  report the results of a systematic analysis of algorithmic
                  proposals for top-n recommendation tasks. Specifically, we
                  considered 18 algorithms that were presented at top-level
                  research conferences in the last years. Only 7 of them could
                  be reproduced with reasonable effort. For these methods, it
                  however turned out that 6 of them can often be outperformed
                  with comparably simple heuristic methods, e.g., based on
                  nearest-neighbor or graph-based techniques. The remaining one
                  clearly outperformed the baselines but did not consistently
                  outperform a well-tuned non-neural linear ranking method.
                  Overall, our work sheds light on a number of potential
                  problems in today's machine learning scholarship and calls for
                  improved scientific practices in this area. Source code of our
                  experiments and full results are available at:
                  https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation.},
  archivePrefix   = {arXiv},
  eprint          = {1907.06902v1},
  primaryClass    = {cs.IR},
}

@misc{dan_reprod,
  author          = {Dan Frank},
  howpublished    = {https://stripe.com/blog/reproducible-research},
  note            = {Online; accessed 06 January 2019},
  title           = {Reproducible research: Stripe's approach to data science},
  year            = 2016,
}

@inproceedings{dann2017unifying,
  title           = {Unifying PAC and regret: Uniform PAC bounds for episodic
                  reinforcement learning},
  author          = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  booktitle       = {Advances in Neural Information Processing Systems},
  pages           = {5713--5723},
  year            = 2017
}

@article{davies2018loihi,
  title           = {Loihi: A neuromorphic manycore processor with on-chip
                  learning},
  author          = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and
                  Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and
                  Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain,
                  Shweta and others},
  journal         = {IEEE Micro},
  volume          = 38,
  number          = 1,
  pages           = {82--99},
  year            = 2018,
  publisher       = {IEEE}
}

@inproceedings{dremel,
  title           = {Dremel: Interactive Analysis of Web-Scale Datasets},
  author          = {Sergey Melnik and Andrey Gubarev and Jing Jing Long and
                  Geoffrey Romer and Shiva Shivakumar and Matt Tolton and Theo
                  Vassilakis},
  year            = 2010,
  URL             = {http://www.vldb2010.org/accept.htm},
  booktitle       = {Proc. of the 36th Int'l Conf on Very Large Data Bases},
  pages           = {330-339}
}

@article{drepper2007every,
  title           = {What every programmer should know about memory},
  author          = {Drepper, Ulrich},
  year            = 2007
}

@article{dupont19_augmen_neural_odes,
  author          = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
  title           = {Augmented Neural Odes},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1904.01681v1},
  abstract        = {We show that Neural Ordinary Differential Equations (ODEs)
                  learn representations that preserve the topology of the input
                  space and prove that this implies the existence of functions
                  Neural ODEs cannot represent. To address these limitations, we
                  introduce Augmented Neural ODEs which, in addition to being
                  more expressive models, are empirically more stable,
                  generalize better and have a lower computational cost than
                  Neural ODEs.},
  archivePrefix   = {arXiv},
  eprint          = {1904.01681},
  primaryClass    = {stat.ML},
}

@article{espeholt18_impal,
  author          = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and
                  Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron,
                  Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and
                  Legg, Shane and Kavukcuoglu, Koray},
  title           = {Impala: Scalable Distributed Deep-Rl With Importance
                  Weighted Actor-Learner Architectures},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1802.01561v3},
  abstract        = {In this work we aim to solve a large collection of tasks
                  using a single reinforcement learning agent with a single set
                  of parameters. A key challenge is to handle the increased
                  amount of data and extended training time. We have developed a
                  new distributed agent IMPALA (Importance Weighted
                  Actor-Learner Architecture) that not only uses resources more
                  efficiently in single-machine training but also scales to
                  thousands of machines without sacrificing data efficiency or
                  resource utilisation. We achieve stable learning at high
                  throughput by combining decoupled acting and learning with a
                  novel off-policy correction method called V-trace. We
                  demonstrate the effectiveness of IMPALA for multi-task
                  reinforcement learning on DMLab-30 (a set of 30 tasks from the
                  DeepMind Lab environment (Beattie et al., 2016)) and Atari-57
                  (all available Atari games in Arcade Learning Environment
                  (Bellemare et al., 2013a)). Our results show that IMPALA is
                  able to achieve better performance than previous agents with
                  less data, and crucially exhibits positive transfer between
                  tasks as a result of its multi-task approach.},
  archivePrefix   = {arXiv},
  eprint          = {1802.01561},
  primaryClass    = {cs.LG},
}

@article{eysenbach18_diver_is_all_you_need,
  author          = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian
                  and Levine, Sergey},
  title           = {Diversity Is All You Need: Learning Skills Without a Reward
                  Function},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1802.06070v6},
  abstract        = {Intelligent creatures can explore their environments and
                  learn useful skills without supervision. In this paper, we
                  propose DIAYN ('Diversity is All You Need'), a method for
                  learning useful skills without a reward function. Our proposed
                  method learns skills by maximizing an information theoretic
                  objective using a maximum entropy policy. On a variety of
                  simulated robotic tasks, we show that this simple objective
                  results in the unsupervised emergence of diverse skills, such
                  as walking and jumping. In a number of reinforcement learning
                  benchmark environments, our method is able to learn a skill
                  that solves the benchmark task despite never receiving the
                  true task reward. We show how pretrained skills can provide a
                  good parameter initialization for downstream tasks, and can be
                  composed hierarchically to solve complex, sparse reward tasks.
                  Our results suggest that unsupervised discovery of skills can
                  serve as an effective pretraining mechanism for overcoming
                  challenges of exploration and data efficiency in reinforcement
                  learning.},
  archivePrefix   = {arXiv},
  eprint          = {1802.06070},
  primaryClass    = {cs.AI},
}

@misc{felipe_demys_join_algor,
  author          = {Felipe Oliveira Carvalho},
  howpublished    =
                  {http://blog.felipe.rs/2019/01/29/demystifying-join-algorithms/},
  note            = {Online; accessed 03 February 2019},
  title           = {Demystifying JOIN Algorithms},
  year            = 2019,
}

@article{florian07_reinf_learn_throug_modul_spike,
  author          = {Răzvan V. Florian},
  title           = {Reinforcement Learning Through Modulation of
                  Spike-Timing-Dependent Synaptic Plasticity},
  journal         = {Neural Computation},
  volume          = 19,
  number          = 6,
  pages           = {1468-1502},
  year            = 2007,
  doi             = {10.1162/neco.2007.19.6.1468},
  url             = {https://doi.org/10.1162/neco.2007.19.6.1468},
  DATE_ADDED      = {Mon Nov 4 15:30:29 2019},
}

@inproceedings{florian2005,
  author          = {Florian, Răzvan},
  year            = 2005,
  month           = 10,
  pages           = {8 pp.-},
  title           = {A reinforcement learning algorithm for spiking neural
                  networks},
  volume          = 2005,
  isbn            = {0-7695-2453-2},
  journal         = {Proceedings - Seventh International Symposium on Symbolic
                  and Numeric Algorithms for Scientific Computing, SYNASC 2005},
  doi             = {10.1109/SYNASC.2005.13}
}

@misc{gary_networ_protoc,
  author          = {Gary Bernhardt},
  howpublished    =
                  {https://www.destroyallsoftware.com/compendium/network-protocols?share_key=97d3ba4c24d21147},
  note            = {Online; accessed 25 January 2019},
  title           = {Network Protocols &ndash; Programmer's Compendium},
  year            = 2019,
}

@book{gerstner2002spiking,
  title           = {Spiking neuron models: Single neurons, populations,
                  plasticity},
  author          = {Gerstner, Wulfram and Kistler, Werner M},
  year            = 2002,
  publisher       = {Cambridge university press}
}

@misc{gibson18_neural_ordin_differ_equat,
  author          = {Kevin Gibson},
  howpublished    =
                  {https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/},
  note            = {Online; accessed 28 February 2019},
  title           = {Neural networks as Ordinary Differential Equations},
  year            = 2018,
}

@inproceedings{googlecartographer,
  title           = {Real-Time Loop Closure in 2D LIDAR SLAM},
  author          = {Wolfgang Hess and Damon Kohler and Holger Rapp and Daniel
                  Andor},
  year            = 2016,
  booktitle       = {2016 IEEE International Conference on Robotics and
                  Automation (ICRA)},
  pages           = {1271--1278}
}

@article{gu16_q_prop,
  author          = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin
                  and Turner, Richard E. and Levine, Sergey},
  title           = {Q-Prop: Sample-Efficient Policy Gradient With an Off-Policy
                  Critic},
  journal         = {CoRR},
  year            = 2016,
  url             = {http://arxiv.org/abs/1611.02247v3},
  abstract        = {Model-free deep reinforcement learning (RL) methods have
                  been successful in a wide variety of simulated domains.
                  However, a major obstacle facing deep RL in the real world is
                  their high sample complexity. Batch policy gradient methods
                  offer stable learning, but at the cost of high variance, which
                  often requires large batches. TD-style methods, such as
                  off-policy actor-critic and Q-learning, are more
                  sample-efficient but biased, and often require costly
                  hyperparameter sweeps to stabilize. In this work, we aim to
                  develop methods that combine the stability of policy gradients
                  with the efficiency of off-policy RL. We present Q-Prop, a
                  policy gradient method that uses a Taylor expansion of the
                  off-policy critic as a control variate. Q-Prop is both sample
                  efficient and stable, and effectively combines the benefits of
                  on-policy and off-policy methods. We analyze the connection
                  between Q-Prop and existing model-free algorithms, and use
                  control variate theory to derive two variants of Q-Prop with
                  conservative and aggressive adaptation. We show that
                  conservative Q-Prop provides substantial gains in sample
                  efficiency over trust region policy optimization (TRPO) with
                  generalized advantage estimation (GAE), and improves stability
                  over deep deterministic policy gradient (DDPG), the
                  state-of-the-art on-policy and off-policy methods, on OpenAI
                  Gym's MuJoCo continuous control environments.},
  archivePrefix   = {arXiv},
  eprint          = {1611.02247},
  primaryClass    = {cs.LG},
}

@article{guetig14_to_spike_or_when_to_spike,
  author          = {Robert G{\"u}tig},
  title           = {To Spike, Or When To Spike?},
  journal         = {Current Opinion in Neurobiology},
  volume          = 25,
  number          = {nil},
  pages           = {134-139},
  year            = 2014,
  doi             = {10.1016/j.conb.2014.01.004},
  url             = {https://doi.org/10.1016/j.conb.2014.01.004},
  DATE_ADDED      = {Fri Nov 1 16:23:04 2019},
}

@article{guo17_deepf,
  author          = {Guo, Huifeng and Tang, Ruiming and Ye, Yunming and Li,
                  Zhenguo and He, Xiuqiang},
  title           = {Deepfm: a Factorization-Machine Based Neural Network for
                  Ctr Prediction},
  journal         = {CoRR},
  year            = 2017,
  url             = {http://arxiv.org/abs/1703.04247v1},
  abstract        = {Learning sophisticated feature interactions behind user
                  behaviors is critical in maximizing CTR for recommender
                  systems. Despite great progress, existing methods seem to have
                  a strong bias towards low- or high-order interactions, or
                  require expertise feature engineering. In this paper, we show
                  that it is possible to derive an end-to-end learning model
                  that emphasizes both low- and high-order feature interactions.
                  The proposed model, DeepFM, combines the power of
                  factorization machines for recommendation and deep learning
                  for feature learning in a new neural network architecture.
                  Compared to the latest Wide \& Deep model from Google, DeepFM
                  has a shared input to its "wide" and "deep" parts, with no
                  need of feature engineering besides raw features.
                  Comprehensive experiments are conducted to demonstrate the
                  effectiveness and efficiency of DeepFM over the existing
                  models for CTR prediction, on both benchmark data and
                  commercial data.},
  archivePrefix   = {arXiv},
  eprint          = {1703.04247},
  primaryClass    = {cs.IR},
}

@article{haan19_causal_confus_imitat_learn,
  author          = {Haan, Pim de and Jayaraman, Dinesh and Levine, Sergey},
  title           = {Causal Confusion in Imitation Learning},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1905.11979v2},
  abstract        = {Behavioral cloning reduces policy learning to supervised
                  learning by training a discriminative model to predict expert
                  actions given observations. Such discriminative models are
                  non-causal: the training procedure is unaware of the causal
                  structure of the interaction between the expert and the
                  environment. We point out that ignoring causality is
                  particularly damaging because of the distributional shift in
                  imitation learning. In particular, it leads to a
                  counter-intuitive "causal misidentification" phenomenon:
                  access to more information can yield worse performance. We
                  investigate how this problem arises, and propose a solution to
                  combat it through targeted interventions---either environment
                  interaction or expert queries---to determine the correct
                  causal model. We show that causal misidentification occurs in
                  several benchmark control domains as well as realistic driving
                  settings, and validate our solution against DAgger and other
                  baselines and ablations.},
  archivePrefix   = {arXiv},
  eprint          = {1905.11979},
  primaryClass    = {cs.LG},
}

@article{haber17_stabl_archit_deep_neural_networ,
  author          = {Haber, Eldad and Ruthotto, Lars},
  title           = {Stable Architectures for Deep Neural Networks},
  journal         = {CoRR},
  year            = 2017,
  url             = {http://arxiv.org/abs/1705.03341v3},
  abstract        = {Deep neural networks have become invaluable tools for
                  supervised machine learning, e.g., classification of text or
                  images. While often offering superior results over traditional
                  techniques and successfully expressing complicated patterns in
                  data, deep architectures are known to be challenging to design
                  and train such that they generalize well to new data.
                  Important issues with deep architectures are numerical
                  instabilities in derivative-based learning algorithms commonly
                  called exploding or vanishing gradients. In this paper we
                  propose new forward propagation techniques inspired by systems
                  of Ordinary Differential Equations (ODE) that overcome this
                  challenge and lead to well-posed learning problems for
                  arbitrarily deep networks. The backbone of our approach is our
                  interpretation of deep learning as a parameter estimation
                  problem of nonlinear dynamical systems. Given this
                  formulation, we analyze stability and well-posedness of deep
                  learning and use this new understanding to develop new network
                  architectures. We relate the exploding and vanishing gradient
                  phenomenon to the stability of the discrete ODE and present
                  several strategies for stabilizing deep learning for very deep
                  networks. While our new architectures restrict the solution
                  space, several numerical experiments show their
                  competitiveness with state-of-the-art networks.},
  archivePrefix   = {arXiv},
  eprint          = {1705.03341},
  primaryClass    = {cs.LG},
}

@misc{handmade_how_to_write_better,
  author          = {Handmade},
  howpublished    =
                  {https://handmade.network/wiki/7138-how_to_write_better_game_libraries},
  note            = {Online; accessed 12 December 2019},
  title           = {How to write better (game) libraries | handmade.network
                  Wiki},
  year            = 2019,
}

@article{haug18_teach_inver_reinf_learn_via_featur_demon,
  author          = {Haug, Luis and Tschiatschek, Sebastian and Singla, Adish},
  title           = {Teaching Inverse Reinforcement Learners Via Features and
                  Demonstrations},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1810.08926v4},
  abstract        = {Learning near-optimal behaviour from an expert's
                  demonstrations typically relies on the assumption that the
                  learner knows the features that the true reward function
                  depends on. In this paper, we study the problem of learning
                  from demonstrations in the setting where this is not the case,
                  i.e., where there is a mismatch between the worldviews of the
                  learner and the expert. We introduce a natural quantity, the
                  teaching risk, which measures the potential suboptimality of
                  policies that look optimal to the learner in this setting. We
                  show that bounds on the teaching risk guarantee that the
                  learner is able to find a near-optimal policy using standard
                  algorithms based on inverse reinforcement learning. Based on
                  these findings, we suggest a teaching scheme in which the
                  expert can decrease the teaching risk by updating the
                  learner's worldview, and thus ultimately enable her to find a
                  near-optimal policy.},
  archivePrefix   = {arXiv},
  eprint          = {1810.08926},
  primaryClass    = {cs.LG},
}

@article{he15_deep_resid_learn_image_recog,
  author          = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
                  Jian},
  title           = {Deep Residual Learning for Image Recognition},
  journal         = {CoRR},
  year            = 2015,
  url             = {http://arxiv.org/abs/1512.03385v1},
  abstract        = {Deeper neural networks are more difficult to train. We
                  present a residual learning framework to ease the training of
                  networks that are substantially deeper than those used
                  previously. We explicitly reformulate the layers as learning
                  residual functions with reference to the layer inputs, instead
                  of learning unreferenced functions. We provide comprehensive
                  empirical evidence showing that these residual networks are
                  easier to optimize, and can gain accuracy from considerably
                  increased depth. On the ImageNet dataset we evaluate residual
                  nets with a depth of up to 152 layers---8x deeper than VGG
                  nets but still having lower complexity. An ensemble of these
                  residual nets achieves 3.57 \% error on the ImageNet test set.
                  This result won the 1st place on the ILSVRC 2015
                  classification task. We also present analysis on CIFAR-10 with
                  100 and 1000 layers. The depth of representations is of
                  central importance for many visual recognition tasks. Solely
                  due to our extremely deep representations, we obtain a 28 \%
                  relative improvement on the COCO object detection dataset.
                  Deep residual nets are foundations of our submissions to
                  ILSVRC \& COCO 2015 competitions, where we also won the 1st
                  places on the tasks of ImageNet detection, ImageNet
                  localization, COCO detection, and COCO segmentation.},
  archivePrefix   = {arXiv},
  eprint          = {1512.03385},
  primaryClass    = {cs.CV},
}

@article{heeger2000poisson,
  title           = {Poisson model of spike generation},
  author          = {Heeger, David},
  journal         = {Handout, University of Standford},
  volume          = 5,
  pages           = {1--13},
  year            = 2000
}

@article{heeger2000poisson,
  title           = {Poisson model of spike generation},
  author          = {Heeger, David},
  journal         = {Handout, University of Standford},
  volume          = 5,
  pages           = {1--13},
  year            = 2000
}

@misc{home_cookiec_data_scien,
  author          = {DrivenData},
  howpublished    = {https://drivendata.github.io/cookiecutter-data-science/},
  note            = {Online; accessed 06 January 2019},
  title           = {Home - Cookiecutter Data Science},
  year            = 2019,
}

@misc{home_keras_docum,
  author          = {Keras},
  howpublished    = {https://keras.io/},
  note            = {Online; accessed 08 January 2019},
  title           = {Home Keras Documentation},
  year            = 2019,
}

@article{horgan18_distr_prior_exper_replay,
  author          = {Horgan, Dan and Quan, John and Budden, David and
                  Barth-Maron, Gabriel and Hessel, Matteo and Hasselt, Hado van
                  and Silver, David},
  title           = {Distributed Prioritized Experience Replay},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1803.00933v1},
  abstract        = {We propose a distributed architecture for deep
                  reinforcement learning at scale, that enables agents to learn
                  effectively from orders of magnitude more data than previously
                  possible. The algorithm decouples acting from learning: the
                  actors interact with their own instances of the environment by
                  selecting actions according to a shared neural network, and
                  accumulate the resulting experience in a shared experience
                  replay memory; the learner replays samples of experience and
                  updates the neural network. The architecture relies on
                  prioritized experience replay to focus only on the most
                  significant data generated by the actors. Our architecture
                  substantially improves the state of the art on the Arcade
                  Learning Environment, achieving better final performance in a
                  fraction of the wall-clock training time.},
  archivePrefix   = {arXiv},
  eprint          = {1803.00933},
  primaryClass    = {cs.LG},
}

@misc{houghton18_calcul_mutual_infor_between_two_spike_train,
  DATE_ADDED      = {Thu Jan 23 11:52:27 2020},
  author          = {Conor Houghton},
  doi             = {10.1101/423608},
  title           = {Calculating the Mutual Information Between Two Spike
                  Trains},
  url             = {https://doi.org/10.1101/423608},
  year            = 2018,
}

@article{huang18_gpipe,
  author          = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and
                  Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee,
                  HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui
                  and Chen, Zhifeng},
  title           = {Gpipe: Efficient Training of Giant Neural Networks Using
                  Pipeline Parallelism},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1811.06965v5},
  abstract        = {Scaling up deep neural network capacity has been known as
                  an effective approach to improving model quality for several
                  different machine learning tasks. In many cases, increasing
                  model capacity beyond the memory limit of a single accelerator
                  has required developing special algorithms or infrastructure.
                  These solutions are often architecture-specific and do not
                  transfer to other tasks. To address the need for efficient and
                  task-independent model parallelism, we introduce GPipe, a
                  pipeline parallelism library that allows scaling any network
                  that can be expressed as a sequence of layers. By pipelining
                  different sub-sequences of layers on separate accelerators,
                  GPipe provides the flexibility of scaling a variety of
                  different networks to gigantic sizes efficiently. Moreover,
                  GPipe utilizes a novel batch-splitting pipelining algorithm,
                  resulting in almost linear speedup when a model is partitioned
                  across multiple accelerators. We demonstrate the advantages of
                  GPipe by training large-scale neural networks on two different
                  tasks with distinct network architectures: (i) Image
                  Classification: We train a 557-million-parameter AmoebaNet
                  model and attain a top-1 accuracy of 84.4 \% on ImageNet-2012,
                  (ii) Multilingual Neural Machine Translation: We train a
                  single 6-billion-parameter, 128-layer Transformer model on a
                  corpus spanning over 100 languages and achieve better quality
                  than all bilingual models.},
  archivePrefix   = {arXiv},
  eprint          = {1811.06965},
  primaryClass    = {cs.CV},
}

@article{huh17_gradien_descen_spikin_neural_networ,
  author          = {Huh, Dongsung and Sejnowski, Terrence J.},
  title           = {Gradient Descent for Spiking Neural Networks},
  journal         = {CoRR},
  year            = 2017,
  url             = {http://arxiv.org/abs/1706.04698v2},
  abstract        = {Much of studies on neural computation are based on network
                  models of static neurons that produce analog output, despite
                  the fact that information processing in the brain is
                  predominantly carried out by dynamic neurons that produce
                  discrete pulses called spikes. Research in spike-based
                  computation has been impeded by the lack of efficient
                  supervised learning algorithm for spiking networks. Here, we
                  present a gradient descent method for optimizing spiking
                  network models by introducing a differentiable formulation of
                  spiking networks and deriving the exact gradient calculation.
                  For demonstration, we trained recurrent spiking networks on
                  two dynamic tasks: one that requires optimizing fast
                  (~millisecond) spike-based interactions for efficient encoding
                  of information, and a delayed memory XOR task over extended
                  duration (~second). The results show that our method indeed
                  optimizes the spiking network dynamics on the time scale of
                  individual spikes as well as behavioral time scales. In
                  conclusion, our result offers a general purpose supervised
                  learning algorithm for spiking neural networks, thus advancing
                  further investigations on spike-based computation.},
  archivePrefix   = {arXiv},
  eprint          = {1706.04698},
  primaryClass    = {q-bio.NC},
}

@article{ivanov19_moder_deep_reinf_learn_algor,
  author          = {Ivanov, Sergey and D'yakonov, Alexander},
  title           = {Modern Deep Reinforcement Learning Algorithms},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1906.10025v2},
  abstract        = {Recent advances in Reinforcement Learning, grounded on
                  combining classical theoretical results with Deep Learning
                  paradigm, led to breakthroughs in many artificial intelligence
                  tasks and gave birth to Deep Reinforcement Learning (DRL) as a
                  field of research. In this work latest DRL algorithms are
                  reviewed with a focus on their theoretical justification,
                  practical limitations and observed empirical properties.},
  archivePrefix   = {arXiv},
  eprint          = {1906.10025},
  primaryClass    = {cs.LG},
}

@book{james2013introduction,
  title           = {An introduction to statistical learning},
  author          = {James, Gareth and Witten, Daniela and Hastie, Trevor and
                  Tibshirani, Robert},
  volume          = 112,
  year            = 2013,
  publisher       = {Springer}
}

@article{jang18_introd_to_spikin_neural_networ,
  author          = {Jang, Hyeryung and Simeone, Osvaldo and Gardner, Brian and
                  Gr{\"u}ning, Andr{\'e}},
  title           = {An Introduction To Spiking Neural Networks: Probabilistic
                  Models, Learning Rules, and Applications},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1812.03929v4},
  abstract        = {Spiking Neural Networks (SNNs) are distributed trainable
                  systems whose computing elements, or neurons, are
                  characterized by internal analog dynamics and by digital and
                  sparse synaptic communications. The sparsity of the synaptic
                  spiking inputs and the corresponding event-driven nature of
                  neural processing can be leveraged by hardware implementations
                  that have demonstrated significant energy reductions as
                  compared to conventional Artificial Neural Networks (ANNs).
                  Most existing training algorithms for SNNs have been designed
                  either for biological plausibility or through conversion from
                  pre-trained ANNs via rate encoding. This paper aims at
                  providing an introduction to SNNs by focusing on a
                  probabilistic signal processing methodology that enables the
                  direct derivation of learning rules leveraging the unique time
                  encoding capabilities of SNNs. To this end, the paper adopts
                  discrete-time probabilistic models for networked spiking
                  neurons, and it derives supervised and unsupervised learning
                  rules from first principles by using variational inference.
                  Examples and open research problems are also provided.},
  archivePrefix   = {arXiv},
  eprint          = {1812.03929v4},
  primaryClass    = {eess.SP},
}

@incollection{jin_q_learning_provably_efficient,
  title           = {Is Q-Learning Provably Efficient?},
  author          = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and
                  Jordan, Michael I},
  booktitle       = {Advances in Neural Information Processing Systems 31},
  editor          = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman
                  and N. Cesa-Bianchi and R. Garnett},
  pages           = {4863--4873},
  year            = 2018,
  publisher       = {Curran Associates, Inc.},
  url             =
                  {http://papers.nips.cc/paper/7735-is-q-learning-provably-efficient.pdf}
}

@article{johndrow15_optim_approx_markov_chain_bayes_infer,
  author          = {Johndrow, James E. and Mattingly, Jonathan C. and
                  Mukherjee, Sayan and Dunson, David},
  title           = {Optimal Approximating Markov Chains for Bayesian Inference},
  journal         = {CoRR},
  year            = 2015,
  url             = {http://arxiv.org/abs/1508.03387v3},
  abstract        = {The Markov Chain Monte Carlo method is the dominant
                  paradigm for posterior computation in Bayesian analysis. It is
                  common to control computation time by making approximations to
                  the Markov transition kernel. Comparatively little attention
                  has been paid to computational optimality in these
                  approximating Markov Chains, or when such approximations are
                  justified relative to obtaining shorter paths from the exact
                  kernel. We give simple, sharp bounds for uniform
                  approximations of uniformly mixing Markov chains. We then
                  suggest a notion of optimality that incorporates computation
                  time and approximation error, and use our bounds to make
                  generalizations about properties of good approximations in the
                  uniformly mixing setting. The relevance of these properties is
                  demonstrated in applications to a minibatching-based
                  approximate MCMC algorithm for large $n$ logistic regression
                  and low-rank approximations for Gaussian processes.},
  archivePrefix   = {arXiv},
  eprint          = {1508.03387},
  primaryClass    = {stat.CO},
}

@article{johndrow17_bayes_shrin_at_gwas_scale,
  author          = {Johndrow, James E. and Orenstein, Paulo and Bhattacharya,
                  Anirban},
  title           = {Bayes Shrinkage At Gwas Scale: Convergence and
                  Approximation Theory of a Scalable Mcmc Algorithm for the
                  Horseshoe Prior},
  journal         = {CoRR},
  year            = 2017,
  url             = {http://arxiv.org/abs/1705.00841v3},
  abstract        = {The horseshoe prior is frequently employed in Bayesian
                  analysis of high-dimensional models, and has been shown to
                  achieve minimax optimal risk properties when the truth is
                  sparse. While optimization-based algorithms for the extremely
                  popular Lasso and elastic net procedures can scale to
                  dimension in the hundreds of thousands, algorithms for the
                  horseshoe that use Markov chain Monte Carlo (MCMC) for
                  computation are limited to problems an order of magnitude
                  smaller. This is due to high computational cost per step and
                  growth of the variance of time-averaging estimators as a
                  function of dimension. We propose two new MCMC algorithms for
                  computation in these models that have improved performance
                  compared to existing alternatives. One of the algorithms also
                  approximates an expensive matrix product to give orders of
                  magnitude speedup in high-dimensional applications. We prove
                  that the exact algorithm is geometrically ergodic, and give
                  guarantees for the accuracy of the approximate algorithm using
                  perturbation theory. Versions of the approximation algorithm
                  that gradually decrease the approximation error as the chain
                  extends are shown to be exact. The scalability of the
                  algorithm is illustrated in simulations with problem size as
                  large as $N=5,000$ observations and $p=50,000$ predictors, and
                  an application to a genome-wide association study with
                  $N=2,267$ and $p=98,385$. The empirical results also show that
                  the new algorithm yields estimates with lower mean squared
                  error, intervals with better coverage, and elucidates features
                  of the posterior that were often missed by previous algorithms
                  in high dimensions, including bimodality of posterior
                  marginals indicating uncertainty about which covariates belong
                  in the model.},
  archivePrefix   = {arXiv},
  eprint          = {1705.00841},
  primaryClass    = {stat.CO},
}

@article{kaelbling1996reinforcement,
  title           = {Reinforcement learning: A survey},
  author          = {Kaelbling, Leslie Pack and Littman, Michael L and Moore,
                  Andrew W},
  journal         = {Journal of artificial intelligence research},
  volume          = 4,
  pages           = {237--285},
  year            = 1996
}

@article{kamalaruban19_inter_teach_algor_inver_reinf_learn,
  author          = {Kamalaruban, Parameswaran and Devidze, Rati and Cevher,
                  Volkan and Singla, Adish},
  title           = {Interactive Teaching Algorithms for Inverse Reinforcement
                  Learning},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1905.11867v3},
  abstract        = {We study the problem of inverse reinforcement learning
                  (IRL) with the added twist that the learner is assisted by a
                  helpful teacher. More formally, we tackle the following
                  algorithmic question: How could a teacher provide an
                  informative sequence of demonstrations to an IRL learner to
                  speed up the learning process? We present an interactive
                  teaching framework where a teacher adaptively chooses the next
                  demonstration based on learner's current policy. In
                  particular, we design teaching algorithms for two concrete
                  settings: an omniscient setting where a teacher has full
                  knowledge about the learner's dynamics and a blackbox setting
                  where the teacher has minimal knowledge. Then, we study a
                  sequential variant of the popular MCE-IRL learner and prove
                  convergence guarantees of our teaching algorithm in the
                  omniscient setting. Extensive experiments with a car driving
                  simulator environment show that the learning progress can be
                  speeded up drastically as compared to an uninformative
                  teacher.},
  archivePrefix   = {arXiv},
  eprint          = {1905.11867},
  primaryClass    = {cs.LG},
}

@InProceedings{ klaus_greff-proc-scipy-2017,
  author          = { {K}laus {G}reff and {A}aron {K}lein and {M}artin
                  {C}hovanec and {F}rank {H}utter and {J}\"urgen {S}chmidhuber },
  title           = { {T}he {S}acred {I}nfrastructure for {C}omputational
                  {R}esearch },
  booktitle       = { {P}roceedings of the 16th {P}ython in {S}cience
                  {C}onference },
  pages           = { 49 - 56 },
  year            = { 2017 },
  editor          = { {K}aty {H}uff and {D}avid {L}ippa and {D}illon {N}iederhut
                  and {M} {P}acer },
  doi             = { 10.25080/shinma-7f4c6e7-008 }
}

@book{koller2009probabilistic,
  title           = {Probabilistic graphical models: principles and techniques},
  author          = {Koller, Daphne and Friedman, Nir and Bach, Francis},
  year            = 2009,
  publisher       = {MIT press}
}

@Book{lakdawalla18_curios,
  author          = {Lakdawalla, Emily},
  title           = {The design and engineering of Curiosity : how the Mars
                  Rover performs its job},
  year            = 2018,
  publisher       = {Springer},
  address         = {Cham, Switzerland},
  isbn            = 9783319681467,
}

@article{levine18_reinf_learn_contr_as_probab_infer,
  author          = {Levine, Sergey},
  title           = {Reinforcement Learning and Control As Probabilistic
                  Inference: Tutorial and Review},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1805.00909v3},
  abstract        = {The framework of reinforcement learning or optimal control
                  provides a mathematical formalization of intelligent decision
                  making that is powerful and broadly applicable. While the
                  general form of the reinforcement learning problem enables
                  effective reasoning about uncertainty, the connection between
                  reinforcement learning and inference in probabilistic models
                  is not immediately obvious. However, such a connection has
                  considerable value when it comes to algorithm design:
                  formalizing a problem as probabilistic inference in principle
                  allows us to bring to bear a wide array of approximate
                  inference tools, extend the model in flexible and powerful
                  ways, and reason about compositionality and partial
                  observability. In this article, we will discuss how a
                  generalization of the reinforcement learning or optimal
                  control problem, which is sometimes termed maximum entropy
                  reinforcement learning, is equivalent to exact probabilistic
                  inference in the case of deterministic dynamics, and
                  variational inference in the case of stochastic dynamics. We
                  will present a detailed derivation of this framework, overview
                  prior work that has drawn on this and related ideas to propose
                  new reinforcement learning and control algorithms, and
                  describe perspectives on future research.},
  archivePrefix   = {arXiv},
  eprint          = {1805.00909},
  primaryClass    = {cs.LG},
}

@article{li16_simpl_scalab_accur_poster_inter_estim,
  author          = {Li, Cheng and Srivastava, Sanvesh and Dunson, David B.},
  title           = {Simple, Scalable and Accurate Posterior Interval
                  Estimation},
  journal         = {CoRR},
  year            = 2016,
  url             = {http://arxiv.org/abs/1605.04029v2},
  abstract        = {There is a lack of simple and scalable algorithms for
                  uncertainty quantification. Bayesian methods quantify
                  uncertainty through posterior and predictive distributions,
                  but it is difficult to rapidly estimate summaries of these
                  distributions, such as quantiles and intervals. Variational
                  Bayes approximations are widely used, but may badly
                  underestimate posterior covariance. Typically, the focus of
                  Bayesian inference is on point and interval estimates for
                  one-dimensional functionals of interest. In small scale
                  problems, Markov chain Monte Carlo algorithms remain the gold
                  standard, but such algorithms face major problems in scaling
                  up to big data. Various modifications have been proposed based
                  on parallelization and approximations based on subsamples, but
                  such approaches are either highly complex or lack theoretical
                  support and/or good performance outside of narrow settings. We
                  propose a very simple and general posterior interval
                  estimation algorithm, which is based on running Markov chain
                  Monte Carlo in parallel for subsets of the data and averaging
                  quantiles estimated from each subset. We provide strong
                  theoretical guarantees and illustrate performance in several
                  applications.},
  archivePrefix   = {arXiv},
  eprint          = {1605.04029},
  primaryClass    = {stat.CO},
}

@article{li18_deep_reinf_learn,
  author          = {Li, Yuxi},
  title           = {Deep Reinforcement Learning},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1810.06339v1},
  abstract        = {We discuss deep reinforcement learning in an overview
                  style. We draw a big picture, filled with details. We discuss
                  six core elements, six important mechanisms, and twelve
                  applications, focusing on contemporary work, and in historical
                  contexts. We start with background of artificial intelligence,
                  machine learning, deep learning, and reinforcement learning
                  (RL), with resources. Next we discuss RL core elements,
                  including value function, policy, reward, model, exploration
                  vs. exploitation, and representation. Then we discuss
                  important mechanisms for RL, including attention and memory,
                  unsupervised learning, hierarchical RL, multi-agent RL,
                  relational RL, and learning to learn. After that, we discuss
                  RL applications, including games, robotics, natural language
                  processing (NLP), computer vision, finance, business
                  management, healthcare, education, energy, transportation,
                  computer systems, and, science, engineering, and art. Finally
                  we summarize briefly, discuss challenges and opportunities,
                  and close with an epilogue.},
  archivePrefix   = {arXiv},
  eprint          = {1810.06339},
  primaryClass    = {cs.LG},
}

@article{lian18_xdeep,
  author          = {Lian, Jianxun and Zhou, Xiaohuan and Zhang, Fuzheng and
                  Chen, Zhongxia and Xie, Xing and Sun, Guangzhong},
  title           = {Xdeepfm: Combining Explicit and Implicit Feature
                  Interactions for Recommender Systems},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1803.05170v3},
  abstract        = {Combinatorial features are essential for the success of
                  many commercial models. Manually crafting these features
                  usually comes with high cost due to the variety, volume and
                  velocity of raw data in web-scale systems. Factorization based
                  models, which measure interactions in terms of vector product,
                  can learn patterns of combinatorial features automatically and
                  generalize to unseen features as well. With the great success
                  of deep neural networks (DNNs) in various fields, recently
                  researchers have proposed several DNN-based factorization
                  model to learn both low- and high-order feature interactions.
                  Despite the powerful ability of learning an arbitrary function
                  from data, plain DNNs generate feature interactions implicitly
                  and at the bit-wise level. In this paper, we propose a novel
                  Compressed Interaction Network (CIN), which aims to generate
                  feature interactions in an explicit fashion and at the
                  vector-wise level. We show that the CIN share some
                  functionalities with convolutional neural networks (CNNs) and
                  recurrent neural networks (RNNs). We further combine a CIN and
                  a classical DNN into one unified model, and named this new
                  model eXtreme Deep Factorization Machine (xDeepFM). On one
                  hand, the xDeepFM is able to learn certain bounded-degree
                  feature interactions explicitly; on the other hand, it can
                  learn arbitrary low- and high-order feature interactions
                  implicitly. We conduct comprehensive experiments on three
                  real-world datasets. Our results demonstrate that xDeepFM
                  outperforms state-of-the-art models. We have released the
                  source code of xDeepFM at
                  \url{https://github.com/Leavingseason/xDeepFM}.},
  archivePrefix   = {arXiv},
  eprint          = {1803.05170},
  primaryClass    = {cs.LG},
}

@misc{lilian_domain_random_sim2r_trans,
  author          = {Lilian Weng},
  howpublished    =
                  {https://lilianweng.github.io/lil-log/2019/05/05/domain-randomization.html},
  note            = {Online; accessed 28 June 2019},
  title           = {Domain Randomization for Sim2Real Transfer},
  year            = 2019,
}

@article{lu17_beyon_finit_layer_neural_networ,
  author          = {Lu, Yiping and Zhong, Aoxiao and Li, Quanzheng and Dong,
                  Bin},
  title           = {Beyond Finite Layer Neural Networks: Bridging Deep
                  Architectures and Numerical Differential Equations},
  journal         = {CoRR},
  year            = 2017,
  url             = {http://arxiv.org/abs/1710.10121v2},
  abstract        = {In our work, we bridge deep neural network design with
                  numerical differential equations. We show that many effective
                  networks, such as ResNet, PolyNet, FractalNet and RevNet, can
                  be interpreted as different numerical discretizations of
                  differential equations. This finding brings us a brand new
                  perspective on the design of effective deep architectures. We
                  can take advantage of the rich knowledge in numerical analysis
                  to guide us in designing new and potentially more effective
                  deep networks. As an example, we propose a linear multi-step
                  architecture (LM-architecture) which is inspired by the linear
                  multi-step method solving ordinary differential equations. The
                  LM-architecture is an effective structure that can be used on
                  any ResNet-like networks. In particular, we demonstrate that
                  LM-ResNet and LM-ResNeXt (i.e. the networks obtained by
                  applying the LM-architecture on ResNet and ResNeXt
                  respectively) can achieve noticeably higher accuracy than
                  ResNet and ResNeXt on both CIFAR and ImageNet with comparable
                  numbers of trainable parameters. In particular, on both CIFAR
                  and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress
                  ($>50$\ \%) the original networks while maintaining a similar
                  performance. This can be explained mathematically using the
                  concept of modified equation from numerical analysis. Last but
                  not least, we also establish a connection between stochastic
                  control and noise injection in the training process which
                  helps to improve generalization of the networks. Furthermore,
                  by relating stochastic training strategy with stochastic
                  dynamic system, we can easily apply stochastic training to the
                  networks with the LM-architecture. As an example, we
                  introduced stochastic depth to LM-ResNet and achieve
                  significant improvement over the original LM-ResNet on
                  CIFAR10.},
  archivePrefix   = {arXiv},
  eprint          = {1710.10121},
  primaryClass    = {cs.CV},
}

@misc{maciver_hard_things,
  author          = {David R. MacIver},
  howpublished    = {https://www.drmaciver.com/2019/05/how-to-do-hard-things/},
  note            = {Online; accessed 20 May 2019},
  title           = {How to do hard things},
  year            = 2019,
}

@article{mnih16_async_method_deep_reinf_learn,
  author          = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and
                  Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and
                  Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  title           = {Asynchronous Methods for Deep Reinforcement Learning},
  journal         = {CoRR},
  year            = 2016,
  url             = {http://arxiv.org/abs/1602.01783v2},
  abstract        = {We propose a conceptually simple and lightweight framework
                  for deep reinforcement learning that uses asynchronous
                  gradient descent for optimization of deep neural network
                  controllers. We present asynchronous variants of four standard
                  reinforcement learning algorithms and show that parallel
                  actor-learners have a stabilizing effect on training allowing
                  all four methods to successfully train neural network
                  controllers. The best performing method, an asynchronous
                  variant of actor-critic, surpasses the current
                  state-of-the-art on the Atari domain while training for half
                  the time on a single multi-core CPU instead of a GPU.
                  Furthermore, we show that asynchronous actor-critic succeeds
                  on a wide variety of continuous motor control problems as well
                  as on a new task of navigating random 3D mazes using a visual
                  input.},
  archivePrefix   = {arXiv},
  eprint          = {1602.01783},
  primaryClass    = {cs.LG},
}

@article{mnih2013playing,
  title           = {Playing atari with deep reinforcement learning},
  author          = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David
                  and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan
                  and Riedmiller, Martin},
  journal         = {arXiv preprint arXiv:1312.5602},
  year            = 2013
}

@article{murphy2014machine,
  title           = {Machine learning: a probabilistic perspective. 2012},
  author          = {Murphy, Kevin P},
  journal         = {Cit{\'e} en},
  pages           = 117,
  year            = 2014
}

@article{nair15_massiv_paral_method_deep_reinf_learn,
  author          = {Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and
                  Alcicek, Cagdas and Fearon, Rory and Maria, Alessandro De and
                  Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie,
                  Charles and Petersen, Stig and Legg, Shane and Mnih, Volodymyr
                  and Kavukcuoglu, Koray and Silver, David},
  title           = {Massively Parallel Methods for Deep Reinforcement Learning},
  journal         = {CoRR},
  year            = 2015,
  url             = {http://arxiv.org/abs/1507.04296v2},
  abstract        = {We present the first massively distributed architecture for
                  deep reinforcement learning. This architecture uses four main
                  components: parallel actors that generate new behaviour;
                  parallel learners that are trained from stored experience; a
                  distributed neural network to represent the value function or
                  behaviour policy; and a distributed store of experience. We
                  used our architecture to implement the Deep Q-Network
                  algorithm (DQN). Our distributed algorithm was applied to 49
                  games from Atari 2600 games from the Arcade Learning
                  Environment, using identical hyperparameters. Our performance
                  surpassed non-distributed DQN in 41 of the 49 games and also
                  reduced the wall-time required to achieve these results by an
                  order of magnitude on most games.},
  archivePrefix   = {arXiv},
  eprint          = {1507.04296},
  primaryClass    = {cs.LG},
}

@article{nakano11_spikin_neural_networ_model_free,
  author          = {Takashi Nakano and Makoto Otsuka},
  title           = {Spiking Neural Network Model of Free-Energy-Based
                  Reinforcement Learning},
  journal         = {BMC Neuroscience},
  volume          = 12,
  number          = {S1},
  pages           = {P244},
  year            = 2011,
  doi             = {10.1186/1471-2202-12-s1-p244},
  url             = {https://doi.org/10.1186/1471-2202-12-s1-p244},
  DATE_ADDED      = {Thu Jan 16 23:13:33 2020},
}

@misc{nateliason_how_take_smart_notes,
  author          = {Nat Eliason},
  howpublished    = {https://www.nateliason.com/blog/smart-notes},
  note            = {Online; accessed 14 February 2020},
  title           = {How to Take Smart Notes: A Step-by-Step Guide - Nat
                  Eliason},
  year            = 2020,
}

@article{neftci19_surrog_gradien_learn_spikin_neural_networ,
  author          = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  title           = {Surrogate Gradient Learning in Spiking Neural Networks},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1901.09948v2},
  abstract        = {Spiking neural networks are nature's versatile solution to
                  fault-tolerant and energy efficient signal processing. To
                  translate these benefits into hardware, a growing number of
                  neuromorphic spiking neural network processors attempt to
                  emulate biological neural networks. These developments have
                  created an imminent need for methods and tools to enable such
                  systems to solve real-world signal processing problems. Like
                  conventional neural networks, spiking neural networks can be
                  trained on real, domain specific data. However, their training
                  requires overcoming a number of challenges linked to their
                  binary and dynamical nature. This article elucidates
                  step-by-step the problems typically encountered when training
                  spiking neural networks, and guides the reader through the key
                  concepts of synaptic plasticity and data-driven learning in
                  the spiking setting. To that end, it gives an overview of
                  existing approaches and provides an introduction to surrogate
                  gradient methods, specifically, as a particularly flexible and
                  efficient method to overcome the aforementioned challenges.},
  archivePrefix   = {arXiv},
  eprint          = {1901.09948},
  primaryClass    = {cs.NE},
}

@article{newcombe15_how_amazon_web_servic_uses_formal_method,
  author          = {Chris Newcombe and Tim Rath and Fan Zhang and Bogdan
                  Munteanu and Marc Brooker and Michael Deardeuff},
  title           = {How Amazon Web Services Uses Formal Methods},
  journal         = {Communications of the ACM},
  volume          = 58,
  number          = 4,
  pages           = {66-73},
  year            = 2015,
  doi             = {10.1145/2699417},
  url             = {https://doi.org/10.1145/2699417},
  DATE_ADDED      = {Thu Jan 16 14:52:06 2020},
}

@misc{nilil_instal_ubunt_ros_wiki,
  author          = {nil},
  howpublished    = {http://wiki.ros.org/melodic/Installation/Ubuntu},
  note            = {Online; accessed 16 October 2019},
  title           = {melodic/Installation/Ubuntu - ROS Wiki},
  year            = {nil},
}

@misc{nilil_ros_introd_ros_wiki,
  author          = {nil},
  howpublished    = {http://wiki.ros.org/ROS/Introduction},
  note            = {Online; accessed 15 October 2019},
  title           = {ROS/Introduction - ROS Wiki},
  year            = {nil},
}

@misc{openai_gym,
  author          = {OpenAI},
  howpublished    = {https://gym.openai.com/envs/CartPole-v0/},
  note            = {Online; accessed 02 November 2019},
  title           = {OpenAI Gym},
  year            = 2019,
}

@article{paine19_makin_effic_use_demon_to,
  author          = {Paine, Tom Le and Gulcehre, Caglar and Shahriari, Bobak and
                  Denil, Misha and Hoffman, Matt and Soyer, Hubert and Tanburn,
                  Richard and Kapturowski, Steven and Rabinowitz, Neil and
                  Williams, Duncan and Barth-Maron, Gabriel and Wang, Ziyu and
                  Freitas, Nando de and Team, Worlds},
  title           = {Making Efficient Use of Demonstrations To Solve Hard
                  Exploration Problems},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1909.01387v1},
  abstract        = {This paper introduces R2D3, an agent that makes efficient
                  use of demonstrations to solve hard exploration problems in
                  partially observable environments with highly variable initial
                  conditions. We also introduce a suite of eight tasks that
                  combine these three properties, and show that R2D3 can solve
                  several of the tasks where other state of the art methods
                  (both with and without demonstrations) fail to see even a
                  single successful trajectory after tens of billions of steps
                  of exploration.},
  archivePrefix   = {arXiv},
  eprint          = {1909.01387},
  primaryClass    = {cs.LG},
}

@article{papamakarios19_normal_flows_probab_model_infer,
  author          = {Papamakarios, George and Nalisnick, Eric and Rezende,
                  Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan,
                  Balaji},
  title           = {Normalizing Flows for Probabilistic Modeling and Inference},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1912.02762v1},
  abstract        = {Normalizing flows provide a general mechanism for defining
                  expressive probability distributions, only requiring the
                  specification of a (usually simple) base distribution and a
                  series of bijective transformations. There has been much
                  recent work on normalizing flows, ranging from improving their
                  expressive power to expanding their application. We believe
                  the field has now matured and is in need of a unified
                  perspective. In this review, we attempt to provide such a
                  perspective by describing flows through the lens of
                  probabilistic modeling and inference. We place special
                  emphasis on the fundamental principles of flow design, and
                  discuss foundational topics such as expressive power and
                  computational trade-offs. We also broaden the conceptual
                  framing of flows by relating them to more general probability
                  transformations. Lastly, we summarize the use of flows for
                  tasks such as generative modeling, approximate inference, and
                  supervised learning.},
  archivePrefix   = {arXiv},
  eprint          = {1912.02762},
  primaryClass    = {stat.ML},
}

@article{pati17_statis_optim_variat_bayes,
  author          = {Pati, Debdeep and Bhattacharya, Anirban and Yang, Yun},
  title           = {On Statistical Optimality of Variational Bayes},
  journal         = {CoRR},
  year            = 2017,
  url             = {http://arxiv.org/abs/1712.08983v1},
  abstract        = {The article addresses a long-standing open problem on the
                  justification of using variational Bayes methods for parameter
                  estimation. We provide general conditions for obtaining
                  optimal risk bounds for point estimates acquired from
                  mean-field variational Bayesian inference. The conditions
                  pertain to the existence of certain test functions for the
                  distance metric on the parameter space and minimal assumptions
                  on the prior. A general recipe for verification of the
                  conditions is outlined which is broadly applicable to existing
                  Bayesian models with or without latent variables. As
                  illustrations, specific applications to Latent Dirichlet
                  Allocation and Gaussian mixture models are discussed.},
  archivePrefix   = {arXiv},
  eprint          = {1712.08983},
  primaryClass    = {math.ST},
}

@article{pfeiffer18_deep_learn_with_spikin_neuron,
  author          = {Michael Pfeiffer and Thomas Pfeil},
  title           = {Deep Learning With Spiking Neurons: Opportunities and
                  Challenges},
  journal         = {Frontiers in Neuroscience},
  volume          = 12,
  number          = {nil},
  pages           = {nil},
  year            = 2018,
  doi             = {10.3389/fnins.2018.00774},
  url             = {https://doi.org/10.3389/fnins.2018.00774},
  DATE_ADDED      = {Mon Aug 12 20:01:23 2019},
}

@article{pfeiffer2018deep,
  title           = {Deep learning with spiking neurons: opportunities and
                  challenges},
  author          = {Pfeiffer, Michael and Pfeil, Thomas},
  journal         = {Frontiers in neuroscience},
  volume          = 12,
  year            = 2018,
  publisher       = {Frontiers Media SA}
}

@book{pinsky2010introduction,
  title           = {An introduction to stochastic modeling},
  author          = {Pinsky, Mark and Karlin, Samuel},
  year            = 2010,
  publisher       = {Academic press}
}

@InProceedings{pmlr-v38-srivastava15,
  title           = {{WASP: Scalable Bayes via barycenters of subset
                  posteriors}},
  author          = {Sanvesh Srivastava and Volkan Cevher and Quoc Dinh and
                  David Dunson},
  booktitle       = {Proceedings of the Eighteenth International Conference on
                  Artificial Intelligence and Statistics},
  pages           = {912--920},
  year            = 2015,
  editor          = {Guy Lebanon and S. V. N. Vishwanathan},
  volume          = 38,
  series          = {Proceedings of Machine Learning Research},
  address         = {San Diego, California, USA},
  month           = {09--12 May},
  publisher       = {PMLR},
  pdf             = {http://proceedings.mlr.press/v38/srivastava15.pdf},
  url             = {http://proceedings.mlr.press/v38/srivastava15.html},
  abstract        = {The promise of Bayesian methods for big data sets has not
                  fully been realized due to the lack of scalable computational
                  algorithms. For massive data, it is necessary to store and
                  process subsets on different machines in a distributed manner.
                  We propose a simple, general, and highly efficient approach,
                  which first runs a posterior sampling algorithm in parallel on
                  different machines for subsets of a large data set. To combine
                  these subset posteriors, we calculate the Wasserstein
                  barycenter via a highly efficient linear program. The
                  resulting estimate for the Wasserstein posterior (WASP) has an
                  atomic form, facilitating straightforward estimation of
                  posterior summaries of functionals of interest. The WASP
                  approach allows posterior sampling algorithms for smaller data
                  sets to be trivially scaled to huge data. We provide
                  theoretical justification in terms of posterior consistency
                  and algorithm efficiency. Examples are provided in complex
                  settings including Gaussian process regression and
                  nonparametric Bayes mixture models.}
}

@InProceedings{pmlr-v70-haarnoja17a,
  title           = {Reinforcement Learning with Deep Energy-Based Policies},
  author          = {Tuomas Haarnoja and Haoran Tang and Pieter Abbeel and
                  Sergey Levine},
  booktitle       = {Proceedings of the 34th International Conference on Machine
                  Learning},
  pages           = {1352--1361},
  year            = 2017,
  editor          = {Doina Precup and Yee Whye Teh},
  volume          = 70,
  series          = {Proceedings of Machine Learning Research},
  address         = {International Convention Centre, Sydney, Australia},
  month           = {06--11 Aug},
  publisher       = {PMLR},
  pdf             =
                  {http://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf},
  url             = {http://proceedings.mlr.press/v70/haarnoja17a.html},
  abstract        = {We propose a method for learning expressive energy-based
                  policies for continuous states and actions, which has been
                  feasible only in tabular domains before. We apply our method
                  to learning maximum entropy policies, resulting into a new
                  algorithm, called soft Q-learning, that expresses the optimal
                  policy via a Boltzmann distribution. We use the recently
                  proposed amortized Stein variational gradient descent to learn
                  a stochastic sampling network that approximates samples from
                  this distribution. The benefits of the proposed algorithm
                  include improved exploration and compositionality that allows
                  transferring skills between tasks, which we confirm in
                  simulated experiments with swimming and walking robots. We
                  also draw a connection to actor-critic methods, which can be
                  viewed performing approximate inference on the corresponding
                  energy-based model.}
}

@InProceedings{pmlr-v89-o-connor19a,
  title           = {Training a Spiking Neural Network with Equilibrium
                  Propagation},
  author          = {O'Connor, Peter and Gavves, Efstratios and Welling, Max},
  booktitle       = {Proceedings of Machine Learning Research},
  pages           = {1516--1523},
  year            = 2019,
  editor          = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume          = 89,
  series          = {Proceedings of Machine Learning Research},
  month           = {16--18 Apr},
  publisher       = {PMLR},
  pdf             =
                  {http://proceedings.mlr.press/v89/o-connor19a/o-connor19a.pdf},
  url             = {http://proceedings.mlr.press/v89/o-connor19a.html},
  abstract        = {Backpropagation is almost universally used to train
                  artificial neural networks. However, there are several reasons
                  that backpropagation could not be plausibly implemented by
                  biological neurons. Among these are the facts that (1)
                  biological neurons appear to lack any mechanism for sending
                  gradients backwards across synapses, and (2) biological
                  “spiking” neurons emit binary signals, whereas
                  back-propagation requires that neurons communicate continuous
                  values between one another. Recently, Scellier and Bengio
                  [2017], demonstrated an alternative to backpropagation, called
                  Equilibrium Propagation, wherein gradients are implicitly
                  computed by the dynamics of the neural network, so that
                  neurons do not need an internal mechanism for backpropagation
                  of gradients. This provides an interesting solution to problem
                  (1). In this paper, we address problem (2) by proposing a way
                  in which Equilibrium Propagation can be implemented with
                  neurons which are constrained to just communicate binary
                  values at each time step. We show that with appropriate
                  step-size annealing, we can converge to the same fixed-point
                  as a real-valued neural network, and that with predictive
                  coding, we can make this convergence much faster. We
                  demonstrate that the resulting model can be used to train a
                  spiking neural network using the update scheme from
                  Equilibrium propagation.}
}

@article{pong19_skew_fit,
  author          = {Pong, Vitchyr H. and Dalal, Murtaza and Lin, Steven and
                  Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
  title           = {Skew-Fit: State-Covering Self-Supervised Reinforcement
                  Learning},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1903.03698v2},
  abstract        = {In standard reinforcement learning, each new skill requires
                  a manually-designed reward function, which takes considerable
                  manual effort and engineering. Self-supervised goal setting
                  has the potential to automate this process, enabling an agent
                  to propose its own goals and acquire skills that achieve these
                  goals. However, such methods typically rely on
                  manually-designed goal distributions, or heuristics to force
                  the agent to explore a wide range of states. We propose a
                  formal exploration objective for goal-reaching policies that
                  maximizes state coverage. We show that this objective is
                  equivalent to maximizing the entropy of the goal distribution
                  together with goal reaching performance, where goals
                  correspond to entire states. We present an algorithm called
                  Skew-Fit for learning such a maximum-entropy goal
                  distribution, and show that under certain regularity
                  conditions, our method converges to a uniform distribution
                  over the set of possible states, even when we do not know this
                  set beforehand. Skew-Fit enables self-supervised agents to
                  autonomously choose and practice diverse goals. Our
                  experiments show that it can learn a variety of manipulation
                  tasks from images, including opening a door with a real robot,
                  entirely from scratch and without any manually-designed reward
                  function.},
  archivePrefix   = {arXiv},
  eprint          = {1903.03698},
  primaryClass    = {cs.LG},
}

@article{poole19_variat_bound_mutual_infor,
  author          = {Poole, Ben and Ozair, Sherjil and Oord, Aaron van den and
                  Alemi, Alexander A. and Tucker, George},
  title           = {On Variational Bounds of Mutual Information},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1905.06922v1},
  abstract        = {Estimating and optimizing Mutual Information (MI) is core
                  to many problems in machine learning; however, bounding MI in
                  high dimensions is challenging. To establish tractable and
                  scalable objectives, recent work has turned to variational
                  bounds parameterized by neural networks, but the relationships
                  and tradeoffs between these bounds remains unclear. In this
                  work, we unify these recent developments in a single
                  framework. We find that the existing variational lower bounds
                  degrade when the MI is large, exhibiting either high bias or
                  high variance. To address this problem, we introduce a
                  continuum of lower bounds that encompasses previous bounds and
                  flexibly trades off bias and variance. On high-dimensional,
                  controlled problems, we empirically characterize the bias and
                  variance of the bounds and their gradients and demonstrate the
                  effectiveness of our new bounds for estimation and
                  representation learning.},
  archivePrefix   = {arXiv},
  eprint          = {1905.06922v1},
  primaryClass    = {cs.LG},
}

@article{queiroz06_reinf_learn_simpl_contr_task,
  author          = {Murilo Saraiva de Queiroz and Roberto Coelho de Berrêdo and
                  Antônio de P{\'a}dua Braga},
  title           = {Reinforcement Learning of a Simple Control Task Using the
                  Spike Response Model},
  journal         = {Neurocomputing},
  volume          = 70,
  number          = {1-3},
  pages           = {14-20},
  year            = 2006,
  doi             = {10.1016/j.neucom.2006.07.002},
  url             = {https://doi.org/10.1016/j.neucom.2006.07.002},
  DATE_ADDED      = {Thu Aug 29 12:45:08 2019},
}

@article{rackauckas19_diffeq,
  author          = {Rackauckas, Chris and Innes, Mike and Ma, Yingbo and
                  Bettencourt, Jesse and White, Lyndon and Dixit, Vaibhav},
  title           = {Diffeqflux.jl - a Julia Library for Neural Differential
                  Equations},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1902.02376v1},
  abstract        = {DiffEqFlux.jl is a library for fusing neural networks and
                  differential equations. In this work we describe differential
                  equations from the viewpoint of data science and discuss the
                  complementary nature between machine learning models and
                  differential equations. We demonstrate the ability to
                  incorporate DifferentialEquations.jl-defined differential
                  equation problems into a Flux-defined neural network, and vice
                  versa. The advantages of being able to use the entire
                  DifferentialEquations.jl suite for this purpose is
                  demonstrated by counter examples where simple integration
                  strategies fail, but the sophisticated integration strategies
                  provided by the DifferentialEquations.jl library succeed. This
                  is followed by a demonstration of delay differential equations
                  and stochastic differential equations inside of neural
                  networks. We show high-level functionality for defining neural
                  ordinary differential equations (neural networks embedded into
                  the differential equation) and describe the extra models in
                  the Flux model zoo which includes neural stochastic
                  differential equations. We conclude by discussing the various
                  adjoint methods used for backpropogation of the differential
                  equation solvers. DiffEqFlux.jl is an important contribution
                  to the area, as it allows the full weight of the differential
                  equation solvers developed from decades of research in the
                  scientific computing field to be readily applied to the
                  challenges posed by machine learning and data science.},
  archivePrefix   = {arXiv},
  eprint          = {1902.02376},
  primaryClass    = {cs.LG},
}

@article{ranganath13_black_box_variat_infer,
  author          = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
  title           = {Black Box Variational Inference},
  journal         = {CoRR},
  year            = 2013,
  url             = {http://arxiv.org/abs/1401.0118v1},
  abstract        = {Variational inference has become a widely used method to
                  approximate posteriors in complex latent variables models.
                  However, deriving a variational inference algorithm generally
                  requires significant model-specific analysis, and these
                  efforts can hinder and deter us from quickly developing and
                  exploring a variety of models for a problem at hand. In this
                  paper, we present a "black box" variational inference
                  algorithm, one that can be quickly applied to many models with
                  little additional derivation. Our method is based on a
                  stochastic optimization of the variational objective where the
                  noisy gradient is computed from Monte Carlo samples from the
                  variational distribution. We develop a number of methods to
                  reduce the variance of the gradient, always maintaining the
                  criterion that we want to avoid difficult model-based
                  derivations. We evaluate our method against the corresponding
                  black box sampling based methods. We find that our method
                  reaches better predictive likelihoods much faster than
                  sampling methods. Finally, we demonstrate that Black Box
                  Variational Inference lets us easily explore a wide space of
                  models by quickly constructing and evaluating several models
                  of longitudinal healthcare data.},
  archivePrefix   = {arXiv},
  eprint          = {1401.0118},
  primaryClass    = {stat.ML},
}

@inproceedings{ratliff2006maximum,
  title           = {Maximum margin planning},
  author          = {Ratliff, Nathan D and Bagnell, J Andrew and Zinkevich,
                  Martin A},
  booktitle       = {Proceedings of the 23rd international conference on Machine
                  learning},
  pages           = {729--736},
  year            = 2006,
  organization    = {ACM}
}

@misc{ravi_bayesian_teaching_mnist,
  author          = {Ravi Sojitra},
  howpublished    =
                  {https://ravisoji.com/2018/03/04/bayesian-teaching-as-explanation.html},
  note            = {Online; accessed 19 May 2019},
  title           = {Bayesian Teaching as Model Explanation: An MNIST Example},
  year            = 2018,
}

@article{richardson06_markov_logic_networ,
  author          = {Matthew Richardson and Pedro Domingos},
  title           = {Markov Logic Networks},
  journal         = {Machine Learning},
  volume          = 62,
  number          = {1-2},
  pages           = {107-136},
  year            = 2006,
  doi             = {10.1007/s10994-006-5833-1},
  url             = {https://doi.org/10.1007/s10994-006-5833-1},
  DATE_ADDED      = {Wed Jul 24 22:57:34 2019},
}

@book{ross2014introduction,
  title           = {Introduction to probability models},
  author          = {Ross, Sheldon M},
  year            = 2014,
  publisher       = {Academic press}
}

@article{rueckauer16_theor_tools_conver_analog_to,
  author          = {Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang
                  and Pfeiffer, Michael},
  title           = {Theory and Tools for the Conversion of Analog To Spiking
                  Convolutional Neural Networks},
  journal         = {CoRR},
  year            = 2016,
  url             = {http://arxiv.org/abs/1612.04052v1},
  abstract        = {Deep convolutional neural networks (CNNs) have shown great
                  potential for numerous real-world machine learning
                  applications, but performing inference in large CNNs in
                  real-time remains a challenge. We have previously demonstrated
                  that traditional CNNs can be converted into deep spiking
                  neural networks (SNNs), which exhibit similar accuracy while
                  reducing both latency and computational load as a consequence
                  of their data-driven, event-based style of computing. Here we
                  provide a novel theory that explains why this conversion is
                  successful, and derive from it several new tools to convert a
                  larger and more powerful class of deep networks into SNNs. We
                  identify the main sources of approximation errors in previous
                  conversion methods, and propose simple mechanisms to fix these
                  issues. Furthermore, we develop spiking implementations of
                  common CNN operations such as max-pooling, softmax, and
                  batch-normalization, which allow almost loss-less conversion
                  of arbitrary CNN architectures into the spiking domain.
                  Empirical evaluation of different network architectures on the
                  MNIST and CIFAR10 benchmarks leads to the best SNN results
                  reported to date.},
  archivePrefix   = {arXiv},
  eprint          = {1612.04052},
  primaryClass    = {stat.ML},
}

@article{ruiz19_contr_diver_combin_variat_infer_mcmc,
  author          = {Ruiz, Francisco J. R. and Titsias, Michalis K.},
  title           = {A Contrastive Divergence for Combining Variational
                  Inference and Mcmc},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1905.04062v1},
  abstract        = {We develop a method to combine Markov chain Monte Carlo
                  (MCMC) and variational inference (VI), leveraging the
                  advantages of both inference approaches. Specifically, we
                  improve the variational distribution by running a few MCMC
                  steps. To make inference tractable, we introduce the
                  variational contrastive divergence (VCD), a new divergence
                  that replaces the standard Kullback-Leibler (KL) divergence
                  used in VI. The VCD captures a notion of discrepancy between
                  the initial variational distribution and its improved version
                  (obtained after running the MCMC steps), and it converges
                  asymptotically to the symmetrized KL divergence between the
                  variational distribution and the posterior of interest. The
                  VCD objective can be optimized efficiently with respect to the
                  variational parameters via stochastic optimization. We show
                  experimentally that optimizing the VCD leads to better
                  predictive performance on two latent variable models: logistic
                  matrix factorization and variational autoencoders (VAEs).},
  archivePrefix   = {arXiv},
  eprint          = {1905.04062},
  primaryClass    = {stat.ML},
}

@article{rusu15_polic_distil,
  author          = {Rusu, Andrei A. and Colmenarejo, Sergio Gomez and Gulcehre,
                  Caglar and Desjardins, Guillaume and Kirkpatrick, James and
                  Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and
                  Hadsell, Raia},
  title           = {Policy Distillation},
  journal         = {CoRR},
  year            = 2015,
  url             = {http://arxiv.org/abs/1511.06295v2},
  abstract        = {Policies for complex visual tasks have been successfully
                  learned with deep reinforcement learning, using an approach
                  called deep Q-networks (DQN), but relatively large
                  (task-specific) networks and extensive training are needed to
                  achieve good performance. In this work, we present a novel
                  method called policy distillation that can be used to extract
                  the policy of a reinforcement learning agent and train a new
                  network that performs at the expert level while being
                  dramatically smaller and more efficient. Furthermore, the same
                  method can be used to consolidate multiple task-specific
                  policies into a single policy. We demonstrate these claims
                  using the Atari domain and show that the multi-task distilled
                  agent outperforms the single-task teachers as well as a
                  jointly-trained DQN agent.},
  archivePrefix   = {arXiv},
  eprint          = {1511.06295},
  primaryClass    = {cs.LG},
}

@article{sallans04a_ferl,
  author          = {Sallans, Brian and Hinton, Geoffrey},
  year            = 2004,
  month           = 08,
  pages           = {1063-1088},
  title           = {Reinforcement Learning with Factored States and Actions.},
  volume          = 5,
  journal         = {Journal of Machine Learning Research}
}

@article{sboev18_spikin_neural_networ_reinf_learn,
  author          = {Alexander Sboev and Danila Vlasov and Roman Rybka and
                  Alexey Serenko},
  title           = {Spiking Neural Network Reinforcement Learning Method Based
                  on Temporal Coding and Stdp},
  journal         = {Procedia Computer Science},
  volume          = 145,
  number          = {nil},
  pages           = {458-463},
  year            = 2018,
  doi             = {10.1016/j.procs.2018.11.107},
  url             = {https://doi.org/10.1016/j.procs.2018.11.107},
  DATE_ADDED      = {Mon Sep 30 11:08:34 2019},
}

@article{sboev18_spikin_neural_networ_reinf_learn,
  author          = {Alexander Sboev and Danila Vlasov and Roman Rybka and
                  Alexey Serenko},
  title           = {Spiking Neural Network Reinforcement Learning Method Based
                  on Temporal Coding and Stdp},
  journal         = {Procedia Computer Science},
  volume          = 145,
  number          = {nil},
  pages           = {458-463},
  year            = 2018,
  doi             = {10.1016/j.procs.2018.11.107},
  url             = {https://doi.org/10.1016/j.procs.2018.11.107},
  DATE_ADDED      = {Mon Sep 30 11:08:34 2019},
}

@article{schaul15_prior_exper_replay,
  author          = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and
                  Silver, David},
  title           = {Prioritized Experience Replay},
  journal         = {CoRR},
  year            = 2015,
  url             = {http://arxiv.org/abs/1511.05952v4},
  abstract        = {Experience replay lets online reinforcement learning agents
                  remember and reuse experiences from the past. In prior work,
                  experience transitions were uniformly sampled from a replay
                  memory. However, this approach simply replays transitions at
                  the same frequency that they were originally experienced,
                  regardless of their significance. In this paper we develop a
                  framework for prioritizing experience, so as to replay
                  important transitions more frequently, and therefore learn
                  more efficiently. We use prioritized experience replay in Deep
                  Q-Networks (DQN), a reinforcement learning algorithm that
                  achieved human-level performance across many Atari games. DQN
                  with prioritized experience replay achieves a new
                  state-of-the-art, outperforming DQN with uniform replay on 41
                  out of 49 games.},
  archivePrefix   = {arXiv},
  eprint          = {1511.05952},
  primaryClass    = {cs.LG},
}

@article{schliebs13_evolv_spikin_neural_networ_survey,
  author          = {Stefan Schliebs and Nikola Kasabov},
  title           = {Evolving Spiking Neural Network-A Survey},
  journal         = {Evolving Systems},
  volume          = 4,
  number          = 2,
  pages           = {87-98},
  year            = 2013,
  doi             = {10.1007/s12530-013-9074-9},
  url             = {https://doi.org/10.1007/s12530-013-9074-9},
  DATE_ADDED      = {Mon Jan 6 18:15:22 2020},
}

@article{schulman15_high_dimen_contin_contr_using,
  author          = {Schulman, John and Moritz, Philipp and Levine, Sergey and
                  Jordan, Michael and Abbeel, Pieter},
  title           = {High-Dimensional Continuous Control Using Generalized
                  Advantage Estimation},
  journal         = {CoRR},
  year            = 2015,
  url             = {http://arxiv.org/abs/1506.02438v6},
  abstract        = {Policy gradient methods are an appealing approach in
                  reinforcement learning because they directly optimize the
                  cumulative reward and can straightforwardly be used with
                  nonlinear function approximators such as neural networks. The
                  two main challenges are the large number of samples typically
                  required, and the difficulty of obtaining stable and steady
                  improvement despite the nonstationarity of the incoming data.
                  We address the first challenge by using value functions to
                  substantially reduce the variance of policy gradient estimates
                  at the cost of some bias, with an exponentially-weighted
                  estimator of the advantage function that is analogous to
                  TD(lambda). We address the second challenge by using trust
                  region optimization procedure for both the policy and the
                  value function, which are represented by neural networks. Our
                  approach yields strong empirical results on highly challenging
                  3D locomotion tasks, learning running gaits for bipedal and
                  quadrupedal simulated robots, and learning a policy for
                  getting the biped to stand up from starting out lying on the
                  ground. In contrast to a body of prior work that uses
                  hand-crafted policy representations, our neural network
                  policies map directly from raw kinematics to joint torques.
                  Our algorithm is fully model-free, and the amount of simulated
                  experience required for the learning tasks on 3D bipeds
                  corresponds to 1-2 weeks of real time.},
  archivePrefix   = {arXiv},
  eprint          = {1506.02438},
  primaryClass    = {cs.LG},
}

@article{schultz97_neural_subst_predic_rewar,
  author          = {W. Schultz and P. Dayan and P. R. Montague},
  title           = {A Neural Substrate of Prediction and Reward},
  journal         = {Science},
  volume          = 275,
  number          = 5306,
  pages           = {1593-1599},
  year            = 1997,
  doi             = {10.1126/science.275.5306.1593},
  url             = {https://doi.org/10.1126/science.275.5306.1593},
  DATE_ADDED      = {Sun Dec 15 13:23:11 2019},
}

@book{shalev2014understanding,
  title           = {Understanding machine learning: From theory to algorithms},
  author          = {Shalev-Shwartz, Shai and Ben-David, Shai},
  year            = 2014,
  publisher       = {Cambridge university press}
}

@article{shwartz-ziv17_openin_black_box_deep_neural,
  author          = {Shwartz-Ziv, Ravid and Tishby, Naftali},
  title           = {Opening the Black Box of Deep Neural Networks Via
                  Information},
  journal         = {CoRR},
  year            = 2017,
  url             = {http://arxiv.org/abs/1703.00810v3},
  abstract        = {Despite their great success, there is still no
                  comprehensive theoretical understanding of learning with Deep
                  Neural Networks (DNNs) or their inner organization. Previous
                  work proposed to analyze DNNs in the \textit{Information
                  Plane}; i.e., the plane of the Mutual Information values that
                  each layer preserves on the input and output variables. They
                  suggested that the goal of the network is to optimize the
                  Information Bottleneck (IB) tradeoff between compression and
                  prediction, successively, for each layer. In this work we
                  follow up on this idea and demonstrate the effectiveness of
                  the Information-Plane visualization of DNNs. Our main results
                  are: (i) most of the training epochs in standard DL are spent
                  on {\emph compression} of the input to efficient
                  representation and not on fitting the training labels. (ii)
                  The representation compression phase begins when the training
                  errors becomes small and the Stochastic Gradient Decent (SGD)
                  epochs change from a fast drift to smaller training error into
                  a stochastic relaxation, or random diffusion, constrained by
                  the training error value. (iii) The converged layers lie on or
                  very close to the Information Bottleneck (IB) theoretical
                  bound, and the maps from the input to any hidden layer and
                  from this hidden layer to the output satisfy the IB
                  self-consistent equations. This generalization through noise
                  mechanism is unique to Deep Neural Networks and absent in one
                  layer networks. (iv) The training time is dramatically reduced
                  when adding more hidden layers. Thus the main advantage of the
                  hidden layers is computational. This can be explained by the
                  reduced relaxation time, as this it scales super-linearly
                  (exponentially for simple diffusion) with the information
                  compression from the previous layer.},
  archivePrefix   = {arXiv},
  eprint          = {1703.00810},
  primaryClass    = {cs.LG},
}

@article{simard17_machin_teach,
  author          = {Simard, Patrice Y. and Amershi, Saleema and Chickering,
                  David M. and Pelton, Alicia Edelman and Ghorashi, Soroush and
                  Meek, Christopher and Ramos, Gonzalo and Suh, Jina and Verwey,
                  Johan and Wang, Mo and Wernsing, John},
  title           = {Machine Teaching: a New Paradigm for Building Machine
                  Learning Systems},
  journal         = {CoRR},
  year            = 2017,
  url             = {http://arxiv.org/abs/1707.06742v3},
  abstract        = {The current processes for building machine learning systems
                  require practitioners with deep knowledge of machine learning.
                  This significantly limits the number of machine learning
                  systems that can be created and has led to a mismatch between
                  the demand for machine learning systems and the ability for
                  organizations to build them. We believe that in order to meet
                  this growing demand for machine learning systems we must
                  significantly increase the number of individuals that can
                  teach machines. We postulate that we can achieve this goal by
                  making the process of teaching machines easy, fast and above
                  all, universally accessible. While machine learning focuses on
                  creating new algorithms and improving the accuracy of
                  "learners", the machine teaching discipline focuses on the
                  efficacy of the "teachers". Machine teaching as a discipline
                  is a paradigm shift that follows and extends principles of
                  software engineering and programming languages. We put a
                  strong emphasis on the teacher and the teacher's interaction
                  with data, as well as crucial components such as techniques
                  and design principles of interaction and visualization. In
                  this paper, we present our position regarding the discipline
                  of machine teaching and articulate fundamental machine
                  teaching principles. We also describe how, by decoupling
                  knowledge about machine learning algorithms from the process
                  of teaching, we can accelerate innovation and empower millions
                  of new uses for machine learning models.},
  archivePrefix   = {arXiv},
  eprint          = {1707.06742},
  primaryClass    = {cs.LG},
}

@inproceedings{smith_quoc_bayes_generalization_sgd,
  title           = {A Bayesian Perspective on Generalization and Stochastic
                  Gradient Descent},
  author          = {Sam Smith and Quoc V. Le},
  year            = 2018,
  URL             = {https://openreview.net/pdf?id=BJij4yg0Z}
}

@misc{so_why_move_shared_ptr,
  author          = {nil},
  howpublished    =
                  {https://stackoverflow.com/questions/41871115/why-would-i-stdmove-an-stdshared-ptr},
  note            = {Online; accessed 25 February 2019},
  title           = {c++ - Why would I std::move an std::shared_ptr? - Stack
                  Overflow},
  year            = 2019,
}

@inproceedings{spikeprop,
  author          = {Bohte, Sander and Kok, Joost and Poutré, Johannes},
  year            = 2000,
  month           = 01,
  pages           = {419-424},
  title           = {SpikeProp: backpropagation for networks of spiking
                  neurons.},
  volume          = 48,
  journal         = {ESANN}
}

@article{stemmler1996single,
  title           = {A single spike suffices: the simplest form of stochastic
                  resonance in model neurons},
  author          = {Stemmler, Martin},
  journal         = {Network: Computation in Neural Systems},
  volume          = 7,
  number          = 4,
  pages           = {687--716},
  year            = 1996,
  publisher       = {Taylor \& Francis}
}

@article{stemmler96_singl_spike_suffic,
  author          = {Martin Stemmler},
  title           = {A Single Spike Suffices: the Simplest Form of Stochastic
                  Resonance in Model Neurons},
  journal         = {Network: Computation in Neural Systems},
  volume          = 7,
  number          = 4,
  pages           = {687-716},
  year            = 1996,
  doi             = {10.1088/0954-898x_7_4_005},
  url             = {https://doi.org/10.1088/0954-898x_7_4_005},
  DATE_ADDED      = {Sat Nov 2 19:32:20 2019},
}

@article{stock19_and_bit_goes_down,
  author          = {Stock, Pierre and Joulin, Armand and Gribonval, R{\'e}mi
                  and Graham, Benjamin and J{\'e}gou, Herv{\'e}},
  title           = {And the Bit Goes Down: Revisiting the Quantization of
                  Neural Networks},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1907.05686v2},
  abstract        = {In this paper, we address the problem of reducing the
                  memory footprint of ResNet-like convolutional network
                  architectures. We introduce a vector quantization method that
                  aims at preserving the quality of the reconstruction of the
                  network outputs and not its weights. The advantage of our
                  approach is that it minimizes the loss reconstruction error
                  for in-domain inputs and does not require any labelled data.
                  We also use byte-aligned codebooks to produce compressed
                  networks with efficient inference on CPU. We validate our
                  approach by quantizing a high performing ResNet-50 model to a
                  memory size of 5 MB (20x compression factor) while preserving
                  a top-1 accuracy of 76.1 \% on ImageNet object classification
                  and by compressing a Mask R-CNN with a size budget around 6
                  MB.},
  archivePrefix   = {arXiv},
  eprint          = {1907.05686},
  primaryClass    = {cs.CV},
}

@inproceedings{stolle2002learning,
  title           = {Learning options in reinforcement learning},
  author          = {Stolle, Martin and Precup, Doina},
  booktitle       = {International Symposium on abstraction, reformulation, and
                  approximation},
  pages           = {212--223},
  year            = 2002,
  organization    = {Springer}
}

@inproceedings{sutton2000policy,
  title           = {Policy gradient methods for reinforcement learning with
                  function approximation},
  author          = {Sutton, Richard S and McAllester, David A and Singh,
                  Satinder P and Mansour, Yishay},
  booktitle       = {Advances in neural information processing systems},
  pages           = {1057--1063},
  year            = 2000
}

@inproceedings{systemsengineering_6575245,
  author          = {R. {Welch} and D. {Limonadi} and R. {Manning}},
  booktitle       = {2013 8th International Conference on System of Systems
                  Engineering},
  title           = {Systems engineering the Curiosity Rover: A retrospective},
  year            = 2013,
  pages           = {70-75},
  keywords        = {entry, descent and landing (spacecraft);Mars;planetary
                  rovers;planetary surfaces;systems engineering;systems sampling
                  system;rover complex science payload;scientific
                  exploration;Earth;entry descent and landing system;cruise
                  system;spacecraft;MSL;Mars surface;Mars Science Laboratory
                  Curiosity Rover;systems engineering;Mars;Instruments;Aerospace
                  electronics;Actuators;Software;Computer
                  architecture;Complexity theory;Mars;Rovers;Systems
                  Engineering;Robotics},
  doi             = {10.1109/SYSoSE.2013.6575245},
  month           = {June},
}

@misc{tensorflow_stand_keras,
  author          = {Tensorflow},
  howpublished    =
                  {https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a},
  note            = {Online; accessed 07 January 2019},
  title           = {Standardizing on Keras: Guidance on High-level APIs in
                  TensorFlow 2.0},
  year            = 2018,
}

@book{thrun2005probabilistic,
  title           = {Probabilistic robotics},
  author          = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  year            = 2005,
  publisher       = {MIT press}
}

@article{titsias18_unbias_implic_variat_infer,
  author          = {Titsias, Michalis K. and Ruiz, Francisco J. R.},
  title           = {Unbiased Implicit Variational Inference},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1808.02078v3},
  abstract        = {We develop unbiased implicit variational inference (UIVI),
                  a method that expands the applicability of variational
                  inference by defining an expressive variational family. UIVI
                  considers an implicit variational distribution obtained in a
                  hierarchical manner using a simple reparameterizable
                  distribution whose variational parameters are defined by
                  arbitrarily flexible deep neural networks. Unlike previous
                  works, UIVI directly optimizes the evidence lower bound (ELBO)
                  rather than an approximation to the ELBO. We demonstrate UIVI
                  on several models, including Bayesian multinomial logistic
                  regression and variational autoencoders, and show that UIVI
                  achieves both tighter ELBO and better predictive performance
                  than existing approaches at a similar computational cost.},
  archivePrefix   = {arXiv},
  eprint          = {1808.02078},
  primaryClass    = {stat.ML},
}

@article{training_deep_snn_bpp_lee,
  author          = {Lee, Jun and Delbruck, Tobi and Pfeiffer, Michael},
  year            = 2016,
  month           = 08,
  title           = {Training Deep Spiking Neural Networks Using
                  Backpropagation},
  volume          = 10,
  journal         = {Frontiers in Neuroscience},
  doi             = {10.3389/fnins.2016.00508}
}

@misc{trimstray_se_hackers,
  author          = {trimstray},
  howpublished    = {https://twitter.com/trimstray/status/1086705742793658369},
  note            = {Online; accessed 09 February 2019},
  title           = {trimstray on Twitter: &quot;Search Engines for
                  Hackers:&#10;&#10;https://t.co/Awr3X88Xu1&#10;https://t.co/03trsWUrnP&#10;https://t.co/B9IHX23MeC&#10;https://t.co/uO1oFjB7Eb&#10;https://t.co/NE7FSOQsPl&#10;https://t.co/s2wG7cOGa5&#10;https://t.co/uBqtz7QuUD&#10;https://t.co/IZx4B82wLQ&#10;https://t.co/Oa04GvDxTp&#10;https://t.co/TKjuUVU9il&#10;&#10;#it
                  #tech&quot;},
  year            = 2019,
}

@article{tschiatschek19_learn_aware_teach,
  author          = {Tschiatschek, Sebastian and Ghosh, Ahana and Haug, Luis and
                  Devidze, Rati and Singla, Adish},
  title           = {Learner-Aware Teaching: Inverse Reinforcement Learning With
                  Preferences and Constraints},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1906.00429v1},
  abstract        = {Inverse reinforcement learning (IRL) enables an agent to
                  learn complex behavior by observing demonstrations from a
                  (near-)optimal policy. The typical assumption is that the
                  learner's goal is to match the teacher's demonstrated
                  behavior. In this paper, we consider the setting where the
                  learner has her own preferences that she additionally takes
                  into consideration. These preferences can for example capture
                  behavioral biases, mismatched worldviews, or physical
                  constraints. We study two teaching approaches:
                  learner-agnostic teaching, where the teacher provides
                  demonstrations from an optimal policy ignoring the learner's
                  preferences, and learner-aware teaching, where the teacher
                  accounts for the learner's preferences. We design
                  learner-aware teaching algorithms and show that significant
                  performance improvements can be achieved over learner-agnostic
                  teaching.},
  archivePrefix   = {arXiv},
  eprint          = {1906.00429},
  primaryClass    = {cs.LG},
}

@misc{ucla_causal_discus,
  author          = {UCLSA},
  howpublished    = {http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html},
  note            = {Online; accessed 11 February 2019},
  title           = {CAUSALITY - Discussion},
  year            = {nil},
}

@article{urbanczik09_gradien_learn_rule_tempot,
  author          = {Robert Urbanczik and Walter Senn},
  title           = {A Gradient Learning Rule for the Tempotron},
  journal         = {Neural Computation},
  volume          = 21,
  number          = 2,
  pages           = {340-352},
  year            = 2009,
  doi             = {10.1162/neco.2008.09-07-605},
  url             = {https://doi.org/10.1162/neco.2008.09-07-605},
  DATE_ADDED      = {Fri Nov 1 16:00:33 2019},
}

@inproceedings{van2016deep,
  title           = {Deep reinforcement learning with double q-learning},
  author          = {Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle       = {Thirtieth AAAI conference on artificial intelligence},
  year            = 2016
}

@article{victor2005spike,
  title           = {Spike train metrics},
  author          = {Victor, Jonathan D},
  journal         = {Current opinion in neurobiology},
  volume          = 15,
  number          = 5,
  pages           = {585--592},
  year            = 2005,
  publisher       = {Elsevier}
}

@article{wang18_dkn,
  author          = {Wang, Hongwei and Zhang, Fuzheng and Xie, Xing and Guo,
                  Minyi},
  title           = {Dkn: Deep Knowledge-Aware Network for News Recommendation},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1801.08284v2},
  abstract        = {Online news recommender systems aim to address the
                  information explosion of news and make personalized
                  recommendation for users. In general, news language is highly
                  condensed, full of knowledge entities and common sense.
                  However, existing methods are unaware of such external
                  knowledge and cannot fully discover latent knowledge-level
                  connections among news. The recommended results for a user are
                  consequently limited to simple patterns and cannot be extended
                  reasonably. Moreover, news recommendation also faces the
                  challenges of high time-sensitivity of news and dynamic
                  diversity of users' interests. To solve the above problems, in
                  this paper, we propose a deep knowledge-aware network (DKN)
                  that incorporates knowledge graph representation into news
                  recommendation. DKN is a content-based deep recommendation
                  framework for click-through rate prediction. The key component
                  of DKN is a multi-channel and word-entity-aligned
                  knowledge-aware convolutional neural network (KCNN) that fuses
                  semantic-level and knowledge-level representations of news.
                  KCNN treats words and entities as multiple channels, and
                  explicitly keeps their alignment relationship during
                  convolution. In addition, to address users' diverse interests,
                  we also design an attention module in DKN to dynamically
                  aggregate a user's history with respect to current candidate
                  news. Through extensive experiments on a real online news
                  platform, we demonstrate that DKN achieves substantial gains
                  over state-of-the-art deep recommendation models. We also
                  validate the efficacy of the usage of knowledge in DKN.},
  archivePrefix   = {arXiv},
  eprint          = {1801.08284},
  primaryClass    = {stat.ML},
}

@article{whittington19_theor_error_back_propag_brain,
  author          = {James C.R. Whittington and Rafal Bogacz},
  title           = {Theories of Error Back-Propagation in the Brain},
  journal         = {Trends in Cognitive Sciences},
  volume          = 23,
  number          = 3,
  pages           = {235-250},
  year            = 2019,
  doi             = {10.1016/j.tics.2018.12.005},
  url             = {https://doi.org/10.1016/j.tics.2018.12.005},
  DATE_ADDED      = {Tue Aug 20 10:09:27 2019},
}

@misc{wiki_pubsub,
  author          = {nil},
  howpublished    =
                  {http://wiki.ros.org/ROS/Tutorials/WritingPublisherSubscriber
                  \%28python \%29},
  note            = {Online; accessed 17 October 2019},
  title           = {ROS/Tutorials/WritingPublisherSubscriber(python) - ROS
                  Wiki},
  year            = {nil},
}

@misc{wiki_service,
  author          = {nil},
  howpublished    = {http://wiki.ros.org/ROS/Tutorials/WritingServiceClient
                  \%28python \%29},
  note            = {Online; accessed 17 October 2019},
  title           = {ROS/Tutorials/WritingServiceClient(python) - ROS Wiki},
  year            = {nil},
}

@article{wilson2019bayesian,
  title           = {The Case for {B}ayesian Deep Learning},
  author          = {Wilson, Andrew Gordon},
  note            = {Accessible at
                  \url{https://cims.nyu.edu/~andrewgw/caseforbdl.pdf}},
  journal         = {NYU Courant Technical Report},
  year            = 2019
}

@article{woodward19_learn_to_inter_learn_assis,
  author          = {Woodward, Mark and Finn, Chelsea and Hausman, Karol},
  title           = {Learning To Interactively Learn and Assist},
  journal         = {CoRR},
  year            = 2019,
  url             = {http://arxiv.org/abs/1906.10187v2},
  abstract        = {When deploying autonomous agents in the real world, we need
                  effective ways of communicating objectives to them.
                  Traditional skill learning has revolved around reinforcement
                  and imitation learning, each with rigid constraints on the
                  format of information exchanged between the human and the
                  agent. While scalar rewards carry little information,
                  demonstrations require significant effort to provide and may
                  carry more information than is necessary. Furthermore, rewards
                  and demonstrations are often defined and collected before
                  training begins, when the human is most uncertain about what
                  information would help the agent. In contrast, when humans
                  communicate objectives with each other, they make use of a
                  large vocabulary of informative behaviors, including
                  non-verbal communication, and often communicate throughout
                  learning, responding to observed behavior. In this way, humans
                  communicate intent with minimal effort. In this paper, we
                  propose such interactive learning as an alternative to reward
                  or demonstration-driven learning. To accomplish this, we
                  introduce a multi-agent training framework that enables an
                  agent to learn from another agent who knows the current task.
                  Through a series of experiments, we demonstrate the emergence
                  of a variety of interactive learning behaviors, including
                  information-sharing, information-seeking, and
                  question-answering. Most importantly, we find that our
                  approach produces an agent that is capable of learning
                  interactively from a human user, without a set of explicit
                  demonstrations or a reward function, and achieving
                  significantly better performance cooperatively with a human
                  than a human performing the task alone.},
  archivePrefix   = {arXiv},
  eprint          = {1906.10187},
  primaryClass    = {cs.AI},
}

@article{you17_large_batch_train_convol_networ,
  author          = {You, Yang and Gitman, Igor and Ginsburg, Boris},
  title           = {Large Batch Training of Convolutional Networks},
  journal         = {CoRR},
  year            = 2017,
  url             = {http://arxiv.org/abs/1708.03888v3},
  abstract        = {A common way to speed up training of large convolutional
                  networks is to add computational units. Training is then
                  performed using data-parallel synchronous Stochastic Gradient
                  Descent (SGD) with mini-batch divided between computational
                  units. With an increase in the number of nodes, the batch size
                  grows. But training with large batch size often results in the
                  lower model accuracy. We argue that the current recipe for
                  large batch training (linear learning rate scaling with
                  warm-up) is not general enough and training may diverge. To
                  overcome this optimization difficulties we propose a new
                  training algorithm based on Layer-wise Adaptive Rate Scaling
                  (LARS). Using LARS, we scaled Alexnet up to a batch size of
                  8K, and Resnet-50 to a batch size of 32K without loss in
                  accuracy.},
  archivePrefix   = {arXiv},
  eprint          = {1708.03888},
  primaryClass    = {cs.CV},
}

@article{zhu18_overv_machin_teach,
  author          = {Zhu, Xiaojin and Singla, Adish and Zilles, Sandra and
                  Rafferty, Anna N.},
  title           = {An Overview of Machine Teaching},
  journal         = {CoRR},
  year            = 2018,
  url             = {http://arxiv.org/abs/1801.05927v1},
  abstract        = {In this paper we try to organize machine teaching as a
                  coherent set of ideas. Each idea is presented as varying along
                  a dimension. The collection of dimensions then form the
                  problem space of machine teaching, such that existing teaching
                  problems can be characterized in this space. We hope this
                  organization allows us to gain deeper understanding of
                  individual teaching problems, discover connections among them,
                  and identify gaps in the field.},
  archivePrefix   = {arXiv},
  eprint          = {1801.05927},
  primaryClass    = {cs.LG},
}

@inproceedings{ziebart2008_maxentrl,
  author          = {Ziebart, Brian and Maas, Andrew and Bagnell, J. and Dey,
                  Anind},
  year            = 2008,
  month           = 01,
  pages           = {1433-1438},
  title           = {Maximum Entropy Inverse Reinforcement Learning.}
}
