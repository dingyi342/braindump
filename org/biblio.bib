
@article{10.3389/fnbot.2019.00018,
  title = {Supervised Learning in {{SNN}} via Reward-Modulated Spike-Timing-Dependent Plasticity for a Target Reaching Vehicle},
  author = {Bing, Zhenshan and Baumann, Ivan and Jiang, Zhuangyi and Huang, Kai and Cai, Caixia and Knoll, Alois},
  year = {2019},
  volume = {13},
  pages = {18},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2019.00018},
  abstract = {Spiking neural networks (SNNs) offer many advantages over traditional artificial neural networks (ANNs) such as biological plausibility, fast information processing, and energy efficiency. Although SNNs have been used to solve a variety of control tasks using the Spike-Timing-Dependent Plasticity (STDP) learning rule, existing solutions usually involve hard-coded network architectures solving specific tasks rather than solving different kinds of tasks generally. This results in neglecting one of the biggest advantages of ANNs, i.e., being general-purpose and easy-to-use due to their simple network architecture, which usually consists of an input layer, one or multiple hidden layers and an output layer. This paper addresses the problem by introducing an end-to-end learning approach of spiking neural networks constructed with one hidden layer and reward-modulated Spike-Timing-Dependent Plasticity (R-STDP) synapses in an all-to-all fashion. We use the supervised reward-modulated Spike-Timing-Dependent-Plasticity learning rule to train two different SNN-based sub-controllers to replicate a desired obstacle avoiding and goal approaching behavior, provided by pre-generated datasets. Together they make up a target-reaching controller, which is used to control a simulated mobile robot to reach a target area while avoiding obstacles in its path. We demonstrate the performance and effectiveness of our trained SNNs to achieve target reaching tasks in different unknown scenarios.},
  journal = {Frontiers in Neurorobotics}
}

@article{10.3389/fncom.2017.00024,
  title = {Equilibrium Propagation: {{Bridging}} the Gap between Energy-Based Models and Backpropagation},
  author = {Scellier, Benjamin and Bengio, Yoshua},
  year = {2017},
  volume = {11},
  pages = {24},
  issn = {1662-5188},
  doi = {10.3389/fncom.2017.00024},
  abstract = {We introduce Equilibrium Propagation, a learning framework for energy-based models. It involves only one kind of neural computation, performed in both the first phase (when the prediction is made) and the second phase of training (after the target or prediction error is revealed). Although this algorithm computes the gradient of an objective function just like Backpropagation, it does not need a special computation or circuit for the second phase, where errors are implicitly propagated. Equilibrium Propagation shares similarities with Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: our algorithm computes the gradient of a well-defined objective function. Because the objective function is defined in terms of local perturbations, the second phase of Equilibrium Propagation corresponds to only nudging the prediction (fixed point or stationary distribution) toward a configuration that reduces prediction error. In the case of a recurrent multi-layer supervised network, the output units are slightly nudged toward their target in the second phase, and the perturbation introduced at the output layer propagates backward in the hidden layers. We show that the signal ``back-propagated'' during this second phase corresponds to the propagation of error derivatives and encodes the gradient of the objective function, when the synaptic update corresponds to a standard form of spike-timing dependent plasticity. This work makes it more plausible that a mechanism similar to Backpropagation could be implemented by brains, since leaky integrator neural computation performs both inference and error back-propagation in our model. The only local difference between the two phases is whether synaptic changes are allowed or not. We also show experimentally that multi-layer recurrently connected networks with 1, 2, and 3 hidden layers can be trained by Equilibrium Propagation on the permutation-invariant MNIST task.},
  journal = {Frontiers in Computational Neuroscience}
}

@article{10.3389/fninf.2018.00089,
  title = {{{BindsNET}}: {{A}} Machine Learning-Oriented Spiking Neural Networks Library in Python},
  author = {Hazan, Hananel and Saunders, Daniel J. and Khan, Hassaan and Patel, Devdhar and Sanghavi, Darpan T. and Siegelmann, Hava T. and Kozma, Robert},
  year = {2018},
  volume = {12},
  pages = {89},
  issn = {1662-5196},
  doi = {10.3389/fninf.2018.00089},
  journal = {Frontiers in Neuroinformatics}
}

@article{10.3389/fnins.2015.00481,
  title = {Poker-Dvs and {{MNIST}}-{{DVS}}. {{Their}} History, How They Were Made, and Other Details},
  author = {{Serrano-Gotarredona}, Teresa and {Linares-Barranco}, Bernab{\'e}},
  year = {2015},
  volume = {9},
  pages = {481},
  issn = {1662-453X},
  doi = {10.3389/fnins.2015.00481},
  abstract = {This article reports on two databases for event-driven object recognition using a Dynamic Vision Sensor (DVS). The first, which we call Poker-DVS and is being released together with this article, was obtained by browsing specially made poker card decks in front of a DVS camera for 2\textendash{}4 s. Each card appeared on the screen for about 20\textendash{}30 ms. The poker pips were tracked and isolated off-line to constitute the 131-recording Poker-DVS database. The second database, which we call MNIST-DVS and which was released in December 2013, consists of a set of 30,000 DVS camera recordings obtained by displaying 10,000 moving symbols from the standard MNIST 70,000-picture database on an LCD monitor for about 2\textendash{}3 s each. Each of the 10,000 symbols was displayed at three different scales, so that event-driven object recognition algorithms could easily be tested for different object sizes. This article tells the story behind both databases, covering, among other aspects, details of how they work and the reasons for their creation. We provide not only the databases with corresponding scripts, but also the scripts and data used to generate the figures shown in this article (as Supplementary Material).},
  journal = {Frontiers in Neuroscience}
}

@article{10.3389/fnins.2016.00508,
  title = {Training Deep Spiking Neural Networks Using Backpropagation},
  author = {Lee, Jun Haeng and Delbruck, Tobi and Pfeiffer, Michael},
  year = {2016},
  volume = {10},
  pages = {508},
  issn = {1662-453X},
  doi = {10.3389/fnins.2016.00508},
  abstract = {Deep spiking neural networks (SNNs) hold the potential for improving the latency and energy efficiency of deep neural networks through data-driven event-based computation. However, training such networks is difficult due to the non-differentiable nature of spike events. In this paper, we introduce a novel technique, which treats the membrane potentials of spiking neurons as differentiable signals, where discontinuities at spike times are considered as noise. This enables an error backpropagation mechanism for deep SNNs that follows the same principles as in conventional deep networks, but works directly on spike signals and membrane potentials. Compared with previous methods relying on indirect training and conversion, our technique has the potential to capture the statistics of spikes more precisely. We evaluate the proposed framework on artificially generated events from the original MNIST handwritten digit benchmark, and also on the N-MNIST benchmark recorded with an event-based dynamic vision sensor, in which the proposed method reduces the error rate by a factor of more than three compared to the best previous SNN, and also achieves a higher accuracy than a conventional convolutional neural network (CNN) trained and tested on the same data. We demonstrate in the context of the MNIST task that thanks to their event-driven operation, deep SNNs (both fully connected and convolutional) trained with our method achieve accuracy equivalent with conventional neural networks. In the N-MNIST example, equivalent accuracy is achieved with about five times fewer computational operations.},
  file = {/home/jethro/Zotero/storage/EUP5P3XN/Lee et al. - 2016 - Training deep spiking neural networks using backpr.pdf},
  journal = {Frontiers in Neuroscience}
}

@article{10.3389/fnins.2018.00435,
  title = {Training Deep Spiking Convolutional Neural Networks with {{STDP}}-{{Based}} Unsupervised Pre-Training Followed by Supervised Fine-Tuning},
  author = {Lee, Chankyu and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  year = {2018},
  volume = {12},
  pages = {435},
  issn = {1662-453X},
  doi = {10.3389/fnins.2018.00435},
  abstract = {Spiking Neural Networks (SNNs) are fast becoming a promising candidate for brain-inspired neuromorphic computing because of their inherent power efficiency and impressive inference accuracy across several cognitive tasks such as image classification and speech recognition. The recent efforts in SNNs have been focused on implementing deeper networks with multiple hidden layers to incorporate exponentially more difficult functional representations. In this paper, we propose a pre-training scheme using biologically plausible unsupervised learning, namely Spike-Timing-Dependent-Plasticity (STDP), in order to better initialize the parameters in multi-layer systems prior to supervised optimization. The multi-layer SNN is comprised of alternating convolutional and pooling layers followed by fully-connected layers, which are populated with leaky integrate-and-fire spiking neurons. We train the deep SNNs in two phases wherein, first, convolutional kernels are pre-trained in a layer-wise manner with unsupervised learning followed by fine-tuning the synaptic weights with spike-based supervised gradient descent backpropagation. Our experiments on digit recognition demonstrate that the STDP-based pre-training with gradient-based optimization provides improved robustness, faster ( 2.5 \texttimes{}) training time and better generalization compared with purely gradient-based training without pre-training.},
  journal = {Frontiers in Neuroscience}
}

@article{17_bartek,
  title = {Bartek's Coding Blog: {{How}} Not\textsubscript{n}ull Can Improve Your Code?},
  author = {{Bartek}},
  year = {2017},
  annote = {Online; accessed 02 April 2019}
}

@inproceedings{abbeel2004apprenticeship,
  title = {Apprenticeship Learning via Inverse Reinforcement Learning},
  booktitle = {Proceedings of the Twenty-First International Conference on {{Machine}} Learning},
  author = {Abbeel, Pieter and Ng, Andrew Y},
  year = {2004},
  pages = {1},
  organization = {{ACM}}
}

@article{aenugu19_reinf_learn_with_spikin_coagen,
  title = {Reinforcement Learning with Spiking Coagents},
  author = {Aenugu, Sneha and Sharma, Abhishek and Yelamarthi, Sasikiran and Hazan, Hananel and Thomas, Philip S. and Kozma, Robert},
  year = {2019},
  abstract = {Neuroscientific theory suggests that dopaminergic neurons broadcast global reward prediction errors to large areas of the brain influencing the synaptic plasticity of the neurons in those regions. We build on this theory to propose a multi-agent learning framework with spiking neurons in the generalized linear model (GLM) formulation as agents, to solve reinforcement learning (RL) tasks. We show that a network of GLM spiking agents connected in a hierarchical fashion, where each spiking agent modulates its firing policy based on local information and a global prediction error, can learn complex action representations to solve RL tasks. We further show how leveraging principles of modularity and population coding inspired from the brain can help reduce variance in the learning updates making it a viable optimization technique.},
  archivePrefix = {arXiv},
  eprint = {1910.06489},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{aiskinLee,
  title = {A Neuro-Inspired Artificial Peripheral Nervous System for Scalable Electronic Skins},
  author = {Lee, Wang Wei and Tan, Yu Jun and Yao, Haicheng and Li, Si and See, Hian Hian and Hon, Matthew and Ng, Kian Ann and Xiong, Betty and Ho, John S. and Tee, Benjamin C. K.},
  year = {2019},
  volume = {4},
  publisher = {{Science Robotics}},
  doi = {10.1126/scirobotics.aax2198},
  abstract = {The human sense of touch is essential for dexterous tool usage, spatial awareness, and social communication. Equipping intelligent human-like androids and prosthetics with electronic skins\textemdash{}a large array of sensors spatially distributed and capable of rapid somatosensory perception\textemdash{}will enable them to work collaboratively and naturally with humans to manipulate objects in unstructured living environments. Previously reported tactile-sensitive electronic skins largely transmit the tactile information from sensors serially, resulting in readout latency bottlenecks and complex wiring as the number of sensors increases. Here, we introduce the Asynchronously Coded Electronic Skin (ACES)\textemdash{}a neuromimetic architecture that enables simultaneous transmission of thermotactile information while maintaining exceptionally low readout latencies, even with array sizes beyond 10,000 sensors. We demonstrate prototype arrays of up to 240 artificial mechanoreceptors that transmitted events asynchronously at a constant latency of 1 ms while maintaining an ultra-high temporal precision of \&lt;60 ns, thus resolving fine spatiotemporal features necessary for rapid tactile perception. Our platform requires only a single electrical conductor for signal propagation, realizing sensor arrays that are dynamically reconfigurable and robust to damage. We anticipate that the ACES platform can be integrated with a wide range of skin-like sensors for artificial intelligence (AI)\textendash{}enhanced autonomous robots, neuroprosthetics, and neuromorphic computing hardware for dexterous object manipulation and somatosensory perception.},
  elocation-id = {eaax2198},
  eprint = {https://robotics.sciencemag.org/content/4/32/eaax2198.full.pdf},
  journal = {Science Robotics},
  number = {32}
}

@inproceedings{andrychowicz2017hindsight,
  title = {Hindsight Experience Replay},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and Zaremba, Wojciech},
  year = {2017},
  pages = {5048--5058}
}

@inproceedings{anschel2017averaged,
  title = {Averaged-Dqn: {{Variance}} Reduction and Stabilization for Deep Reinforcement Learning},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  author = {Anschel, Oron and Baram, Nir and Shimkin, Nahum},
  year = {2017},
  pages = {176--185},
  organization = {{JMLR. org}}
}

@article{arjovsky19_invar_risk_minim,
  title = {Invariant Risk Minimization},
  author = {Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and {Lopez-Paz}, David},
  year = {2019},
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  archivePrefix = {arXiv},
  eprint = {1907.02893},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {stat.ML}
}

@article{baltrusaitis17:_multim_machin_learn,
  title = {Multimodal Machine Learning: {{A}} Survey and Taxonomy},
  author = {Baltru{\v s}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  year = {2017},
  abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. Baltru, Tadas, Ahuja, C., \& Morency, L., Multimodal machine learning: a survey and taxonomy, CoRR, (), (2017). In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
  archivePrefix = {arXiv},
  eprint = {1705.09406},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{bellec18_long_short_term_memor_learn,
  title = {Long Short-Term Memory and Learning-to-Learn in Networks of Spiking Neurons},
  author = {Bellec, Guillaume and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
  year = {2018},
  abstract = {Recurrent networks of spiking neurons (RSNNs) underlie the astounding computing and learning capabilities of the brain. But computing and learning capabilities of RSNN models have remained poor, at least in comparison with artificial neural networks (ANNs). We address two possible reasons for that. One is that RSNNs in the brain are not randomly connected or designed according to simple rules, and they do not start learning as a tabula rasa network. Rather, RSNNs in the brain were optimized for their tasks through evolution, development, and prior experience. Details of these optimization processes are largely unknown. But their functional contribution can be approximated through powerful optimization methods, such as backpropagation through time (BPTT). A second major mismatch between RSNNs in the brain and models is that the latter only show a small fraction of the dynamics of neurons and synapses in the brain. We include neurons in our RSNN model that reproduce one prominent dynamical process of biological neurons that takes place at the behaviourally relevant time scale of seconds: neuronal adaptation. We denote these networks as LSNNs because of their Long short-term memory. The inclusion of adapting neurons drastically increases the computing and learning capability of RSNNs if they are trained and configured by deep learning (BPTT combined with a rewiring algorithm that optimizes the network architecture). In fact, the computational performance of these RSNNs approaches for the first time that of LSTM networks. In addition RSNNs with adapting neurons can acquire abstract knowledge from prior learning in a Learning-to-Learn (L2L) scheme, and transfer that knowledge in order to learn new but related tasks from very few examples. We demonstrate this for supervised learning and reinforcement learning.},
  archivePrefix = {arXiv},
  eprint = {1803.09574},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.NE}
}

@book{bishop2006pattern,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M},
  year = {2006},
  publisher = {{springer}}
}

@article{borman2004expectation,
  title = {The Expectation Maximization Algorithm-a Short Tutorial},
  author = {Borman, Sean},
  year = {2004},
  volume = {41},
  journal = {Submitted for publication}
}

@article{brown18_machin_teach_inver_reinf_learn,
  title = {Machine Teaching for Inverse Reinforcement Learning: {{Algorithms}} and Applications},
  author = {Brown, Daniel S. and Niekum, Scott},
  year = {2018},
  abstract = {Inverse reinforcement learning (IRL) infers a reward function from demonstrations, allowing for policy improvement and generalization. However, despite much recent interest in IRL, little work has been done to understand the minimum set of demonstrations needed to teach a specific sequential decision-making task. We formalize the problem of finding maximally informative demonstrations for IRL as a machine teaching problem where the goal is to find the minimum number of demonstrations needed to specify the reward equivalence class of the demonstrator. We extend previous work on algorithmic teaching for sequential decision-making tasks by showing a reduction to the set cover problem which enables an efficient approximation algorithm for determining the set of maximally-informative demonstrations. We apply our proposed machine teaching algorithm to two novel applications: providing a lower bound on the number of queries needed to learn a policy using active IRL and developing a novel IRL algorithm that can learn more efficiently from informative demonstrations than a standard IRL approach.},
  archivePrefix = {arXiv},
  eprint = {1805.07687},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{buckar_c_packag_manag_missin,
  title = {Buckaroo - the {{C}}++ Package Manager Value\textsubscript{p}tr - the Missing {{C}}++ Smart-Pointer},
  author = {{Buckaroo}},
  year = {2019},
  annote = {Online; accessed 24 February 2019}
}

@article{camerer2004cognitive,
  title = {A Cognitive Hierarchy Model of Games},
  author = {Camerer, Colin F and Ho, Teck-Hua and Chong, Juin-Kuan},
  year = {2004},
  volume = {119},
  pages = {861--898},
  publisher = {{MIT Press}},
  journal = {The Quarterly Journal of Economics},
  number = {3}
}

@book{chatterjee06_regres_analy_examp,
  title = {Regression Analysis by Example},
  author = {Chatterjee, Samprit and Hadi, Ali S.},
  year = {2006},
  publisher = {{John Wiley \& Sons, Inc.}},
  doi = {10.1002/0470055464},
  date_added = {Tue Jan 15 13:13:16 2019},
  series = {Wiley Series in Probability and Statistics}
}

@article{chen16_xgboos,
  title = {Xgboost: A Scalable Tree Boosting System},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archivePrefix = {arXiv},
  eprint = {1603.02754},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{chen18_neural_ordin_differ_equat,
  title = {Neural Ordinary Differential Equations},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2018},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archivePrefix = {arXiv},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{chen18_neural_ordin_differ_equat,
  title = {Neural Ordinary Differential Equations},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2018},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archivePrefix = {arXiv},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{chen20_simpl_framew_contr_learn_visual_repres,
  title = {A Simple Framework for Contrastive Learning of Visual Representations},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5 \% top-1 accuracy, which is a 7 \% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1 \% of the labels, we achieve 85.8 \% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archivePrefix = {arXiv},
  eprint = {2002.05709},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{chen3rabit,
  title = {{{RABIT}}: {{A}} Reliable Allreduce and Broadcast Interface},
  author = {Chen, Tianqi and Cano, Ignacio and Zhou, Tianyi},
  volume = {3},
  journal = {Transfer},
  number = {2}
}

@article{codesbay_codes_cplus_smart,
  title = {{{GitHub}} - {{CodesBay}}/{{CplusPlus}}{{{\textsubscript{S}}}}{{martPointer}}: {{This}} Repository Contains Description of {{C}}++11 and {{C}}++14 {{Smart Pointers Trilogy}} of Shared\textsubscript{p}tr, Unique\textsubscript{p}tr and Weak\textsubscript{p}tr},
  author = {{CodesBay}},
  year = {nil},
  annote = {Online; accessed 25 February 2019}
}

@article{colyer_tmp_neural_ode,
  title = {Neural Ordinary Differential Equations},
  author = {Colyer, Adrian},
  year = {2019},
  annote = {Online; accessed 28 February 2019}
}

@article{comsa19_tempor_codin_spikin_neural_networ,
  title = {Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function},
  author = {Comsa, Iulia M. and Potempa, Krzysztof and Versari, Luca and Fischbacher, Thomas and Gesmundo, Andrea and Alakuijala, Jyrki},
  year = {2019},
  abstract = {The timing of individual neuronal spikes is essential for biological brains to make fast responses to sensory stimuli. However, conventional artificial neural networks lack the intrinsic temporal coding ability present in biological networks. We propose a spiking neural network model that encodes information in the relative timing of individual neuron spikes. In classification tasks, the output of the network is indicated by the first neuron to spike in the output layer. This temporal coding scheme allows the supervised training of the network with backpropagation, using locally exact derivatives of the postsynaptic spike times with respect to presynaptic spike times. The network operates using a biologically-plausible alpha synaptic transfer function. Additionally, we use trainable synchronisation pulses that provide bias, add flexibility during training and exploit the decay part of the alpha function. We show that such networks can be trained successfully on noisy Boolean logic tasks and on the MNIST dataset encoded in time. The results show that the spiking neural network outperforms comparable spiking models on MNIST and achieves similar quality to fully connected conventional networks with the same architecture. We also find that the spiking network spontaneously discovers two operating regimes, mirroring the accuracy-speed trade-off observed in human decision-making: a slow regime, where a decision is taken after all hidden neurons have spiked and the accuracy is very high, and a fast regime, where a decision is taken very fast but the accuracy is lower. These results demonstrate the computational power of spiking networks with biological characteristics that encode information in the timing of individual neurons. By studying temporal coding in spiking networks, we aim to create building blocks towards energy-efficient and more complex biologically-inspired neural architectures.},
  archivePrefix = {arXiv},
  eprint = {1907.13223},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.NE}
}

@article{comsa19_tempor_codin_spikin_neural_networ,
  title = {Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function},
  author = {Comsa, Iulia M. and Potempa, Krzysztof and Versari, Luca and Fischbacher, Thomas and Gesmundo, Andrea and Alakuijala, Jyrki},
  year = {2019},
  abstract = {The timing of individual neuronal spikes is essential for biological brains to make fast responses to sensory stimuli. However, conventional artificial neural networks lack the intrinsic temporal coding ability present in biological networks. We propose a spiking neural network model that encodes information in the relative timing of individual neuron spikes. In classification tasks, the output of the network is indicated by the first neuron to spike in the output layer. This temporal coding scheme allows the supervised training of the network with backpropagation, using locally exact derivatives of the postsynaptic spike times with respect to presynaptic spike times. The network operates using a biologically-plausible alpha synaptic transfer function. Additionally, we use trainable synchronisation pulses that provide bias, add flexibility during training and exploit the decay part of the alpha function. We show that such networks can be trained successfully on noisy Boolean logic tasks and on the MNIST dataset encoded in time. The results show that the spiking neural network outperforms comparable spiking models on MNIST and achieves similar quality to fully connected conventional networks with the same architecture. We also find that the spiking network spontaneously discovers two operating regimes, mirroring the accuracy-speed trade-off observed in human decision-making: a slow regime, where a decision is taken after all hidden neurons have spiked and the accuracy is very high, and a fast regime, where a decision is taken very fast but the accuracy is lower. These results demonstrate the computational power of spiking networks with biological characteristics that encode information in the timing of individual neurons. By studying temporal coding in spiking networks, we aim to create building blocks towards energy-efficient and more complex biologically-inspired neural architectures.},
  archivePrefix = {arXiv},
  eprint = {1907.13223},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.NE}
}

@article{cppref_raii,
  title = {{{RAII}} - Cppreference.Com},
  author = {{nil}},
  year = {nil},
  annote = {Online; accessed 25 January 2019}
}

@article{Cybenko1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  year = {1989},
  month = dec,
  volume = {2},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  journal = {Mathematics of Control, Signals and Systems},
  number = {4}
}

@inproceedings{dabney2018distributional,
  title = {Distributional Reinforcement Learning with Quantile Regression},
  booktitle = {Thirty-Second {{AAAI}} Conference on Artificial Intelligence},
  author = {Dabney, Will and Rowland, Mark and Bellemare, Marc G and Munos, R{\'e}mi},
  year = {2018}
}

@article{dacrema19_are_we_reall_makin_much_progr,
  title = {Are We Really Making Much Progress? {{A}} Worrying Analysis of Recent Neural Recommendation Approaches},
  author = {Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach, Dietmar},
  year = {2019},
  abstract = {Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models. In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable effort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientific practices in this area. Source code of our experiments and full results are available at: https://github.com/MaurizioFD/RecSys2019\textsubscript{D}eepLearning\textsubscript{E}valuation.},
  archivePrefix = {arXiv},
  eprint = {1907.06902v1},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.IR}
}

@article{dan_reprod,
  title = {Reproducible Research: {{Stripe}}'s Approach to Data Science},
  author = {Frank, Dan},
  year = {2016},
  annote = {Online; accessed 06 January 2019}
}

@inproceedings{dann2017unifying,
  title = {Unifying {{PAC}} and Regret: {{Uniform PAC}} Bounds for Episodic Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  year = {2017},
  pages = {5713--5723}
}

@article{davies2018loihi,
  title = {Loihi: {{A}} Neuromorphic Manycore Processor with on-Chip Learning},
  author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and others},
  year = {2018},
  volume = {38},
  pages = {82--99},
  publisher = {{IEEE}},
  journal = {IEEE Micro},
  number = {1}
}

@book{DBLP:books/oreilly/Kleppmann2014,
  title = {Designing Data-Intensive Applications: {{The}} Big Ideas behind Reliable, Scalable, and Maintainable Systems},
  author = {Kleppmann, Martin},
  year = {2016},
  publisher = {{O'Reilly}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/books/oreilly/Kleppmann2014},
  isbn = {978-1-4493-7332-0},
  timestamp = {Mon, 27 May 2019 17:50:46 +0200}
}

@inproceedings{dremel,
  title = {Dremel: {{Interactive}} Analysis of Web-Scale Datasets},
  booktitle = {Proc. of the 36th Int'l Conf on Very Large Data Bases},
  author = {Melnik, Sergey and Gubarev, Andrey and Long, Jing Jing and Romer, Geoffrey and Shivakumar, Shiva and Tolton, Matt and Vassilakis, Theo},
  year = {2010},
  pages = {330--339}
}

@article{drepper2007every,
  title = {What Every Programmer Should Know about Memory},
  author = {Drepper, Ulrich},
  year = {2007}
}

@inproceedings{Du2010APA,
  title = {A {{POMDP}} Approach to Robot Motion Planning under Uncertainty},
  author = {Du, Yanzhu and Hsu, David and Kurniawati, Hanna and Lee, Wee Sun and Ong, Sylvie C. W. and Png, Shao Wei},
  year = {2010}
}

@article{dupont19_augmen_neural_odes,
  title = {Augmented Neural Odes},
  author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
  year = {2019},
  abstract = {We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.},
  archivePrefix = {arXiv},
  eprint = {1904.01681},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {stat.ML}
}

@article{espeholt18_impal,
  title = {Impala: {{Scalable}} Distributed Deep-Rl with Importance Weighted Actor-Learner Architectures},
  author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  year = {2018},
  abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
  archivePrefix = {arXiv},
  eprint = {1802.01561},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{eysenbach18_diver_is_all_you_need,
  title = {Diversity Is All You Need: {{Learning}} Skills without a Reward Function},
  author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  year = {2018},
  abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
  archivePrefix = {arXiv},
  eprint = {1802.06070},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.AI}
}

@article{felipe_demys_join_algor,
  title = {Demystifying {{JOIN}} Algorithms},
  author = {Carvalho, Felipe Oliveira},
  year = {2019},
  annote = {Online; accessed 03 February 2019}
}

@article{florian07_reinf_learn_throug_modul_spike,
  title = {Reinforcement Learning through Modulation of Spike-Timing-Dependent Synaptic Plasticity},
  author = {Florian, R{\u a}zvan V.},
  year = {2007},
  volume = {19},
  pages = {1468--1502},
  doi = {10.1162/neco.2007.19.6.1468},
  date_added = {Mon Nov 4 15:30:29 2019},
  journal = {Neural Computation},
  number = {6}
}

@inproceedings{florian2005,
  title = {A Reinforcement Learning Algorithm for Spiking Neural Networks},
  author = {Florian, R{\u a}zvan},
  year = {2005},
  month = oct,
  volume = {2005},
  pages = {8 pp.-},
  doi = {10.1109/SYNASC.2005.13},
  isbn = {0-7695-2453-2},
  series = {Proceedings - {{Seventh International Symposium}} on {{Symbolic}} and {{Numeric Algorithms}} for {{Scientific Computing}}, {{SYNASC}} 2005}
}

@article{gary_networ_protoc,
  title = {Network Protocols \&ndash; Programmer's Compendium},
  author = {Bernhardt, Gary},
  year = {2019},
  annote = {Online; accessed 25 January 2019}
}

@book{gerstner2002spiking,
  title = {Spiking Neuron Models: {{Single}} Neurons, Populations, Plasticity},
  author = {Gerstner, Wulfram and Kistler, Werner M},
  year = {2002},
  publisher = {{Cambridge university press}}
}

@article{gibson18_neural_ordin_differ_equat,
  title = {Neural Networks as Ordinary Differential Equations},
  author = {Gibson, Kevin},
  year = {2018},
  annote = {Online; accessed 28 February 2019}
}

@inproceedings{googlecartographer,
  title = {Real-Time Loop Closure in {{2D LIDAR SLAM}}},
  booktitle = {2016 {{IEEE}} International Conference on Robotics and Automation ({{ICRA}})},
  author = {Hess, Wolfgang and Kohler, Damon and Rapp, Holger and Andor, Daniel},
  year = {2016},
  pages = {1271--1278}
}

@article{gu16_q_prop,
  title = {Q-Prop: {{Sample}}-Efficient Policy Gradient with an off-Policy Critic},
  author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
  year = {2016},
  abstract = {Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.},
  archivePrefix = {arXiv},
  eprint = {1611.02247},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{guetig14_to_spike_or_when_to_spike,
  title = {To Spike, or When to Spike?},
  author = {G{\"u}tig, Robert},
  year = {2014},
  volume = {25},
  pages = {134--139},
  doi = {10.1016/j.conb.2014.01.004},
  date_added = {Fri Nov 1 16:23:04 2019},
  journal = {Current Opinion in Neurobiology},
  number = {nil}
}

@article{guo17_deepf,
  title = {Deepfm: A Factorization-Machine Based Neural Network for Ctr Prediction},
  author = {Guo, Huifeng and Tang, Ruiming and Ye, Yunming and Li, Zhenguo and He, Xiuqiang},
  year = {2017},
  abstract = {Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \& Deep model from Google, DeepFM has a shared input to its "wide" and "deep" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.},
  archivePrefix = {arXiv},
  eprint = {1703.04247},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.IR}
}

@article{haan19_causal_confus_imitat_learn,
  title = {Causal Confusion in Imitation Learning},
  author = {de Haan, Pim and Jayaraman, Dinesh and Levine, Sergey},
  year = {2019},
  abstract = {Behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. Such discriminative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. We point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. In particular, it leads to a counter-intuitive "causal misidentification" phenomenon: access to more information can yield worse performance. We investigate how this problem arises, and propose a solution to combat it through targeted interventions\textemdash{}either environment interaction or expert queries\textemdash{}to determine the correct causal model. We show that causal misidentification occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against DAgger and other baselines and ablations.},
  archivePrefix = {arXiv},
  eprint = {1905.11979},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{haber17_stabl_archit_deep_neural_networ,
  title = {Stable Architectures for Deep Neural Networks},
  author = {Haber, Eldad and Ruthotto, Lars},
  year = {2017},
  abstract = {Deep neural networks have become invaluable tools for supervised machine learning, e.g., classification of text or images. While often offering superior results over traditional techniques and successfully expressing complicated patterns in data, deep architectures are known to be challenging to design and train such that they generalize well to new data. Important issues with deep architectures are numerical instabilities in derivative-based learning algorithms commonly called exploding or vanishing gradients. In this paper we propose new forward propagation techniques inspired by systems of Ordinary Differential Equations (ODE) that overcome this challenge and lead to well-posed learning problems for arbitrarily deep networks. The backbone of our approach is our interpretation of deep learning as a parameter estimation problem of nonlinear dynamical systems. Given this formulation, we analyze stability and well-posedness of deep learning and use this new understanding to develop new network architectures. We relate the exploding and vanishing gradient phenomenon to the stability of the discrete ODE and present several strategies for stabilizing deep learning for very deep networks. While our new architectures restrict the solution space, several numerical experiments show their competitiveness with state-of-the-art networks.},
  archivePrefix = {arXiv},
  eprint = {1705.03341},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{handmade_how_to_write_better,
  title = {How to Write Better (Game) Libraries | Handmade.Network {{Wiki}}},
  author = {{Handmade}},
  year = {2019},
  annote = {Online; accessed 12 December 2019}
}

@article{haug18_teach_inver_reinf_learn_via_featur_demon,
  title = {Teaching Inverse Reinforcement Learners via Features and Demonstrations},
  author = {Haug, Luis and Tschiatschek, Sebastian and Singla, Adish},
  year = {2018},
  abstract = {Learning near-optimal behaviour from an expert's demonstrations typically relies on the assumption that the learner knows the features that the true reward function depends on. In this paper, we study the problem of learning from demonstrations in the setting where this is not the case, i.e., where there is a mismatch between the worldviews of the learner and the expert. We introduce a natural quantity, the teaching risk, which measures the potential suboptimality of policies that look optimal to the learner in this setting. We show that bounds on the teaching risk guarantee that the learner is able to find a near-optimal policy using standard algorithms based on inverse reinforcement learning. Based on these findings, we suggest a teaching scheme in which the expert can decrease the teaching risk by updating the learner's worldview, and thus ultimately enable her to find a near-optimal policy.},
  archivePrefix = {arXiv},
  eprint = {1810.08926},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{he15_deep_resid_learn_image_recog,
  title = {Deep Residual Learning for Image Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\textemdash{}8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57 \% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28 \% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archivePrefix = {arXiv},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.CV}
}

@article{heeger2000poisson,
  title = {Poisson Model of Spike Generation},
  author = {Heeger, David},
  year = {2000},
  volume = {5},
  pages = {1--13},
  journal = {Handout, University of Standford}
}

@article{heeger2000poisson,
  title = {Poisson Model of Spike Generation},
  author = {Heeger, David},
  year = {2000},
  volume = {5},
  pages = {1--13},
  journal = {Handout, University of Standford}
}

@article{home_cookiec_data_scien,
  title = {Home - Cookiecutter Data Science},
  author = {{DrivenData}},
  year = {2019},
  annote = {Online; accessed 06 January 2019}
}

@article{home_keras_docum,
  title = {Home Keras Documentation},
  author = {{Keras}},
  year = {2019},
  annote = {Online; accessed 08 January 2019}
}

@article{horgan18_distr_prior_exper_replay,
  title = {Distributed Prioritized Experience Replay},
  author = {Horgan, Dan and Quan, John and Budden, David and {Barth-Maron}, Gabriel and Hessel, Matteo and van Hasselt, Hado and Silver, David},
  year = {2018},
  abstract = {We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.},
  archivePrefix = {arXiv},
  eprint = {1803.00933},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{houghton18_calcul_mutual_infor_between_two_spike_train,
  title = {Calculating the Mutual Information between Two Spike Trains},
  author = {Houghton, Conor},
  year = {2018},
  doi = {10.1101/423608},
  date_added = {Thu Jan 23 11:52:27 2020}
}

@article{huang18_gpipe,
  title = {Gpipe: {{Efficient}} Training of Giant Neural Networks Using Pipeline Parallelism},
  author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
  year = {2018},
  abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4 \% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
  archivePrefix = {arXiv},
  eprint = {1811.06965},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.CV}
}

@article{huh17_gradien_descen_spikin_neural_networ,
  title = {Gradient Descent for Spiking Neural Networks},
  author = {Huh, Dongsung and Sejnowski, Terrence J.},
  year = {2017},
  abstract = {Much of studies on neural computation are based on network models of static neurons that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast ( millisecond) spike-based interactions for efficient encoding of information, and a delayed memory XOR task over extended duration ( second). The results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as behavioral time scales. In conclusion, our result offers a general purpose supervised learning algorithm for spiking neural networks, thus advancing further investigations on spike-based computation.},
  archivePrefix = {arXiv},
  eprint = {1706.04698},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {q-bio.NC}
}

@article{ivanov19_moder_deep_reinf_learn_algor,
  title = {Modern Deep Reinforcement Learning Algorithms},
  author = {Ivanov, Sergey and D'yakonov, Alexander},
  year = {2019},
  abstract = {Recent advances in Reinforcement Learning, grounded on combining classical theoretical results with Deep Learning paradigm, led to breakthroughs in many artificial intelligence tasks and gave birth to Deep Reinforcement Learning (DRL) as a field of research. In this work latest DRL algorithms are reviewed with a focus on their theoretical justification, practical limitations and observed empirical properties.},
  archivePrefix = {arXiv},
  eprint = {1906.10025},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@book{james2013introduction,
  title = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  volume = {112},
  publisher = {{Springer}}
}

@article{jang18_introd_to_spikin_neural_networ,
  title = {An Introduction to Spiking Neural Networks: {{Probabilistic}} Models, Learning Rules, and Applications},
  author = {Jang, Hyeryung and Simeone, Osvaldo and Gardner, Brian and Gr{\"u}ning, Andr{\'e}},
  year = {2018},
  abstract = {Spiking Neural Networks (SNNs) are distributed trainable systems whose computing elements, or neurons, are characterized by internal analog dynamics and by digital and sparse synaptic communications. The sparsity of the synaptic spiking inputs and the corresponding event-driven nature of neural processing can be leveraged by hardware implementations that have demonstrated significant energy reductions as compared to conventional Artificial Neural Networks (ANNs). Most existing training algorithms for SNNs have been designed either for biological plausibility or through conversion from pre-trained ANNs via rate encoding. This paper aims at providing an introduction to SNNs by focusing on a probabilistic signal processing methodology that enables the direct derivation of learning rules leveraging the unique time encoding capabilities of SNNs. To this end, the paper adopts discrete-time probabilistic models for networked spiking neurons, and it derives supervised and unsupervised learning rules from first principles by using variational inference. Examples and open research problems are also provided.},
  archivePrefix = {arXiv},
  eprint = {1812.03929v4},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {eess.SP}
}

@incollection{jin_q_learning_provably_efficient,
  title = {Is Q-Learning Provably Efficient?},
  booktitle = {Advances in Neural Information Processing Systems 31},
  author = {Jin, Chi and {Allen-Zhu}, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {4863--4873},
  publisher = {{Curran Associates, Inc.}}
}

@article{johndrow15_optim_approx_markov_chain_bayes_infer,
  title = {Optimal Approximating Markov Chains for Bayesian Inference},
  author = {Johndrow, James E. and Mattingly, Jonathan C. and Mukherjee, Sayan and Dunson, David},
  year = {2015},
  abstract = {The Markov Chain Monte Carlo method is the dominant paradigm for posterior computation in Bayesian analysis. It is common to control computation time by making approximations to the Markov transition kernel. Comparatively little attention has been paid to computational optimality in these approximating Markov Chains, or when such approximations are justified relative to obtaining shorter paths from the exact kernel. We give simple, sharp bounds for uniform approximations of uniformly mixing Markov chains. We then suggest a notion of optimality that incorporates computation time and approximation error, and use our bounds to make generalizations about properties of good approximations in the uniformly mixing setting. The relevance of these properties is demonstrated in applications to a minibatching-based approximate MCMC algorithm for large n logistic regression and low-rank approximations for Gaussian processes.},
  archivePrefix = {arXiv},
  eprint = {1508.03387},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {stat.CO}
}

@article{johndrow17_bayes_shrin_at_gwas_scale,
  title = {Bayes Shrinkage at Gwas Scale: {{Convergence}} and Approximation Theory of a Scalable Mcmc Algorithm for the Horseshoe Prior},
  author = {Johndrow, James E. and Orenstein, Paulo and Bhattacharya, Anirban},
  year = {2017},
  abstract = {The horseshoe prior is frequently employed in Bayesian analysis of high-dimensional models, and has been shown to achieve minimax optimal risk properties when the truth is sparse. While optimization-based algorithms for the extremely popular Lasso and elastic net procedures can scale to dimension in the hundreds of thousands, algorithms for the horseshoe that use Markov chain Monte Carlo (MCMC) for computation are limited to problems an order of magnitude smaller. This is due to high computational cost per step and growth of the variance of time-averaging estimators as a function of dimension. We propose two new MCMC algorithms for computation in these models that have improved performance compared to existing alternatives. One of the algorithms also approximates an expensive matrix product to give orders of magnitude speedup in high-dimensional applications. We prove that the exact algorithm is geometrically ergodic, and give guarantees for the accuracy of the approximate algorithm using perturbation theory. Versions of the approximation algorithm that gradually decrease the approximation error as the chain extends are shown to be exact. The scalability of the algorithm is illustrated in simulations with problem size as large as N=5,000 observations and p=50,000 predictors, and an application to a genome-wide association study with N=2,267 and p=98,385. The empirical results also show that the new algorithm yields estimates with lower mean squared error, intervals with better coverage, and elucidates features of the posterior that were often missed by previous algorithms in high dimensions, including bimodality of posterior marginals indicating uncertainty about which covariates belong in the model.},
  archivePrefix = {arXiv},
  eprint = {1705.00841},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {stat.CO}
}

@article{kaelbling1996reinforcement,
  title = {Reinforcement Learning: {{A}} Survey},
  author = {Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  year = {1996},
  volume = {4},
  pages = {237--285},
  journal = {Journal of artificial intelligence research}
}

@article{kamalaruban19_inter_teach_algor_inver_reinf_learn,
  title = {Interactive Teaching Algorithms for Inverse Reinforcement Learning},
  author = {Kamalaruban, Parameswaran and Devidze, Rati and Cevher, Volkan and Singla, Adish},
  year = {2019},
  abstract = {We study the problem of inverse reinforcement learning (IRL) with the added twist that the learner is assisted by a helpful teacher. More formally, we tackle the following algorithmic question: How could a teacher provide an informative sequence of demonstrations to an IRL learner to speed up the learning process? We present an interactive teaching framework where a teacher adaptively chooses the next demonstration based on learner's current policy. In particular, we design teaching algorithms for two concrete settings: an omniscient setting where a teacher has full knowledge about the learner's dynamics and a blackbox setting where the teacher has minimal knowledge. Then, we study a sequential variant of the popular MCE-IRL learner and prove convergence guarantees of our teaching algorithm in the omniscient setting. Extensive experiments with a car driving simulator environment show that the learning progress can be speeded up drastically as compared to an uninformative teacher.},
  archivePrefix = {arXiv},
  eprint = {1905.11867},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@inproceedings{klaus_greff-proc-scipy-2017,
  title = {The {{Sacred Infrastructure}} for {{Computational Research}}},
  booktitle = {Proceedings of the 16th {{Python}} in {{Science Conference}}},
  author = {Greff, Klaus and Klein, Aaron and Chovanec, Martin and Hutter, Frank and {J\"urgen Schmidhuber}},
  editor = {Huff, Katy and Lippa, David and Niederhut, Dillon and {M Pacer}},
  year = {2017},
  pages = {49--56},
  doi = {10.25080/shinma-7f4c6e7-008}
}

@book{koller2009probabilistic,
  title = {Probabilistic Graphical Models: Principles and Techniques},
  author = {Koller, Daphne and Friedman, Nir and Bach, Francis},
  year = {2009},
  publisher = {{MIT press}}
}

@book{lakdawalla18_curios,
  title = {The Design and Engineering of {{Curiosity}} : How the {{Mars Rover}} Performs Its Job},
  author = {Lakdawalla, Emily},
  year = {2018},
  publisher = {{Springer}},
  address = {{Cham, Switzerland}},
  isbn = {978-3-319-68146-7}
}

@article{levine18_reinf_learn_contr_as_probab_infer,
  title = {Reinforcement Learning and Control as Probabilistic Inference: {{Tutorial}} and Review},
  author = {Levine, Sergey},
  year = {2018},
  abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  archivePrefix = {arXiv},
  eprint = {1805.00909},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{li16_simpl_scalab_accur_poster_inter_estim,
  title = {Simple, Scalable and Accurate Posterior Interval Estimation},
  author = {Li, Cheng and Srivastava, Sanvesh and Dunson, David B.},
  year = {2016},
  abstract = {There is a lack of simple and scalable algorithms for uncertainty quantification. Bayesian methods quantify uncertainty through posterior and predictive distributions, but it is difficult to rapidly estimate summaries of these distributions, such as quantiles and intervals. Variational Bayes approximations are widely used, but may badly underestimate posterior covariance. Typically, the focus of Bayesian inference is on point and interval estimates for one-dimensional functionals of interest. In small scale problems, Markov chain Monte Carlo algorithms remain the gold standard, but such algorithms face major problems in scaling up to big data. Various modifications have been proposed based on parallelization and approximations based on subsamples, but such approaches are either highly complex or lack theoretical support and/or good performance outside of narrow settings. We propose a very simple and general posterior interval estimation algorithm, which is based on running Markov chain Monte Carlo in parallel for subsets of the data and averaging quantiles estimated from each subset. We provide strong theoretical guarantees and illustrate performance in several applications.},
  archivePrefix = {arXiv},
  eprint = {1605.04029},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {stat.CO}
}

@article{li18_deep_reinf_learn,
  title = {Deep Reinforcement Learning},
  author = {Li, Yuxi},
  year = {2018},
  abstract = {We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.},
  archivePrefix = {arXiv},
  eprint = {1810.06339},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{lian18_xdeep,
  title = {Xdeepfm: {{Combining}} Explicit and Implicit Feature Interactions for Recommender Systems},
  author = {Lian, Jianxun and Zhou, Xiaohuan and Zhang, Fuzheng and Chen, Zhongxia and Xie, Xing and Sun, Guangzhong},
  year = {2018},
  abstract = {Combinatorial features are essential for the success of many commercial models. Manually crafting these features usually comes with high cost due to the variety, volume and velocity of raw data in web-scale systems. Factorization based models, which measure interactions in terms of vector product, can learn patterns of combinatorial features automatically and generalize to unseen features as well. With the great success of deep neural networks (DNNs) in various fields, recently researchers have proposed several DNN-based factorization model to learn both low- and high-order feature interactions. Despite the powerful ability of learning an arbitrary function from data, plain DNNs generate feature interactions implicitly and at the bit-wise level. In this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level. We show that the CIN share some functionalities with convolutional neural networks (CNNs) and recurrent neural networks (RNNs). We further combine a CIN and a classical DNN into one unified model, and named this new model eXtreme Deep Factorization Machine (xDeepFM). On one hand, the xDeepFM is able to learn certain bounded-degree feature interactions explicitly; on the other hand, it can learn arbitrary low- and high-order feature interactions implicitly. We conduct comprehensive experiments on three real-world datasets. Our results demonstrate that xDeepFM outperforms state-of-the-art models. We have released the source code of xDeepFM at https://github.com/Leavingseason/xDeepFM.},
  archivePrefix = {arXiv},
  eprint = {1803.05170},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{lilian_domain_random_sim2r_trans,
  title = {Domain Randomization for {{Sim2Real}} Transfer},
  author = {Weng, Lilian},
  year = {2019},
  annote = {Online; accessed 28 June 2019}
}

@article{lu17_beyon_finit_layer_neural_networ,
  title = {Beyond Finite Layer Neural Networks: {{Bridging}} Deep Architectures and Numerical Differential Equations},
  author = {Lu, Yiping and Zhong, Aoxiao and Li, Quanzheng and Dong, Bin},
  year = {2017},
  abstract = {In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multi-step method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by applying the LM-architecture on ResNet and ResNeXt respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comparable numbers of trainable parameters. In particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress ({$>$}50 \%) the original networks while maintaining a similar performance. This can be explained mathematically using the concept of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training process which helps to improve generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily apply stochastic training to the networks with the LM-architecture. As an example, we introduced stochastic depth to LM-ResNet and achieve significant improvement over the original LM-ResNet on CIFAR10.},
  archivePrefix = {arXiv},
  eprint = {1710.10121},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.CV}
}

@article{MAASS19971659,
  title = {Networks of Spiking Neurons: {{The}} Third Generation of Neural Network Models},
  author = {Maass, Wolfgang},
  year = {1997},
  volume = {10},
  pages = {1659--1671},
  issn = {0893-6080},
  doi = {https://doi.org/10.1016/S0893-6080(97)00011-7},
  abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
  journal = {Neural Networks},
  keywords = {Computational complexity,Integrate-and-fire neutron,Lower bounds,Sigmoidal neural nets,Spiking neuron},
  number = {9}
}

@article{maciver_hard_things,
  title = {How to Do Hard Things},
  author = {MacIver, David R.},
  year = {2019},
  annote = {Online; accessed 20 May 2019}
}

@article{Merolla668,
  title = {A Million Spiking-Neuron Integrated Circuit with a Scalable Communication Network and Interface},
  author = {Merolla, Paul A. and Arthur, John V. and {Alvarez-Icaza}, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
  year = {2014},
  volume = {345},
  pages = {668--673},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.1254642},
  abstract = {Computers are nowhere near as versatile as our own brains. Merolla et al. applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.Science, this issue p. 668 Inspired by the brains structure, we have developed an efficient, scalable, and flexible non\textendash{}von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
  eprint = {https://science.sciencemag.org/content/345/6197/668.full.pdf},
  journal = {Science},
  number = {6197}
}

@article{Mnih_2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and {al.}, et},
  year = {2015},
  month = feb,
  volume = {518},
  pages = {529--533},
  publisher = {{Springer Nature}},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  journal = {Nature},
  number = {7540}
}

@article{mnih16_async_method_deep_reinf_learn,
  title = {Asynchronous Methods for Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  archivePrefix = {arXiv},
  eprint = {1602.01783},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{mnih2013playing,
  title = {Playing Atari with Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  journal = {arXiv preprint arXiv:1312.5602}
}

@article{murphy2014machine,
  title = {Machine Learning: A Probabilistic Perspective. 2012},
  author = {Murphy, Kevin P},
  year = {2014},
  pages = {117},
  journal = {Cit\'e en}
}

@article{nair15_massiv_paral_method_deep_reinf_learn,
  title = {Massively Parallel Methods for Deep Reinforcement Learning},
  author = {Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and Maria, Alessandro De and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and Legg, Shane and Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David},
  year = {2015},
  abstract = {We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.},
  archivePrefix = {arXiv},
  eprint = {1507.04296},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{nakano11_spikin_neural_networ_model_free,
  title = {Spiking Neural Network Model of Free-Energy-Based Reinforcement Learning},
  author = {Nakano, Takashi and Otsuka, Makoto},
  year = {2011},
  volume = {12},
  pages = {P244},
  doi = {10.1186/1471-2202-12-s1-p244},
  date_added = {Thu Jan 16 23:13:33 2020},
  journal = {BMC Neuroscience},
  number = {S1}
}

@article{nateliason_how_take_smart_notes,
  title = {How to Take Smart Notes: {{A}} Step-by-Step Guide - Nat Eliason},
  author = {Eliason, Nat},
  year = {2020},
  annote = {Online; accessed 14 February 2020}
}

@article{neftci19_surrog_gradien_learn_spikin_neural_networ,
  title = {Surrogate Gradient Learning in Spiking Neural Networks},
  author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  year = {2019},
  abstract = {Spiking neural networks are nature's versatile solution to fault-tolerant and energy efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking neural network processors attempt to emulate biological neural networks. These developments have created an imminent need for methods and tools to enable such systems to solve real-world signal processing problems. Like conventional neural networks, spiking neural networks can be trained on real, domain specific data. However, their training requires overcoming a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training spiking neural networks, and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. To that end, it gives an overview of existing approaches and provides an introduction to surrogate gradient methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
  archivePrefix = {arXiv},
  eprint = {1901.09948},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.NE}
}

@article{newcombe15_how_amazon_web_servic_uses_formal_method,
  title = {How Amazon Web Services Uses Formal Methods},
  author = {Newcombe, Chris and Rath, Tim and Zhang, Fan and Munteanu, Bogdan and Brooker, Marc and Deardeuff, Michael},
  year = {2015},
  volume = {58},
  pages = {66--73},
  doi = {10.1145/2699417},
  date_added = {Thu Jan 16 14:52:06 2020},
  journal = {Communications of the ACM},
  number = {4}
}

@article{nilil_instal_ubunt_ros_wiki,
  title = {Melodic/{{Installation}}/{{Ubuntu}} - {{ROS}} Wiki},
  author = {{nil}},
  year = {nil},
  annote = {Online; accessed 16 October 2019}
}

@article{nilil_ros_introd_ros_wiki,
  title = {{{ROS}}/{{Introduction}} - {{ROS}} Wiki},
  author = {{nil}},
  year = {nil},
  annote = {Online; accessed 15 October 2019}
}

@incollection{NIPS2016_6383,
  title = {Unifying Count-Based Exploration and Intrinsic Motivation},
  booktitle = {Advances in Neural Information Processing Systems 29},
  author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {1471--1479},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{NIPS2018_7415,
  title = {{{SLAYER}}: {{Spike}} Layer Error Reassignment in Time},
  booktitle = {Advances in Neural Information Processing Systems 31},
  author = {Shrestha, Sumit Bam and Orchard, Garrick},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {1412--1421},
  publisher = {{Curran Associates, Inc.}}
}

@incollection{NIPS2018_7417,
  title = {Gradient Descent for Spiking Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 31},
  author = {Huh, Dongsung and Sejnowski, Terrence J},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {1433--1443},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/jethro/Zotero/storage/NPSS2Y37/Huh and Sejnowski - 2018 - Gradient descent for spiking neural networks.pdf}
}

@article{openai_gym,
  title = {{{OpenAI}} Gym},
  author = {{OpenAI}},
  year = {2019},
  annote = {Online; accessed 02 November 2019}
}

@article{paine19_makin_effic_use_demon_to,
  title = {Making Efficient Use of Demonstrations to Solve Hard Exploration Problems},
  author = {Paine, Tom Le and Gulcehre, Caglar and Shahriari, Bobak and Denil, Misha and Hoffman, Matt and Soyer, Hubert and Tanburn, Richard and Kapturowski, Steven and Rabinowitz, Neil and Williams, Duncan and {Barth-Maron}, Gabriel and Wang, Ziyu and de Freitas, Nando and Team, Worlds},
  year = {2019},
  abstract = {This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.},
  archivePrefix = {arXiv},
  eprint = {1909.01387},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{papamakarios19_normal_flows_probab_model_infer,
  title = {Normalizing Flows for Probabilistic Modeling and Inference},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  year = {2019},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1912.02762},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {stat.ML}
}

@article{pati17_statis_optim_variat_bayes,
  title = {On Statistical Optimality of Variational Bayes},
  author = {Pati, Debdeep and Bhattacharya, Anirban and Yang, Yun},
  year = {2017},
  abstract = {The article addresses a long-standing open problem on the justification of using variational Bayes methods for parameter estimation. We provide general conditions for obtaining optimal risk bounds for point estimates acquired from mean-field variational Bayesian inference. The conditions pertain to the existence of certain test functions for the distance metric on the parameter space and minimal assumptions on the prior. A general recipe for verification of the conditions is outlined which is broadly applicable to existing Bayesian models with or without latent variables. As illustrations, specific applications to Latent Dirichlet Allocation and Gaussian mixture models are discussed.},
  archivePrefix = {arXiv},
  eprint = {1712.08983},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {math.ST}
}

@article{pfeiffer2018deep,
  title = {Deep Learning with Spiking Neurons: Opportunities and Challenges},
  author = {Pfeiffer, Michael and Pfeil, Thomas},
  year = {2018},
  volume = {12},
  publisher = {{Frontiers Media SA}},
  journal = {Frontiers in neuroscience}
}

@book{pinsky2010introduction,
  title = {An Introduction to Stochastic Modeling},
  author = {Pinsky, Mark and Karlin, Samuel},
  year = {2010},
  publisher = {{Academic press}}
}

@inproceedings{pmlr-v38-srivastava15,
  title = {{{WASP}}: {{Scalable Bayes}} via Barycenters of Subset Posteriors},
  booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  author = {Srivastava, Sanvesh and Cevher, Volkan and Dinh, Quoc and Dunson, David},
  editor = {Lebanon, Guy and Vishwanathan, S. V. N.},
  year = {2015},
  month = may,
  volume = {38},
  pages = {912--920},
  publisher = {{PMLR}},
  address = {{San Diego, California, USA}},
  abstract = {The promise of Bayesian methods for big data sets has not fully been realized due to the lack of scalable computational algorithms. For massive data, it is necessary to store and process subsets on different machines in a distributed manner. We propose a simple, general, and highly efficient approach, which first runs a posterior sampling algorithm in parallel on different machines for subsets of a large data set. To combine these subset posteriors, we calculate the Wasserstein barycenter via a highly efficient linear program. The resulting estimate for the Wasserstein posterior (WASP) has an atomic form, facilitating straightforward estimation of posterior summaries of functionals of interest. The WASP approach allows posterior sampling algorithms for smaller data sets to be trivially scaled to huge data. We provide theoretical justification in terms of posterior consistency and algorithm efficiency. Examples are provided in complex settings including Gaussian process regression and nonparametric Bayes mixture models.},
  pdf = {http://proceedings.mlr.press/v38/srivastava15.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v70-haarnoja17a,
  title = {Reinforcement Learning with Deep Energy-Based Policies},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  editor = {Precup, Doina and Teh, Yee Whye},
  year = {2017},
  month = aug,
  volume = {70},
  pages = {1352--1361},
  publisher = {{PMLR}},
  address = {{International Convention Centre, Sydney, Australia}},
  abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
  pdf = {http://proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v89-o-connor19a,
  title = {Training a Spiking Neural Network with Equilibrium Propagation},
  booktitle = {Proceedings of Machine Learning Research},
  author = {O'Connor, Peter and Gavves, Efstratios and Welling, Max},
  editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  year = {2019},
  month = apr,
  volume = {89},
  pages = {1516--1523},
  publisher = {{PMLR}},
  abstract = {Backpropagation is almost universally used to train artificial neural networks. However, there are several reasons that backpropagation could not be plausibly implemented by biological neurons. Among these are the facts that (1) biological neurons appear to lack any mechanism for sending gradients backwards across synapses, and (2) biological ``spiking'' neurons emit binary signals, whereas back-propagation requires that neurons communicate continuous values between one another. Recently, Scellier and Bengio [2017], demonstrated an alternative to backpropagation, called Equilibrium Propagation, wherein gradients are implicitly computed by the dynamics of the neural network, so that neurons do not need an internal mechanism for backpropagation of gradients. This provides an interesting solution to problem (1). In this paper, we address problem (2) by proposing a way in which Equilibrium Propagation can be implemented with neurons which are constrained to just communicate binary values at each time step. We show that with appropriate step-size annealing, we can converge to the same fixed-point as a real-valued neural network, and that with predictive coding, we can make this convergence much faster. We demonstrate that the resulting model can be used to train a spiking neural network using the update scheme from Equilibrium propagation.},
  pdf = {http://proceedings.mlr.press/v89/o-connor19a/o-connor19a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{pong19_skew_fit,
  title = {Skew-Fit: {{State}}-Covering Self-Supervised Reinforcement Learning},
  author = {Pong, Vitchyr H. and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
  year = {2019},
  abstract = {In standard reinforcement learning, each new skill requires a manually-designed reward function, which takes considerable manual effort and engineering. Self-supervised goal setting has the potential to automate this process, enabling an agent to propose its own goals and acquire skills that achieve these goals. However, such methods typically rely on manually-designed goal distributions, or heuristics to force the agent to explore a wide range of states. We propose a formal exploration objective for goal-reaching policies that maximizes state coverage. We show that this objective is equivalent to maximizing the entropy of the goal distribution together with goal reaching performance, where goals correspond to entire states. We present an algorithm called Skew-Fit for learning such a maximum-entropy goal distribution, and show that under certain regularity conditions, our method converges to a uniform distribution over the set of possible states, even when we do not know this set beforehand. Skew-Fit enables self-supervised agents to autonomously choose and practice diverse goals. Our experiments show that it can learn a variety of manipulation tasks from images, including opening a door with a real robot, entirely from scratch and without any manually-designed reward function.},
  archivePrefix = {arXiv},
  eprint = {1903.03698},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{poole19_variat_bound_mutual_infor,
  title = {On Variational Bounds of Mutual Information},
  author = {Poole, Ben and Ozair, Sherjil and van den Oord, Aaron and Alemi, Alexander A. and Tucker, George},
  year = {2019},
  abstract = {Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning; however, bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning.},
  archivePrefix = {arXiv},
  eprint = {1905.06922v1},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{queiroz06_reinf_learn_simpl_contr_task,
  title = {Reinforcement Learning of a Simple Control Task Using the Spike Response Model},
  author = {{de Queiroz}, Murilo Saraiva and {de Berr{\^e}do}, Roberto Coelho and {de P{\'a}dua Braga}, Ant{\^o}nio},
  year = {2006},
  volume = {70},
  pages = {14--20},
  doi = {10.1016/j.neucom.2006.07.002},
  date_added = {Thu Aug 29 12:45:08 2019},
  journal = {Neurocomputing},
  number = {1-3}
}

@article{rackauckas19_diffeq,
  title = {Diffeqflux.Jl - a Julia Library for Neural Differential Equations},
  author = {Rackauckas, Chris and Innes, Mike and Ma, Yingbo and Bettencourt, Jesse and White, Lyndon and Dixit, Vaibhav},
  year = {2019},
  abstract = {DiffEqFlux.jl is a library for fusing neural networks and differential equations. In this work we describe differential equations from the viewpoint of data science and discuss the complementary nature between machine learning models and differential equations. We demonstrate the ability to incorporate DifferentialEquations.jl-defined differential equation problems into a Flux-defined neural network, and vice versa. The advantages of being able to use the entire DifferentialEquations.jl suite for this purpose is demonstrated by counter examples where simple integration strategies fail, but the sophisticated integration strategies provided by the DifferentialEquations.jl library succeed. This is followed by a demonstration of delay differential equations and stochastic differential equations inside of neural networks. We show high-level functionality for defining neural ordinary differential equations (neural networks embedded into the differential equation) and describe the extra models in the Flux model zoo which includes neural stochastic differential equations. We conclude by discussing the various adjoint methods used for backpropogation of the differential equation solvers. DiffEqFlux.jl is an important contribution to the area, as it allows the full weight of the differential equation solvers developed from decades of research in the scientific computing field to be readily applied to the challenges posed by machine learning and data science.},
  archivePrefix = {arXiv},
  eprint = {1902.02376},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{Rafferty_2015,
  title = {Faster Teaching via {{POMDP}} Planning},
  author = {Rafferty, Anna N. and Brunskill, Emma and Griffiths, Thomas L. and Shafto, Patrick},
  year = {2015},
  month = sep,
  volume = {40},
  pages = {1290--1332},
  publisher = {{Wiley}},
  issn = {0364-0213},
  doi = {10.1111/cogs.12290},
  journal = {Cognitive Science},
  number = {6}
}

@article{ranganath13_black_box_variat_infer,
  title = {Black Box Variational Inference},
  author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
  year = {2013},
  abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
  archivePrefix = {arXiv},
  eprint = {1401.0118},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {stat.ML}
}

@inproceedings{ratliff2006maximum,
  title = {Maximum Margin Planning},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning},
  author = {Ratliff, Nathan D and Bagnell, J Andrew and Zinkevich, Martin A},
  year = {2006},
  pages = {729--736},
  organization = {{ACM}}
}

@article{ravi_bayesian_teaching_mnist,
  title = {Bayesian Teaching as Model Explanation: {{An MNIST}} Example},
  author = {Sojitra, Ravi},
  year = {2018},
  annote = {Online; accessed 19 May 2019}
}

@article{richardson06_markov_logic_networ,
  title = {Markov Logic Networks},
  author = {Richardson, Matthew and Domingos, Pedro},
  year = {2006},
  volume = {62},
  pages = {107--136},
  doi = {10.1007/s10994-006-5833-1},
  date_added = {Wed Jul 24 22:57:34 2019},
  journal = {Machine Learning},
  number = {1-2}
}

@book{ross2014introduction,
  title = {Introduction to Probability Models},
  author = {Ross, Sheldon M},
  year = {2014},
  publisher = {{Academic press}}
}

@article{rueckauer16_theor_tools_conver_analog_to,
  title = {Theory and Tools for the Conversion of Analog to Spiking Convolutional Neural Networks},
  author = {Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael},
  year = {2016},
  abstract = {Deep convolutional neural networks (CNNs) have shown great potential for numerous real-world machine learning applications, but performing inference in large CNNs in real-time remains a challenge. We have previously demonstrated that traditional CNNs can be converted into deep spiking neural networks (SNNs), which exhibit similar accuracy while reducing both latency and computational load as a consequence of their data-driven, event-based style of computing. Here we provide a novel theory that explains why this conversion is successful, and derive from it several new tools to convert a larger and more powerful class of deep networks into SNNs. We identify the main sources of approximation errors in previous conversion methods, and propose simple mechanisms to fix these issues. Furthermore, we develop spiking implementations of common CNN operations such as max-pooling, softmax, and batch-normalization, which allow almost loss-less conversion of arbitrary CNN architectures into the spiking domain. Empirical evaluation of different network architectures on the MNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.},
  archivePrefix = {arXiv},
  eprint = {1612.04052},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {stat.ML}
}

@article{ruiz19_contr_diver_combin_variat_infer_mcmc,
  title = {A Contrastive Divergence for Combining Variational Inference and Mcmc},
  author = {Ruiz, Francisco J. R. and Titsias, Michalis K.},
  year = {2019},
  abstract = {We develop a method to combine Markov chain Monte Carlo (MCMC) and variational inference (VI), leveraging the advantages of both inference approaches. Specifically, we improve the variational distribution by running a few MCMC steps. To make inference tractable, we introduce the variational contrastive divergence (VCD), a new divergence that replaces the standard Kullback-Leibler (KL) divergence used in VI. The VCD captures a notion of discrepancy between the initial variational distribution and its improved version (obtained after running the MCMC steps), and it converges asymptotically to the symmetrized KL divergence between the variational distribution and the posterior of interest. The VCD objective can be optimized efficiently with respect to the variational parameters via stochastic optimization. We show experimentally that optimizing the VCD leads to better predictive performance on two latent variable models: logistic matrix factorization and variational autoencoders (VAEs).},
  archivePrefix = {arXiv},
  eprint = {1905.04062},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {stat.ML}
}

@article{rusu15_polic_distil,
  title = {Policy Distillation},
  author = {Rusu, Andrei A. and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  year = {2015},
  abstract = {Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.},
  archivePrefix = {arXiv},
  eprint = {1511.06295},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{sallans04a_ferl,
  title = {Reinforcement Learning with Factored States and Actions.},
  author = {Sallans, Brian and Hinton, Geoffrey},
  year = {2004},
  month = aug,
  volume = {5},
  pages = {1063--1088},
  journal = {Journal of Machine Learning Research}
}

@article{sboev18_spikin_neural_networ_reinf_learn,
  title = {Spiking Neural Network Reinforcement Learning Method Based on Temporal Coding and Stdp},
  author = {Sboev, Alexander and Vlasov, Danila and Rybka, Roman and Serenko, Alexey},
  year = {2018},
  volume = {145},
  pages = {458--463},
  doi = {10.1016/j.procs.2018.11.107},
  date_added = {Mon Sep 30 11:08:34 2019},
  journal = {Procedia Computer Science},
  number = {nil}
}

@article{schaul15_prior_exper_replay,
  title = {Prioritized Experience Replay},
  author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  year = {2015},
  abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  archivePrefix = {arXiv},
  eprint = {1511.05952},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{schliebs13_evolv_spikin_neural_networ_survey,
  title = {Evolving Spiking Neural Network-a Survey},
  author = {Schliebs, Stefan and Kasabov, Nikola},
  year = {2013},
  volume = {4},
  pages = {87--98},
  doi = {10.1007/s12530-013-9074-9},
  date_added = {Mon Jan 6 18:15:22 2020},
  journal = {Evolving Systems},
  number = {2}
}

@article{schulman15_high_dimen_contin_contr_using,
  title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  year = {2015},
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  archivePrefix = {arXiv},
  eprint = {1506.02438},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{schultz97_neural_subst_predic_rewar,
  title = {A Neural Substrate of Prediction and Reward},
  author = {Schultz, W. and Dayan, P. and Montague, P. R.},
  year = {1997},
  volume = {275},
  pages = {1593--1599},
  doi = {10.1126/science.275.5306.1593},
  date_added = {Sun Dec 15 13:23:11 2019},
  journal = {Science},
  number = {5306}
}

@article{Severa2016SpikingNA,
  title = {Spiking Network Algorithms for Scientific Computing},
  author = {Severa, William and Parekh, Ojas and Carlson, Kristofor D. and James, Conrad D. and Aimone, James B.},
  year = {2016},
  pages = {1--8},
  journal = {2016 IEEE International Conference on Rebooting Computing (ICRC)}
}

@book{Shalev-Shwartz:2014:UML:2621980,
  title = {Understanding Machine Learning: {{From}} Theory to Algorithms},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  publisher = {{Cambridge University Press}},
  address = {{New York, NY, USA}},
  isbn = {1-107-05713-2 978-1-107-05713-5}
}

@book{shalev2014understanding,
  title = {Understanding Machine Learning: {{From}} Theory to Algorithms},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  publisher = {{Cambridge university press}}
}

@article{SHRESTHA201733,
  title = {Robust Spike-Train Learning in Spike-Event Based Weight Update},
  author = {Shrestha, Sumit Bam and Song, Qing},
  year = {2017},
  volume = {96},
  pages = {33--46},
  issn = {0893-6080},
  doi = {https://doi.org/10.1016/j.neunet.2017.08.010},
  abstract = {Supervised learning algorithms in a spiking neural network either learn a spike-train pattern for a single neuron receiving input spike-train from multiple input synapses or learn to output the first spike time in a feedforward network setting. In this paper, we build upon spike-event based weight update strategy to learn continuous spike-train in a spiking neural network with a hidden layer using a dead zone on\textendash{}off based adaptive learning rate rule which ensures convergence of the learning process in the sense of weight convergence and robustness of the learning process to external disturbances. Based on different benchmark problems, we compare this new method with other relevant spike-train learning algorithms. The results show that the speed of learning is much improved and the rate of successful learning is also greatly improved.},
  journal = {Neural Networks},
  keywords = {Adaptive learning rate,Multilayer spike-train learning,Robust stability,Spiking neural network,Supervised learning,Weight convergence}
}

@article{shwartz-ziv17_openin_black_box_deep_neural,
  title = {Opening the Black Box of Deep Neural Networks via Information},
  author = {{Shwartz-Ziv}, Ravid and Tishby, Naftali},
  year = {2017},
  abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the \emph{Information Plane}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on \emph{c}ompression of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
  archivePrefix = {arXiv},
  eprint = {1703.00810},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{simard17_machin_teach,
  title = {Machine Teaching: A New Paradigm for Building Machine Learning Systems},
  author = {Simard, Patrice Y. and Amershi, Saleema and Chickering, David M. and Pelton, Alicia Edelman and Ghorashi, Soroush and Meek, Christopher and Ramos, Gonzalo and Suh, Jina and Verwey, Johan and Wang, Mo and Wernsing, John},
  year = {2017},
  abstract = {The current processes for building machine learning systems require practitioners with deep knowledge of machine learning. This significantly limits the number of machine learning systems that can be created and has led to a mismatch between the demand for machine learning systems and the ability for organizations to build them. We believe that in order to meet this growing demand for machine learning systems we must significantly increase the number of individuals that can teach machines. We postulate that we can achieve this goal by making the process of teaching machines easy, fast and above all, universally accessible. While machine learning focuses on creating new algorithms and improving the accuracy of "learners", the machine teaching discipline focuses on the efficacy of the "teachers". Machine teaching as a discipline is a paradigm shift that follows and extends principles of software engineering and programming languages. We put a strong emphasis on the teacher and the teacher's interaction with data, as well as crucial components such as techniques and design principles of interaction and visualization. In this paper, we present our position regarding the discipline of machine teaching and articulate fundamental machine teaching principles. We also describe how, by decoupling knowledge about machine learning algorithms from the process of teaching, we can accelerate innovation and empower millions of new uses for machine learning models.},
  archivePrefix = {arXiv},
  eprint = {1707.06742},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@inproceedings{smith_quoc_bayes_generalization_sgd,
  title = {A Bayesian Perspective on Generalization and Stochastic Gradient Descent},
  author = {Smith, Sam and Le, Quoc V.},
  year = {2018}
}

@article{SnnSlam,
  title = {Spiking Neural Network on Neuromorphic Hardware for Energy-Efficient Unidimensional Slam},
  author = {Tang, Guangzhi and Shah, Arpit and Michmizos, Konstantinos P.},
  year = {2019},
  abstract = {Energy-efficient simultaneous localization and mapping (SLAM) is crucial for mobile robots exploring unknown environments. The mammalian brain solves SLAM via a network of specialized neurons, exhibiting asynchronous computations and event-based communications, with very low energy consumption. We propose a brain-inspired spiking neural network (SNN) architecture that solves the unidimensional SLAM by introducing spike-based reference frame transformation, visual likelihood computation, and Bayesian inference. We integrated our neuromorphic algorithm to Intel's Loihi neuromorphic processor, a non-Von Neumann hardware that mimics the brain's computing paradigms. We performed comparative analyses for accuracy and energy-efficiency between our neuromorphic approach and the GMapping algorithm, which is widely used in small environments. Our Loihi-based SNN architecture consumes 100 times less energy than GMapping run on a CPU while having comparable accuracy in head direction localization and map-generation. These results pave the way for scaling our approach towards active-SLAM alternative solutions for Loihi-controlled autonomous robots.},
  archivePrefix = {arXiv},
  eprint = {1903.02504},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.RO}
}

@article{so_why_move_shared_ptr,
  title = {C++ - {{Why}} Would {{I}} Std::Move an Std::Shared\textsubscript{p}tr? - {{Stack Overflow}}},
  author = {{nil}},
  year = {2019},
  annote = {Online; accessed 25 February 2019}
}

@inproceedings{spikeprop,
  title = {{{SpikeProp}}: Backpropagation for Networks of Spiking Neurons.},
  author = {Bohte, Sander and Kok, Joost and Poutr{\'e}, Johannes},
  year = {2000},
  month = jan,
  volume = {48},
  pages = {419--424},
  series = {{{ESANN}}}
}

@article{stemmler96_singl_spike_suffic,
  title = {A Single Spike Suffices: The Simplest Form of Stochastic Resonance in Model Neurons},
  author = {Stemmler, Martin},
  year = {1996},
  volume = {7},
  pages = {687--716},
  doi = {10.1088/0954-898x_7_4_005},
  date_added = {Sat Nov 2 19:32:20 2019},
  journal = {Network: Computation in Neural Systems},
  number = {4}
}

@article{stock19_and_bit_goes_down,
  title = {And the Bit Goes down: {{Revisiting}} the Quantization of Neural Networks},
  author = {Stock, Pierre and Joulin, Armand and Gribonval, R{\'e}mi and Graham, Benjamin and J{\'e}gou, Herv{\'e}},
  year = {2019},
  abstract = {In this paper, we address the problem of reducing the memory footprint of ResNet-like convolutional network architectures. We introduce a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs and not its weights. The advantage of our approach is that it minimizes the loss reconstruction error for in-domain inputs and does not require any labelled data. We also use byte-aligned codebooks to produce compressed networks with efficient inference on CPU. We validate our approach by quantizing a high performing ResNet-50 model to a memory size of 5 MB (20x compression factor) while preserving a top-1 accuracy of 76.1 \% on ImageNet object classification and by compressing a Mask R-CNN with a size budget around 6 MB.},
  archivePrefix = {arXiv},
  eprint = {1907.05686},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.CV}
}

@inproceedings{stolle2002learning,
  title = {Learning Options in Reinforcement Learning},
  booktitle = {International {{Symposium}} on Abstraction, Reformulation, and Approximation},
  author = {Stolle, Martin and Precup, Doina},
  year = {2002},
  pages = {212--223},
  organization = {{Springer}}
}

@inproceedings{sutton2000policy,
  title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  year = {2000},
  pages = {1057--1063}
}

@inproceedings{systemsengineering_6575245,
  title = {Systems Engineering the Curiosity Rover: {{A}} Retrospective},
  booktitle = {2013 8th International Conference on System of Systems Engineering},
  author = {Welch, R. and Limonadi, D. and Manning, R.},
  year = {2013},
  month = jun,
  pages = {70--75},
  doi = {10.1109/SYSoSE.2013.6575245},
  keywords = {Actuators,Aerospace electronics,Complexity theory,Computer architecture,cruise system,descent and landing (spacecraft),Earth,entry,entry descent and landing system,Instruments,Mars,Mars Science Laboratory Curiosity Rover,Mars surface,MSL,planetary rovers,planetary surfaces,Robotics,rover complex science payload,Rovers,scientific exploration,Software,spacecraft,systems engineering,Systems Engineering,systems sampling system}
}

@article{TAVANAEI201947,
  title = {Deep Learning in Spiking Neural Networks},
  author = {Tavanaei, Amirhossein and Ghodrati, Masoud and Kheradpisheh, Saeed Reza and Masquelier, Timoth{\'e}e and Maida, Anthony},
  year = {2019},
  volume = {111},
  pages = {47--63},
  issn = {0893-6080},
  doi = {https://doi.org/10.1016/j.neunet.2018.12.002},
  abstract = {In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.},
  journal = {Neural Networks},
  keywords = {Biological plausibility,Deep learning,Machine learning,Power-efficient architecture,Spiking neural network}
}

@article{tensorflow_stand_keras,
  title = {Standardizing on Keras: {{Guidance}} on High-Level {{APIs}} in {{TensorFlow}} 2.0},
  author = {{Tensorflow}},
  year = {2018},
  annote = {Online; accessed 07 January 2019}
}

@book{thrun2005probabilistic,
  title = {Probabilistic Robotics},
  author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  year = {2005},
  publisher = {{MIT press}}
}

@article{titsias18_unbias_implic_variat_infer,
  title = {Unbiased Implicit Variational Inference},
  author = {Titsias, Michalis K. and Ruiz, Francisco J. R.},
  year = {2018},
  abstract = {We develop unbiased implicit variational inference (UIVI), a method that expands the applicability of variational inference by defining an expressive variational family. UIVI considers an implicit variational distribution obtained in a hierarchical manner using a simple reparameterizable distribution whose variational parameters are defined by arbitrarily flexible deep neural networks. Unlike previous works, UIVI directly optimizes the evidence lower bound (ELBO) rather than an approximation to the ELBO. We demonstrate UIVI on several models, including Bayesian multinomial logistic regression and variational autoencoders, and show that UIVI achieves both tighter ELBO and better predictive performance than existing approaches at a similar computational cost.},
  archivePrefix = {arXiv},
  eprint = {1808.02078},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {stat.ML}
}

@article{training_deep_snn_bpp_lee,
  title = {Training Deep Spiking Neural Networks Using Backpropagation},
  author = {Lee, Jun and Delbruck, Tobi and Pfeiffer, Michael},
  year = {2016},
  month = aug,
  volume = {10},
  doi = {10.3389/fnins.2016.00508},
  journal = {Frontiers in Neuroscience}
}

@article{trimstray_se_hackers,
  title = {Trimstray on {{Twitter}}: \&quot;{{Search Engines}} for {{Hackers}}:\&\#10;\&\#10;{{https://t.co/Awr3X88Xu1\&\#10;https://t.co/03trsWUrnP\&\#10;https://t.co/B9IHX23MeC\&\#10;https://t.co/uO1oFjB7Eb\&\#10;https://t.co/NE7FSOQsPl\&\#10;https://t.co/s2wG7cOGa5\&\#10;https://t.co/uBqtz7QuUD\&\#10;https://t.co/IZx4B82wLQ\&\#10;https://t.co/Oa04GvDxTp\&\#10;https://t.co/TKjuUVU9il\&\#10;\&\#10;\#it}} \#tech\&quot;},
  author = {{trimstray}},
  year = {2019},
  annote = {Online; accessed 09 February 2019}
}

@article{tschiatschek19_learn_aware_teach,
  title = {Learner-Aware Teaching: {{Inverse}} Reinforcement Learning with Preferences and Constraints},
  author = {Tschiatschek, Sebastian and Ghosh, Ahana and Haug, Luis and Devidze, Rati and Singla, Adish},
  year = {2019},
  abstract = {Inverse reinforcement learning (IRL) enables an agent to learn complex behavior by observing demonstrations from a (near-)optimal policy. The typical assumption is that the learner's goal is to match the teacher's demonstrated behavior. In this paper, we consider the setting where the learner has her own preferences that she additionally takes into consideration. These preferences can for example capture behavioral biases, mismatched worldviews, or physical constraints. We study two teaching approaches: learner-agnostic teaching, where the teacher provides demonstrations from an optimal policy ignoring the learner's preferences, and learner-aware teaching, where the teacher accounts for the learner's preferences. We design learner-aware teaching algorithms and show that significant performance improvements can be achieved over learner-agnostic teaching.},
  archivePrefix = {arXiv},
  eprint = {1906.00429},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@article{ucla_causal_discus,
  title = {{{CAUSALITY}} - Discussion},
  author = {{UCLSA}},
  year = {nil},
  annote = {Online; accessed 11 February 2019}
}

@article{urbanczik09_gradien_learn_rule_tempot,
  title = {A Gradient Learning Rule for the Tempotron},
  author = {Urbanczik, Robert and Senn, Walter},
  year = {2009},
  volume = {21},
  pages = {340--352},
  doi = {10.1162/neco.2008.09-07-605},
  date_added = {Fri Nov 1 16:00:33 2019},
  journal = {Neural Computation},
  number = {2}
}

@inproceedings{van2016deep,
  title = {Deep Reinforcement Learning with Double Q-Learning},
  booktitle = {Thirtieth {{AAAI}} Conference on Artificial Intelligence},
  author = {Van Hasselt, Hado and Guez, Arthur and Silver, David},
  year = {2016}
}

@article{victor2005spike,
  title = {Spike Train Metrics},
  author = {Victor, Jonathan D},
  year = {2005},
  volume = {15},
  pages = {585--592},
  publisher = {{Elsevier}},
  journal = {Current opinion in neurobiology},
  number = {5}
}

@article{VITANZA20153122,
  title = {Spiking Neural Controllers in Multi-Agent Competitive Systems for Adaptive Targeted Motor Learning},
  author = {Vitanza, Alessandra and Patan{\'e}, Luca and Arena, Paolo},
  year = {2015},
  volume = {352},
  pages = {3122--3143},
  issn = {0016-0032},
  doi = {https://doi.org/10.1016/j.jfranklin.2015.04.014},
  abstract = {The proposed work introduces a neural control strategy for guiding adaptation in spiking neural structures acting as nonlinear controllers in a group of bio-inspired robots which compete in reaching targets in a virtual environment. The neural structures embedded into each agent are inspired by a specific part of the insect brain, namely Central Complex, devoted to detect, learn and memorize visual features for targeted motor control. A reduced-order model of a spiking neuron is used as the basic building block for the neural controller. The control methodology employs bio-inspired, correlation based learning mechanisms like Spike timing dependent plasticity with the addition of a reward/punishment-based method experimentally found in insects. The reference signal for the overall multi-agent control system is imposed by a global reward, which guides motor learning to direct each agent towards specific visual targets. The neural controllers within the agents start from identical conditions: the learning strategy induces each robot to show anticipated targeting actions upon specific visual stimuli. The whole control structure also contributes to make the robots refractory or more sensitive to specific visual stimuli, showing distinct preferences in future choices. This leads to an environmentally induced, targeted motor control, even without a direct communication among the agents, giving robots, while running, the ability to perform adaptation in real-time. Experiments, carried out in a dynamic simulation environment, show the suitability of the proposed approach. Specific performance indexes, like Shannon׳s Entropy, are adopted to quantitatively analyze diversity and specialization within the group.},
  annote = {Special Issue on Advances in Nonlinear Dynamics and Control},
  journal = {Journal of the Franklin Institute},
  number = {8}
}

@article{wang18_dkn,
  title = {Dkn: {{Deep}} Knowledge-Aware Network for News Recommendation},
  author = {Wang, Hongwei and Zhang, Fuzheng and Xie, Xing and Guo, Minyi},
  year = {2018},
  abstract = {Online news recommender systems aim to address the information explosion of news and make personalized recommendation for users. In general, news language is highly condensed, full of knowledge entities and common sense. However, existing methods are unaware of such external knowledge and cannot fully discover latent knowledge-level connections among news. The recommended results for a user are consequently limited to simple patterns and cannot be extended reasonably. Moreover, news recommendation also faces the challenges of high time-sensitivity of news and dynamic diversity of users' interests. To solve the above problems, in this paper, we propose a deep knowledge-aware network (DKN) that incorporates knowledge graph representation into news recommendation. DKN is a content-based deep recommendation framework for click-through rate prediction. The key component of DKN is a multi-channel and word-entity-aligned knowledge-aware convolutional neural network (KCNN) that fuses semantic-level and knowledge-level representations of news. KCNN treats words and entities as multiple channels, and explicitly keeps their alignment relationship during convolution. In addition, to address users' diverse interests, we also design an attention module in DKN to dynamically aggregate a user's history with respect to current candidate news. Through extensive experiments on a real online news platform, we demonstrate that DKN achieves substantial gains over state-of-the-art deep recommendation models. We also validate the efficacy of the usage of knowledge in DKN.},
  archivePrefix = {arXiv},
  eprint = {1801.08284},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {stat.ML}
}

@book{White:2009:HDG:1717298,
  title = {Hadoop: {{The}} Definitive Guide},
  author = {White, Tom},
  year = {2009},
  edition = {1st},
  publisher = {{O'Reilly Media, Inc.}},
  isbn = {0-596-52197-9 978-0-596-52197-4}
}

@inproceedings{Whiteside2017HowTW,
  title = {How to Write a Technical Paper},
  author = {Whiteside, James D.},
  year = {2017}
}

@article{whittington19_theor_error_back_propag_brain,
  title = {Theories of Error Back-Propagation in the Brain},
  author = {Whittington, James C.R. and Bogacz, Rafal},
  year = {2019},
  volume = {23},
  pages = {235--250},
  doi = {10.1016/j.tics.2018.12.005},
  date_added = {Tue Aug 20 10:09:27 2019},
  journal = {Trends in Cognitive Sciences},
  number = {3}
}

@article{wiki_pubsub,
  title = {{{ROS}}/{{Tutorials}}/{{WritingPublisherSubscriber}}(Python) - {{ROS}} Wiki},
  author = {{nil}},
  year = {nil},
  annote = {Online; accessed 17 October 2019}
}

@article{wiki_service,
  title = {{{ROS}}/{{Tutorials}}/{{WritingServiceClient}}(Python) - {{ROS}} Wiki},
  author = {{nil}},
  year = {nil},
  annote = {Online; accessed 17 October 2019}
}

@article{wilson2019bayesian,
  title = {The Case for {{Bayesian}} Deep Learning},
  author = {Wilson, Andrew Gordon},
  year = {2019},
  annote = {Accessible at https://cims.nyu.edu/\textasciitilde{}andrewgw/caseforbdl.pdf},
  journal = {NYU Courant Technical Report}
}

@article{woodward19_learn_to_inter_learn_assis,
  title = {Learning to Interactively Learn and Assist},
  author = {Woodward, Mark and Finn, Chelsea and Hausman, Karol},
  year = {2019},
  abstract = {When deploying autonomous agents in the real world, we need effective ways of communicating objectives to them. Traditional skill learning has revolved around reinforcement and imitation learning, each with rigid constraints on the format of information exchanged between the human and the agent. While scalar rewards carry little information, demonstrations require significant effort to provide and may carry more information than is necessary. Furthermore, rewards and demonstrations are often defined and collected before training begins, when the human is most uncertain about what information would help the agent. In contrast, when humans communicate objectives with each other, they make use of a large vocabulary of informative behaviors, including non-verbal communication, and often communicate throughout learning, responding to observed behavior. In this way, humans communicate intent with minimal effort. In this paper, we propose such interactive learning as an alternative to reward or demonstration-driven learning. To accomplish this, we introduce a multi-agent training framework that enables an agent to learn from another agent who knows the current task. Through a series of experiments, we demonstrate the emergence of a variety of interactive learning behaviors, including information-sharing, information-seeking, and question-answering. Most importantly, we find that our approach produces an agent that is capable of learning interactively from a human user, without a set of explicit demonstrations or a reward function, and achieving significantly better performance cooperatively with a human than a human performing the task alone.},
  archivePrefix = {arXiv},
  eprint = {1906.10187},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.AI}
}

@article{you17_large_batch_train_convol_networ,
  title = {Large Batch Training of Convolutional Networks},
  author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
  year = {2017},
  abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
  archivePrefix = {arXiv},
  eprint = {1708.03888},
  eprinttype = {arxiv},
  journal = {CoRR},
  primaryClass = {cs.CV}
}

@article{zhu18_overv_machin_teach,
  title = {An Overview of Machine Teaching},
  author = {Zhu, Xiaojin and Singla, Adish and Zilles, Sandra and Rafferty, Anna N.},
  year = {2018},
  abstract = {In this paper we try to organize machine teaching as a coherent set of ideas. Each idea is presented as varying along a dimension. The collection of dimensions then form the problem space of machine teaching, such that existing teaching problems can be characterized in this space. We hope this organization allows us to gain deeper understanding of individual teaching problems, discover connections among them, and identify gaps in the field.},
  archivePrefix = {arXiv},
  eprint = {1801.05927},
  eprinttype = {arxiv},
  file = {/home/jethro/Zotero/storage/M3P8Z5DK/Zhu et al. - 2018 - An overview of machine teaching.pdf},
  journal = {CoRR},
  primaryClass = {cs.LG}
}

@inproceedings{ziebart2008_maxentrl,
  title = {Maximum Entropy Inverse Reinforcement Learning.},
  author = {Ziebart, Brian and Maas, Andrew and Bagnell, J. and Dey, Anind},
  year = {2008},
  month = jan,
  pages = {1433--1438}
}


