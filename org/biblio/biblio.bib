@article{baltrusaitis17:_multim_machin_learn,
  journal         = {CoRR},
  title           = {Multimodal Machine Learning: A Survey and Taxonomy},
  author          = {Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency,
                  Louis-Philippe},
  archivePrefix   = {arXiv},
  year            = 2017,
  eprint          = {1705.09406},
  primaryClass    = {cs.LG},
  abstract        = {Our experience of the world is multimodal - we see objects,
                  hear sounds, feel texture, smell odors, and taste flavors.
                  Modality refers to the way in which something happens or is
                  experienced and a research problem is characterized as
                  multimodal when it includes multiple such modalities.
                  Baltru\vsaitis, Tadas, Ahuja, C., & Morency, L., Multimodal
                  machine learning: a survey and taxonomy, CoRR, (), (2017). In
                  order for Artificial Intelligence to make progress in
                  understanding the world around us, it needs to be able to
                  interpret such multimodal signals together. Multimodal machine
                  learning aims to build models that can process and relate
                  information from multiple modalities. It is a vibrant
                  multi-disciplinary field of increasing importance and with
                  extraordinary potential. Instead of focusing on specific
                  multimodal applications, this paper surveys the recent
                  advances in multimodal machine learning itself and presents
                  them in a common taxonomy. We go beyond the typical early and
                  late fusion categorization and identify broader challenges
                  that are faced by multimodal machine learning, namely:
                  representation, translation, alignment, fusion, and
                  co-learning. This new taxonomy will enable researchers to
                  better understand the state of the field and identify
                  directions for future research.},
  url             = {http://arxiv.org/abs/1705.09406v2},
}

@article{chen20_simpl_framew_contr_learn_visual_repres,
  author          = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and
                  Hinton, Geoffrey},
  title           = {A Simple Framework for Contrastive Learning of Visual
                  Representations},
  journal         = {CoRR},
  year            = 2020,
  url             = {http://arxiv.org/abs/2002.05709v1},
  abstract        = {This paper presents SimCLR: a simple framework for
                  contrastive learning of visual representations. We simplify
                  recently proposed contrastive self-supervised learning
                  algorithms without requiring specialized architectures or a
                  memory bank. In order to understand what enables the
                  contrastive prediction tasks to learn useful representations,
                  we systematically study the major components of our framework.
                  We show that (1) composition of data augmentations plays a
                  critical role in defining effective predictive tasks, (2)
                  introducing a learnable nonlinear transformation between the
                  representation and the contrastive loss substantially improves
                  the quality of the learned representations, and (3)
                  contrastive learning benefits from larger batch sizes and more
                  training steps compared to supervised learning. By combining
                  these findings, we are able to considerably outperform
                  previous methods for self-supervised and semi-supervised
                  learning on ImageNet. A linear classifier trained on
                  self-supervised representations learned by SimCLR achieves
                  76.5 \% top-1 accuracy, which is a 7 \% relative improvement
                  over previous state-of-the-art, matching the performance of a
                  supervised ResNet-50. When fine-tuned on only 1 \% of the
                  labels, we achieve 85.8 \% top-5 accuracy, outperforming
                  AlexNet with 100X fewer labels.},
  archivePrefix   = {arXiv},
  eprint          = {2002.05709},
  primaryClass    = {cs.LG},
}

@article{nakano11_spikin_neural_networ_model_free,
  author          = {Takashi Nakano and Makoto Otsuka},
  title           = {Spiking Neural Network Model of Free-Energy-Based
                  Reinforcement Learning},
  journal         = {BMC Neuroscience},
  volume          = 12,
  number          = {S1},
  pages           = {P244},
  year            = 2011,
  doi             = {10.1186/1471-2202-12-s1-p244},
  url             = {https://doi.org/10.1186/1471-2202-12-s1-p244},
  DATE_ADDED      = {Thu Jan 16 23:13:33 2020},
}

@misc{nateliason_how_take_smart_notes,
  author          = {Nat Eliason},
  howpublished    = {https://www.nateliason.com/blog/smart-notes},
  note            = {Online; accessed 14 February 2020},
  title           = {How to Take Smart Notes: A Step-by-Step Guide - Nat
                  Eliason},
  year            = 2020,
}

@article{newcombe15_how_amazon_web_servic_uses_formal_method,
  author          = {Chris Newcombe and Tim Rath and Fan Zhang and Bogdan
                  Munteanu and Marc Brooker and Michael Deardeuff},
  title           = {How Amazon Web Services Uses Formal Methods},
  journal         = {Communications of the ACM},
  volume          = 58,
  number          = 4,
  pages           = {66-73},
  year            = 2015,
  doi             = {10.1145/2699417},
  url             = {https://doi.org/10.1145/2699417},
  DATE_ADDED      = {Thu Jan 16 14:52:06 2020},
}

@article{sallans04a_ferl,
  author          = {Sallans, Brian and Hinton, Geoffrey},
  year            = 2004,
  month           = 08,
  pages           = {1063-1088},
  title           = {Reinforcement Learning with Factored States and Actions.},
  volume          = 5,
  journal         = {Journal of Machine Learning Research}
}
